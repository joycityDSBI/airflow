"""
ìˆ˜ì •ëœ ë¶€ë¶„:
1. Variable.get() ì—ëŸ¬ ì²˜ë¦¬ ê°œì„  (KeyError, VARIABLE_NOT_FOUND)
2. Airflow 3.0+ í˜¸í™˜ì„± ì²˜ë¦¬
3. ì´ˆê¸° ì‹¤í–‰ ì‹œ Variable ìë™ ìƒì„±
"""

import hashlib
import logging
import os
import smtplib
import time
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from typing import Any, Dict
import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.base import BaseSensorOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from google.cloud import bigquery
import json
from google.oauth2 import service_account

# notion_utils ëª¨ë“ˆ ì„í¬íŠ¸
from notion_utils import update_notion_databases


# notion_utils ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from notion_utils import update_notion_databases
except ImportError:
    logging.warning("âš ï¸ notion_utils ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”ŒëŸ¬ê·¸ì¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")




# ===== ì„¤ì • =====
def get_config(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variableì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°"""
    env_value = os.environ.get(key)
    if env_value:
        return env_value
    
    try:
        # Airflow 3.0+ í˜¸í™˜ì„±
        try:
            from airflow.sdk import Variable as SDKVariable
            return SDKVariable.get(key, default)
        except ImportError:
            # Airflow 2.x
            return Variable.get(key, default_var=default)
    except Exception:
        return default

def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)

# ì´ë©”ì¼ ì„¤ì •
SMTP_HOST = get_config("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(get_config("SMTP_PORT", "587"))
SMTP_USER = get_config("SMTP_USER")
SMTP_PASSWORD = get_config("SMTP_PASSWORD")
EMAIL_TO = get_config("EMAIL_TO")
EMAIL_FROM = get_config("EMAIL_FROM", SMTP_USER)

# BigQuery ì„¤ì •
TARGET_PROJECT = "aibi-service"
TARGET_DATASET = "Service_Set"
METADATA_HASH_VAR = "bq_metadata_hash"
CREDENTIALS_JSON = get_var('GOOGLE_CREDENTIAL_JSON')

COLUMN_QUERY = f"""
SELECT
  table_catalog AS project_id,
  table_schema  AS dataset_id,
  table_name,
  column_name,
  data_type,
  is_nullable,
  CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_table_id
FROM `{TARGET_PROJECT}.{TARGET_DATASET}.INFORMATION_SCHEMA.COLUMNS`
WHERE table_name IN (
  SELECT table_name
  FROM `{TARGET_PROJECT}.{TARGET_DATASET}.INFORMATION_SCHEMA.TABLES`
  WHERE table_type IN ('VIEW', 'MATERIALIZED VIEW')
)
ORDER BY full_table_id, column_name
"""


# ===== Custom Sensor (ìˆ˜ì •ë¨) =====
class BigQueryMetadataChangeSensor(BaseSensorOperator):
    """BigQuery ë©”íƒ€ë°ì´í„° ë³€ê²½ ê°ì§€ Sensor"""
    template_fields = ('project_id', 'query', 'hash_variable')
    
    def __init__(self, *, project_id: str, query: str, hash_variable: str, **kwargs):
        super().__init__(**kwargs)
        self.project_id = project_id
        self.query = query
        self.hash_variable = hash_variable
    
    def _get_variable_safely(self, key: str, default: str = None) -> str:
        # (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        try:
            try:
                from airflow.sdk import Variable as SDKVariable
                return SDKVariable.get(key, default)
            except ImportError:
                return Variable.get(key, default_var=default)
        except Exception:
            return default
    
    def _set_variable_safely(self, key: str, value: str) -> bool:
        """Variableì„ ì•ˆì „í•˜ê²Œ ì €ì¥"""
        try:
            Variable.set(key, value)
            logging.info(f"ğŸ’¾ Variable ì €ì¥ ì„±ê³µ: {key}")
            return True
        except Exception as e:
            logging.error(f"ğŸ”¥ Variable ì €ì¥ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return False
    
    def poke(self, context: Dict[str, Any]) -> bool:
        logging.info("ğŸ” BigQuery ë©”íƒ€ë°ì´í„° ë³€ê²½ ê°ì§€ ì‹œì‘")
        
        try:
            # Credentials ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
            cred_dict = json.loads(CREDENTIALS_JSON)
            if 'private_key' in cred_dict and '\\n' in cred_dict['private_key']:
                cred_dict['private_key'] = cred_dict['private_key'].replace('\\n', '\n')
            
            credentials = service_account.Credentials.from_service_account_info(
                cred_dict, scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            
            bq_client = bigquery.Client(project=self.project_id, credentials=credentials)
            query_job = bq_client.query(self.query)
            results = query_job.result()
            
            rows = []
            for row in results:
                row_dict = dict(row)
                rows.append(str(sorted(row_dict.items())))
            
            data_str = "".join(sorted(rows))
            current_hash = hashlib.sha256(data_str.encode()).hexdigest()
            
            logging.info(f"ğŸ“Š í˜„ì¬ í•´ì‹œ: {current_hash[:16]}...")
            
            # ì´ì „ í•´ì‹œ ê°€ì ¸ì˜¤ê¸°
            previous_hash = self._get_variable_safely(self.hash_variable)
            
            # XComì— í˜„ì¬ í•´ì‹œ í‘¸ì‹œ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ìš©)
            context['ti'].xcom_push(key='current_hash', value=current_hash)
            
            # [ìˆ˜ì •ë¨] ì´ˆê¸° ì‹¤í–‰ì´ê±°ë‚˜ í•´ì‹œê°€ ë‹¤ë¥´ë©´ True ë°˜í™˜
            if previous_hash is None:
                logging.info("ğŸš€ ìµœì´ˆ ì‹¤í–‰ ê°ì§€ (ì´ì „ í•´ì‹œ ì—†ìŒ) -> ì‹¤í–‰")
                return True
            elif current_hash != previous_hash:
                logging.info(f"âœ… ë³€ê²½ ê°ì§€ë¨ ({previous_hash[:8]} -> {current_hash[:8]}) -> ì‹¤í–‰")
                return True
            else:
                logging.info("â¸ï¸ ë³€ê²½ ì—†ìŒ -> ëŒ€ê¸°")
                return False
                
        except Exception as e:
            logging.error(f"ğŸ”¥ Sensor ì—ëŸ¬: {e}", exc_info=True)
            return False


# ===== Task í•¨ìˆ˜ë“¤ =====
def extract_bq_metadata(**context):
    # (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # ë‹¨, ëŒ€ëŸ‰ ë°ì´í„°ì¼ ê²½ìš° XCom ëŒ€ì‹  GCS ì‚¬ìš© ê³ ë ¤ í•„ìš”
    start_ts = time.time()
    try:
        cred_dict = json.loads(CREDENTIALS_JSON)
        if 'private_key' in cred_dict and '\\n' in cred_dict['private_key']:
             cred_dict['private_key'] = cred_dict['private_key'].replace('\\n', '\n')
        credentials = service_account.Credentials.from_service_account_info(cred_dict)
        
        bq_client = bigquery.Client(project=TARGET_PROJECT, credentials=credentials)
        
        # DataFrame ë³€í™˜
        df = bq_client.query(COLUMN_QUERY).result().to_dataframe(create_bqstorage_client=False)
        
        context['ti'].xcom_push(key='metadata_df', value=df.to_dict('records'))
        context['ti'].xcom_push(key='row_count', value=len(df))
        
        logging.info(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(df)} rows")
    except Exception as e:
        logging.exception(f"ğŸ”¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise


def sync_to_notion(**context):
    """Notion ë™ê¸°í™”"""
    start_ts = time.time()
    logging.info("=" * 70)
    logging.info("ğŸ§  Notion ë™ê¸°í™” ì‹œì‘")
    logging.info("=" * 70)
    
    try:
        ti = context['ti']
        
        # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        logging.info("ğŸ“Œ Step 1: XComì—ì„œ ë©”íƒ€ë°ì´í„° ì½ê¸°")
        metadata_records = ti.xcom_pull(
            task_ids='extract_metadata',
            key='metadata_df'
        )
        
        if not metadata_records:
            raise ValueError("XComì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logging.info(f"  âœ… ë°ì´í„° ë¡œë“œ: {len(metadata_records)} records")
        
        # DataFrame ë³€í™˜
        logging.info("ğŸ“Œ Step 2: DataFrame ë³€í™˜")
        df = pd.DataFrame(metadata_records)
        logging.info(f"  âœ… ë³€í™˜ ì™„ë£Œ: {len(df)} rows Ã— {len(df.columns)} cols")
        logging.info(f"  ì»¬ëŸ¼: {df.columns.tolist()}")
        
        # Notion ì—…ë°ì´íŠ¸ (ì—¬ê¸°ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ)
        logging.info("ğŸ“Œ Step 3: update_notion_databases í˜¸ì¶œ")
        result = update_notion_databases(df)  # ğŸ“Œ ì´ì œ ì˜ˆì™¸ë¥¼ throwí•  ê²ƒ!
        logging.info(f"  âœ… Notion ì—…ë°ì´íŠ¸ ì™„ë£Œ: {result}")
        
        took = time.time() - start_ts
        ti.xcom_push(key='success', value=True)
        ti.xcom_push(key='duration', value=round(took, 2))
        
        logging.info("=" * 70)
        logging.info(f"âœ… Notion ë™ê¸°í™” ì™„ë£Œ (â±ï¸ {took:.1f}s)")
        logging.info("=" * 70)
        
    except Exception as e:
        logging.error("=" * 70)
        logging.exception(f"ğŸ”¥ Notion ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error("=" * 70)
        
        context['ti'].xcom_push(key='success', value=False)
        context['ti'].xcom_push(key='error', value=str(e))
        
        # ğŸ“Œ ì—ëŸ¬ë¥¼ raiseí•˜ë©´ DAGì—ì„œ ê°ì§€ ê°€ëŠ¥!
        raise


def send_email_notification(**context):
    """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
    if not all([SMTP_USER, SMTP_PASSWORD, EMAIL_TO]):
        logging.warning("âš ï¸ ì´ë©”ì¼ ì„¤ì • ëˆ„ë½")
        return
    
    ti = context['ti']
    success = ti.xcom_pull(task_ids='sync_to_notion', key='success')
    rows = ti.xcom_pull(task_ids='extract_metadata', key='row_count')
    duration = ti.xcom_pull(task_ids='sync_to_notion', key='duration')
    hash_val = ti.xcom_pull(task_ids='detect_metadata_change', key='current_hash')
    
    status = "ì„±ê³µ" if success else "ì‹¤íŒ¨"
    status_color = "#4CAF50" if success else "#F44336"
    status_emoji = "âœ…" if success else "âŒ"
    
    subject = f"[Airflow] BigQuery â†’ Notion ë™ê¸°í™” {status} ({rows:,} rows)"
    
    body = f"""
    <!DOCTYPE html>
    <html>
    <head><meta charset="UTF-8"></head>
    <body style="font-family: Arial, sans-serif;">
        <div style="max-width: 600px; margin: 0 auto; padding: 20px;">
            <div style="background-color: {status_color}; color: white; padding: 20px; text-align: center;">
                <h2>{status_emoji} BigQuery â†’ Notion ë™ê¸°í™” {status}</h2>
            </div>
            <div style="background-color: #f9f9f9; padding: 20px; border: 1px solid #ddd;">
                <p><strong>ì‹¤í–‰ ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>ì²˜ë¦¬ëœ í–‰ ìˆ˜:</strong> {rows:,} rows</p>
                <p><strong>ì†Œìš” ì‹œê°„:</strong> {duration} ì´ˆ</p>
                <p><strong>ë©”íƒ€ë°ì´í„° í•´ì‹œ:</strong> {hash_val[:16] if hash_val else 'N/A'}...</p>
                <p><strong>ëŒ€ìƒ í”„ë¡œì íŠ¸:</strong> {TARGET_PROJECT}.{TARGET_DATASET}</p>
            </div>
        </div>
    </body>
    </html>
    """
    
    try:
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_FROM or SMTP_USER
        msg['To'] = EMAIL_TO
        msg.attach(MIMEText(body, 'html'))
        
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        
        logging.info("âœ… ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
        
    except Exception as e:
        logging.error(f"ğŸ”¥ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

# ===== 3. [ì‹ ê·œ] ì„±ê³µ í›„ Variable ì—…ë°ì´íŠ¸ í•¨ìˆ˜ =====
def update_variable_after_success(**context):
    """ëª¨ë“  ë™ê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ëë‚œ í›„ Variable ì—…ë°ì´íŠ¸"""
    ti = context['ti']
    current_hash = ti.xcom_pull(task_ids='detect_metadata_change', key='current_hash')
    
    if not current_hash:
        logging.warning("âš ï¸ ì—…ë°ì´íŠ¸í•  í•´ì‹œ ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    key = METADATA_HASH_VAR
    try:
        # Airflow 3.0+ í˜¸í™˜
        try:
            from airflow.sdk import Variable as SDKVariable
            SDKVariable.set(key, current_hash)
        except ImportError:
            Variable.set(key, current_hash)
            
        logging.info(f"ğŸ’¾ Variable ì—…ë°ì´íŠ¸ ì™„ë£Œ: {key} = {current_hash[:16]}...")
    except Exception as e:
        logging.error(f"ğŸ”¥ Variable ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

# ===== DAG ì •ì˜ =====
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

with DAG(
    dag_id='bq_notion_metadata_sync',
    default_args=default_args,
    description='BigQuery ë©”íƒ€ë°ì´í„°ë¥¼ Notionì— ë™ê¸°í™” (5ë¶„ë§ˆë‹¤ ì²´í¬)',
    schedule='*/5 * * * *',  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['bigquery', 'notion', 'metadata', 'etl'],
) as dag:
    
# 1. ê°ì§€
    detect_change = BigQueryMetadataChangeSensor(
        task_id='detect_metadata_change',
        project_id=TARGET_PROJECT,
        query=COLUMN_QUERY,
        hash_variable=METADATA_HASH_VAR,
        poke_interval=60,
        timeout=300,
        mode='poke', # Worker slot ì ìœ ê°€ ë¶€ë‹´ë˜ë©´ 'reschedule' ì‚¬ìš©
    )
    
    # 2. ì¶”ì¶œ
    extract_metadata = PythonOperator(
        task_id='extract_metadata',
        python_callable=extract_bq_metadata,
    )
    
    # 3. ë™ê¸°í™”
    sync_notion = PythonOperator(
        task_id='sync_to_notion',
        python_callable=sync_to_notion,
    )
    
    # 4. [ì‹ ê·œ] Variable ì—…ë°ì´íŠ¸ (ì„±ê³µ ì‹œì—ë§Œ ì‹¤í–‰)
    update_var = PythonOperator(
        task_id='update_hash_variable',
        python_callable=update_variable_after_success,
        trigger_rule=TriggerRule.ALL_SUCCESS, # ì• ë‹¨ê³„ê°€ ëª¨ë‘ ì„±ê³µí•´ì•¼ ì‹¤í–‰
    )
    
    # # Task 4: ì´ë©”ì¼ ì•Œë¦¼
    # send_email = PythonOperator(
    #     task_id='send_email_notification',
    #     python_callable=send_email_notification,
    #     trigger_rule=TriggerRule.ALL_DONE,  # ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‘ ì‹¤í–‰
    # )
    
    # ì˜ì¡´ì„±
    detect_change >> extract_metadata >> sync_notion >> update_var