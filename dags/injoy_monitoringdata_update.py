from datetime import datetime, timedelta, timezone as dt_timezone
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from airflow.models import Variable
import pandas as pd
import requests
from airflow import Dataset
import numpy as np
import json
import logging

dataset_injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

# ì œì™¸ ê·¸ë£¹ í•„í„°ë§
# exclude_groups = ["DITeam", "admins", "users", 
#                     "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ë§ˆì¼€íŒ…ì‚¬ì´ì–¸ìŠ¤íŒ€", 
#                     "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ì˜ˆì¸¡ëª¨ë¸ë§íŒ€"]

exclude_groups = []


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='injoy_monitoringdata_producer',
    default_args=default_args,
    description='Process Databricks audit logs for aibiGenie',
    schedule='0 16 * * *',  # ë§¤ì¼ ìƒˆë²½ 1ì‹œ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['databricks', 'audit', 'genie'],
)

def get_databricks_config():
    """Databricks ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'instance': Variable.get('databricks_instance'),
        'token': Variable.get('databricks_token')
    }

# ì„¤ì •
headers = {
    "Authorization": f"Bearer {get_databricks_config()['token']}"
}

url = f"{get_databricks_config()['instance']}/api/2.0/token/list"

def tokenize_databricks(url, headers):
    
    resp = requests.get(url, headers=headers)

    if resp.status_code != 200:
        print(f"âŒ í† í° ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        print(resp.text)
    else:
        tokens = resp.json().get("token_infos", [])
        
        def convert_ms(ms):
            if ms == -1:
                return None
            return datetime.fromtimestamp(ms / 1000)

        for t in tokens:
            creation = convert_ms(t.get("creation_time"))
            expiry = convert_ms(t.get("expiry_time"))
            if creation:
                print(f"   ìƒì„±ì‹œê°„: {creation.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ìƒì„±ì‹œê°„: ì•Œ ìˆ˜ ì—†ìŒ")
            if expiry:
                print(f"   ë§Œë£Œì‹œê°„: {expiry.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ë§Œë£Œì‹œê°„: ë¬´ê¸°í•œ ë˜ëŠ” ì—†ìŒ")



def extract_audit_logs(**context):
    """Databricks audit ë¡œê·¸ ì¶”ì¶œ ë° ì²˜ë¦¬"""
    from databricks import sql
    
    config = get_databricks_config()
    ds = context.get('ds')
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )

    query = f"""
    WITH raw_log AS (
        SELECT 
            'audit_log' as log_type,
            user_identity.email as user_email,
            action_name,
            request_params,
            response.result,
            CAST(event_time AS TIMESTAMP) + INTERVAL 9 HOURS AS event_time_kst
        FROM system.access.audit
        WHERE service_name = 'aibiGenie'
            AND action_name IN ('createConversationMessage', 'updateConversationMessageFeedback', 'getMessageQueryResult')
            AND DATE(event_time) >= CURRENT_DATE - INTERVAL 1 DAYS
            AND DATE(event_time) < CURRENT_DATE
    ),

    message_tb AS (
        SELECT
            'audit_log' as log_type,
            user_email,
            get_json_object(result, '$.user_id') AS user_id,
            action_name,
            event_time_kst,
            get_json_object(result, '$.space_id') AS space_id,
            get_json_object(result, '$.conversation_id') AS conversation_id,
            get_json_object(result, '$.message_id') AS message_id
        FROM raw_log
        WHERE action_name IN ('createConversationMessage')
    ),

    feedback_rb AS (
        SELECT * EXCEPT(rn)
        FROM (
            SELECT 
                request_params.space_id, 
                request_params.conversation_id, 
                request_params.message_id, 
                request_params.feedback_rating,
                ROW_NUMBER() OVER(
                    PARTITION BY request_params.space_id, request_params.conversation_id, request_params.message_id 
                    ORDER BY event_time_kst DESC
                ) as rn
            FROM raw_log
            WHERE action_name = 'updateConversationMessageFeedback'
        )
        WHERE rn = 1
    )

    SELECT 
        a.* EXCEPT(event_time_kst), 
        b.feedback_rating, 
        a.event_time_kst 
    FROM message_tb AS a
    LEFT JOIN feedback_rb AS b
        ON a.space_id = b.space_id
        AND a.conversation_id = b.conversation_id
        AND a.message_id = b.message_id
    """
    cursor = connection.cursor()
    cursor.execute(query)
    df_audit = cursor.fetchall_arrow().to_pandas()
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Audit log ì¶”ì¶œ ì™„ë£Œ: {len(df_audit)} rows")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_audit', value=df_audit.to_json(orient='split', date_format='iso'))
    
    return len(df_audit)

def get_user_groups(**context):
    """
    Task 2: SCIM APIë¡œ ê·¸ë£¹ ë° ì‚¬ìš©ìž ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # Step 1: ê·¸ë£¹ ì „ì²´ ëª©ë¡ ì¡°íšŒ
    group_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups"
    group_resp = requests.get(group_url, headers=headers)
    
    if group_resp.status_code != 200:
        raise Exception(f"ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {group_resp.status_code} - {group_resp.text}")
    
    groups = group_resp.json().get("Resources", [])
    df_groups = pd.DataFrame([{"group_name": g["displayName"], "group_id": g["id"]} for g in groups])
    
    target_groups = df_groups[~df_groups["group_name"].isin(exclude_groups)]
    
    # Step 3: ê° ê·¸ë£¹ì˜ êµ¬ì„±ì› ìˆ˜ì§‘
    user_group_list = []
    exclude_user_id = "6547992203707764"
    
    for _, row in target_groups.iterrows():
        group_name = row["group_name"]
        group_id = row["group_id"]
        group_detail_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups/{group_id}"
        detail_resp = requests.get(group_detail_url, headers=headers)
        
        if detail_resp.status_code != 200:
            print(f"âš ï¸ ê·¸ë£¹ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ (group_id={group_id}): {detail_resp.status_code}")
            continue
        
        group_detail = detail_resp.json()
        members = group_detail.get("members", [])
        
        for m in members:
            user_id = m.get("value")
            if user_id and user_id != exclude_user_id:
                user_group_list.append({"user_id": user_id, "group_name": group_name})
    
    # Step 4: DataFrameìœ¼ë¡œ ë³€í™˜ ë° ê·¸ë£¹í•‘
    df_user_groups = pd.DataFrame(user_group_list).drop_duplicates()
    
    df_user_groups = (
        df_user_groups
        .groupby("user_id")["group_name"]
        .apply(lambda x: sorted(set(x)))
        .reset_index()
    )
    
    print(f"âœ… ì‚¬ìš©ìž ê·¸ë£¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_user_groups)} users")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_user_groups', value=df_user_groups.to_json(orient='split'))
    
    return len(df_user_groups)

def enrich_with_groups(**context):
    """
    Task 3: Audit logì— ê·¸ë£¹ ì •ë³´ ë³‘í•©
    """
    ti = context['ti']
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit = pd.read_json(ti.xcom_pull(task_ids='extract_audit_logs', key='df_audit'), orient='split')
    df_user_groups = pd.read_json(ti.xcom_pull(task_ids='get_user_groups', key='df_user_groups'), orient='split')
    
    # ë³‘í•©
    df_audit_with_group = df_audit.merge(df_user_groups, on="user_id", how="left")
    
    # group_nameì´ NaNì¸ í–‰ ì œê±°
    df_audit_with_group = df_audit_with_group.dropna(subset=["group_name"])
    
    print(f"âœ… ê·¸ë£¹ ì •ë³´ ë³‘í•© ì™„ë£Œ: {len(df_audit_with_group)} rows")
    
    context['ti'].xcom_push(key='df_audit_with_group', value=df_audit_with_group.to_json(orient='split', date_format='iso'))
    
    return len(df_audit_with_group)

def get_space_info(**context):
    """
    Task 4: Genie APIë¡œ ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
    spaces_url = f"{config['instance']}/api/2.0/genie/spaces"
    resp = requests.get(spaces_url, headers=headers)
    
    if resp.status_code != 200:
        print(f"âš ï¸ ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        space_id_to_name = {}
    else:
        spaces = resp.json().get("spaces", [])
        space_df = pd.DataFrame([
            {"space_id": s.get("space_id"), "space_name": s.get("title")}
            for s in spaces
        ])
        space_id_to_name = dict(zip(space_df["space_id"], space_df["space_name"]))
    
    print(f"âœ… ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(space_id_to_name)} spaces")
    
    context['ti'].xcom_push(key='space_id_to_name', value=space_id_to_name)
    
    return len(space_id_to_name)

def get_message_details(**context):
    """
    Task 5: Genie APIë¡œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    """
    ti = context['ti']
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit_with_group = pd.read_json(ti.xcom_pull(task_ids='enrich_with_groups', key='df_audit_with_group'), orient='split')
    space_id_to_name = ti.xcom_pull(task_ids='get_space_info', key='space_id_to_name')
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ì´ë¦„ ì¶”ê°€
    df_audit_with_group["space_name"] = df_audit_with_group["space_id"].map(space_id_to_name)
    
    # group_nameì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë¯€ë¡œ anyë¡œ ì²´í¬
    def should_exclude(group_list):
        if isinstance(group_list, list):
            return any(g in exclude_groups for g in group_list)
        return False
    
    df_target = df_audit_with_group[~df_audit_with_group["group_name"].apply(should_exclude)].copy()
    
    # ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    contents = []
    queries = []
    statement_ids = []
    
    total_rows = len(df_target)
    print(f"ðŸ”„ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œìž‘: {total_rows} rows")
    
    for idx, row in df_target.iterrows():
        if idx % 100 == 0:
            print(f"  Progress: {idx}/{total_rows}")
        
        space_id = row["space_id"]
        conversation_id = row["conversation_id"]
        message_id = row["message_id"]
        
        if None in [space_id, conversation_id, message_id]:
            contents.append(None)
            queries.append(None)
            statement_ids.append(None)
            continue
        
        url = f"{config['instance']}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages/{message_id}"
        
        try:
            resp = requests.get(url, headers=headers)
            if resp.status_code == 200:
                data = resp.json()
                
                # Content ì²˜ë¦¬
                content_raw = data.get("content")
                if isinstance(content_raw, str):
                    content = content_raw.replace("\n", " ")
                else:
                    content = str(content_raw) if content_raw is not None else None
                
                # Query ì²˜ë¦¬
                attachments = data.get("attachments", [])
                if attachments and isinstance(attachments, list):
                    query = attachments[0].get("query", {}).get("query", None)
                else:
                    query = None
                
                # Statement ID ì²˜ë¦¬
                statement_id = data.get("query_result", {}).get("statement_id")
                
            else:
                content, query, statement_id = None, None, None
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ ({idx}í–‰): {e}")
            content, query, statement_id = None, None, None
        
        contents.append(content)
        queries.append(query)
        statement_ids.append(statement_id)
    
    # ë°ì´í„° ì¶”ê°€
    df_target['content'] = contents
    df_target['query'] = queries
    df_target['statement_id'] = statement_ids
    
    print(f"âœ… ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_target)} rows")
    
    context['ti'].xcom_push(key='df_target', value=df_target.to_json(orient='split', date_format='iso'))
    
    return len(df_target)


def merge_query_history(**context):
    """
    Task 6: Query historyì™€ ë³‘í•© ë° ìµœì¢… ë°ì´í„° ì €ìž¥
    """
    from databricks import sql
    import pandas as pd
    from io import StringIO
    import numpy as np
    import json
    
    ti = context['ti']
    config = get_databricks_config()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    json_data = ti.xcom_pull(task_ids='get_message_details', key='df_target')
    df_target = pd.read_json(StringIO(json_data), orient='split')
    
    # Databricks SQL ì—°ê²°
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )
    
    cursor = connection.cursor()
    
    # Query history ì¡°íšŒ
    query_history_sql = """
    SELECT 
        statement_id, 
        executed_by, 
        execution_status, 
        CAST(total_duration_ms AS DOUBLE) / 1000 AS query_duration_seconds, 
        CAST(result_fetch_duration_ms AS DOUBLE) / 1000 AS query_result_fetch_duration_seconds, 
        CAST(end_time AS TIMESTAMP) + INTERVAL 9 HOURS AS query_end_time_kst
    FROM system.query.history
    WHERE query_source.genie_space_id IS NOT NULL
        AND statement_type = 'SELECT'
        AND DATE(end_time) >= CURRENT_DATE - INTERVAL 1 DAYS
        AND DATE(end_time) < CURRENT_DATE
    """
    
    cursor.execute(query_history_sql)
    query_df = cursor.fetchall_arrow().to_pandas()
    
    print(f"ðŸ“Š Query history ì¡°íšŒ ì™„ë£Œ: {len(query_df)} rows")
    
    # ì»¬ëŸ¼ ì¡´ìž¬ í™•ì¸ ë° rename
    if 'executed_by' in query_df.columns:
        query_df_renamed = query_df.rename(columns={"executed_by": "user_email"})
    else:
        print(f"âš ï¸ Warning: 'executed_by' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        cursor.close()
        connection.close()
        raise KeyError(f"'executed_by' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë³‘í•©
    df_audit_enriched = df_target.merge(
        query_df_renamed[[
            "statement_id", "user_email", "query_end_time_kst", 
            "query_duration_seconds", "query_result_fetch_duration_seconds", "execution_status"
        ]],
        how="left",
        on=["statement_id", "user_email"]
    )
    
    # ì‘ë‹µ ì†Œìš”ì‹œê°„ ê³„ì‚°
    df_audit_enriched["event_time_kst"] = pd.to_datetime(df_audit_enriched["event_time_kst"])
    df_audit_enriched["query_end_time_kst"] = pd.to_datetime(df_audit_enriched["query_end_time_kst"])
    df_audit_enriched["message_response_duration_seconds"] = (
        df_audit_enriched["query_end_time_kst"] - df_audit_enriched["event_time_kst"]
    ).dt.total_seconds()
    
    print(f"âœ… Query history ë³‘í•© ì™„ë£Œ: {len(df_audit_enriched)} rows")
    
    # ===== ê¸°ì¡´ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ =====
    
    cursor.execute("DESCRIBE datahub.injoy_ops_schema.injoy_monitoring_data")
    table_schema = cursor.fetchall()
    table_column_types = {row[0]: row[1] for row in table_schema}
    table_columns = list(table_column_types.keys())
    
    print(f"ðŸ“Š ê¸°ì¡´ í…Œì´ë¸” ì»¬ëŸ¼: {table_columns}")
    
    # DataFrame ì»¬ëŸ¼ì„ í…Œì´ë¸” ì»¬ëŸ¼ì— ë§žì¶”ê¸°
    available_columns = [col for col in table_columns if col in df_audit_enriched.columns]
    df_to_insert = df_audit_enriched[available_columns].copy()
    
    print(f"ðŸ“Š UPSERTí•  ì»¬ëŸ¼: {available_columns}")
    
    # ===== SQL ë¦¬í„°ëŸ´ ë³€í™˜ í•¨ìˆ˜ =====
    
    def value_to_sql_literal(val, col_type):
        """ê°’ì„ SQL ë¦¬í„°ëŸ´ë¡œ ë³€í™˜"""
        # NULL ì²˜ë¦¬
        if val is None:
            return 'NULL'
        
        try:
            if pd.isna(val):
                return 'NULL'
        except (ValueError, TypeError):
            pass
        
        # ARRAY íƒ€ìž… ì²˜ë¦¬
        if 'array' in col_type.lower():
            if isinstance(val, list):
                # ë¦¬ìŠ¤íŠ¸ì˜ ê° ìš”ì†Œë¥¼ ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  ì´ìŠ¤ì¼€ì´í”„
                escaped_items = [f"'{str(item).replace(chr(39), chr(39)+chr(39))}'" for item in val]
                return f"array({', '.join(escaped_items)})"
            else:
                return 'NULL'
        
        # TIMESTAMP ì²˜ë¦¬
        if 'timestamp' in col_type.lower():
            if isinstance(val, pd.Timestamp):
                return f"timestamp '{val.strftime('%Y-%m-%d %H:%M:%S')}'"
            else:
                return f"timestamp '{str(val)}'"
        
        # DOUBLE/FLOAT ì²˜ë¦¬
        if col_type.lower() in ['double', 'float']:
            if pd.notna(val):
                return str(float(val))
            else:
                return 'NULL'
        
        # BIGINT/INT ì²˜ë¦¬
        if col_type.lower() in ['bigint', 'int', 'integer']:
            if pd.notna(val):
                return str(int(val))
            else:
                return 'NULL'
        
        # STRING ì²˜ë¦¬ (ê¸°ë³¸)
        if isinstance(val, str):
            # ìž‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
            escaped_val = val.replace("'", "''")
            return f"'{escaped_val}'"
        
        # ê¸°íƒ€
        return f"'{str(val)}'"
    
    # ===== ìž„ì‹œ í…Œì´ë¸” ìƒì„± (CREATE TABLE AS SELECT VALUES) =====
    
    temp_table = "datahub.injoy_ops_schema.temp_monitoring_data"
    
    # ê¸°ì¡´ ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ
    cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
    print(f"âœ… ê¸°ì¡´ ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ")
    
    # VALUES ì ˆ ìƒì„±
    values_list = []
    for idx, row in df_to_insert.iterrows():
        row_values = []
        for col in available_columns:
            col_type = table_column_types[col]
            sql_literal = value_to_sql_literal(row[col], col_type)
            row_values.append(sql_literal)
        values_list.append(f"({', '.join(row_values)})")
    
    # 100ê°œì”© ë‚˜ëˆ ì„œ INSERT (SQL ë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒ ë°©ì§€)
    batch_size = 100
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ í…Œì´ë¸” ìƒì„±
    first_batch = values_list[:batch_size]
    columns_str = ', '.join([f"`{col}`" for col in available_columns])
    
    create_temp_sql = f"""
    CREATE TABLE {temp_table} AS
    SELECT * FROM VALUES
    {', '.join(first_batch)}
    AS t({columns_str})
    """
    
    print(f"ðŸ“ ìž„ì‹œ í…Œì´ë¸” ìƒì„± (ì²« {len(first_batch)} rows)...")
    cursor.execute(create_temp_sql)
    print(f"âœ… ìž„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
    # ë‚˜ë¨¸ì§€ ë°°ì¹˜ INSERT
    remaining_batches = [values_list[i:i+batch_size] for i in range(batch_size, len(values_list), batch_size)]
    
    for batch_idx, batch in enumerate(remaining_batches):
        insert_batch_sql = f"""
        INSERT INTO {temp_table}
        SELECT * FROM VALUES
        {', '.join(batch)}
        AS t({columns_str})
        """
        print(f"ðŸ“ ë°°ì¹˜ {batch_idx + 2} ì‚½ìž… ì¤‘ ({len(batch)} rows)...")
        cursor.execute(insert_batch_sql)
    
    print(f"âœ… ìž„ì‹œ í…Œì´ë¸”ì— ì´ {len(values_list)} rows ì‚½ìž… ì™„ë£Œ")
    
    # ===== MERGE ì‹¤í–‰ (UPSERT) =====
    
    merge_key = "statement_id"
    update_columns = [col for col in available_columns if col != merge_key]
    update_set = ', '.join([f"target.`{col}` = source.`{col}`" for col in update_columns])
    
    insert_columns = ', '.join([f"`{col}`" for col in available_columns])
    insert_values = ', '.join([f"source.`{col}`" for col in available_columns])
    
    merge_sql = f"""
    MERGE INTO datahub.injoy_ops_schema.injoy_monitoring_data AS target
    USING {temp_table} AS source
    ON target.`{merge_key}` = source.`{merge_key}`
    WHEN MATCHED THEN
        UPDATE SET {update_set}
    WHEN NOT MATCHED THEN
        INSERT ({insert_columns})
        VALUES ({insert_values})
    """
    
    print(f"ðŸ“ MERGE ì‹¤í–‰ ì¤‘...")
    cursor.execute(merge_sql)
    print("âœ… ë°ì´í„° UPSERT ì™„ë£Œ")
    
    # ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ
    cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
    print(f"âœ… ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ")
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Delta í…Œì´ë¸” ì €ìž¥ ì™„ë£Œ: datahub.injoy_ops_schema.injoy_monitoring_data")
    print(f"âœ… ì´ {len(values_list)} rows UPSERTë¨")
    
    return len(values_list)


# Task ì •ì˜
task0 = PythonOperator(
    task_id='tokenize_databricks',
    python_callable=tokenize_databricks,
    op_kwargs={'url': url, 'headers': headers},
    dag=dag,
)

task1 = PythonOperator(
    task_id='extract_audit_logs',
    python_callable=extract_audit_logs,
    dag=dag,
)

task2 = PythonOperator(
    task_id='get_user_groups',
    python_callable=get_user_groups,
    dag=dag,
)

task3 = PythonOperator(
    task_id='enrich_with_groups',
    python_callable=enrich_with_groups,
    dag=dag,
)

task4 = PythonOperator(
    task_id='get_space_info',
    python_callable=get_space_info,
    dag=dag,
)

task5 = PythonOperator(
    task_id='get_message_details',
    python_callable=get_message_details,
    dag=dag,
)

task6 = PythonOperator(
    task_id='merge_query_history',
    python_callable=merge_query_history,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì„¤ì •
task0 >> [task1, task2] >> task3
[task3, task4] >> task5 >> task6