from datetime import datetime, timedelta, timezone as dt_timezone
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from airflow.models import Variable
import pandas as pd
import requests
from airflow import Dataset

dataset_injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

# ì œì™¸ ê·¸ë£¹ í•„í„°ë§
# exclude_groups = ["DITeam", "admins", "users", 
#                     "ì „ëµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ë§ˆì¼€íŒ…ì‚¬ì´ì–¸ìŠ¤íŒ€", 
#                     "ì „ëµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ì˜ˆì¸¡ëª¨ë¸ë§íŒ€"]

exclude_groups = []


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='injoy_monitoringdata_producer',
    default_args=default_args,
    description='Process Databricks audit logs for aibiGenie',
    schedule='0 16 * * *',  # ë§¤ì¼ ìƒˆë²½ 1ì‹œ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['databricks', 'audit', 'genie'],
)

def get_databricks_config():
    """Databricks ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'instance': Variable.get('databricks_instance'),
        'token': Variable.get('databricks_token')
    }

# ì„¤ì •
headers = {
    "Authorization": f"Bearer {get_databricks_config()['token']}"
}

url = f"{get_databricks_config()['instance']}/api/2.0/token/list"

def tokenize_databricks(url, headers):
    
    resp = requests.get(url, headers=headers)

    if resp.status_code != 200:
        print(f"âŒ í† í° ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        print(resp.text)
    else:
        tokens = resp.json().get("token_infos", [])
        
        def convert_ms(ms):
            if ms == -1:
                return None
            return datetime.fromtimestamp(ms / 1000)

        for t in tokens:
            creation = convert_ms(t.get("creation_time"))
            expiry = convert_ms(t.get("expiry_time"))
            if creation:
                print(f"   ìƒì„±ì‹œê°„: {creation.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ìƒì„±ì‹œê°„: ì•Œ ìˆ˜ ì—†ìŒ")
            if expiry:
                print(f"   ë§Œë£Œì‹œê°„: {expiry.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ë§Œë£Œì‹œê°„: ë¬´ê¸°í•œ ë˜ëŠ” ì—†ìŒ")



def extract_audit_logs(**context):
    """Databricks audit ë¡œê·¸ ì¶”ì¶œ ë° ì²˜ë¦¬"""
    from databricks import sql
    
    config = get_databricks_config()
    ds = context.get('ds')
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )

    query = f"""
    WITH raw_log AS (
        SELECT 
            'audit_log' as log_type,
            user_identity.email as user_email,
            action_name,
            request_params,
            response.result,
            CAST(event_time AS TIMESTAMP) + INTERVAL 9 HOURS AS event_time_kst
        FROM system.access.audit
        WHERE service_name = 'aibiGenie'
            AND action_name IN ('createConversationMessage', 'updateConversationMessageFeedback', 'getMessageQueryResult')
            AND DATE(event_time) = '{ds}'  -- ì‹¤í–‰ ë‚ ì§œ ê¸°ì¤€
    ),

    message_tb AS (
        SELECT
            'audit_log' as log_type,
            user_email,
            get_json_object(result, '$.user_id') AS user_id,
            action_name,
            event_time_kst,
            get_json_object(result, '$.space_id') AS space_id,
            get_json_object(result, '$.conversation_id') AS conversation_id,
            get_json_object(result, '$.message_id') AS message_id
        FROM raw_log
        WHERE action_name IN ('createConversationMessage')
    ),

    feedback_rb AS (
        SELECT * EXCEPT(rn)
        FROM (
            SELECT 
                request_params.space_id, 
                request_params.conversation_id, 
                request_params.message_id, 
                request_params.feedback_rating,
                ROW_NUMBER() OVER(
                    PARTITION BY request_params.space_id, request_params.conversation_id, request_params.message_id 
                    ORDER BY event_time_kst DESC
                ) as rn
            FROM raw_log
            WHERE action_name = 'updateConversationMessageFeedback'
        )
        WHERE rn = 1
    )

    SELECT 
        a.* EXCEPT(event_time_kst), 
        b.feedback_rating, 
        a.event_time_kst 
    FROM message_tb AS a
    LEFT JOIN feedback_rb AS b
        ON a.space_id = b.space_id
        AND a.conversation_id = b.conversation_id
        AND a.message_id = b.message_id
    """
    cursor = connection.cursor()
    cursor.execute(query)
    df_audit = cursor.fetchall_arrow().to_pandas()
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Audit log ì¶”ì¶œ ì™„ë£Œ: {len(df_audit)} rows")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_audit', value=df_audit.to_json(orient='split', date_format='iso'))
    
    return len(df_audit)

def get_user_groups(**context):
    """
    Task 2: SCIM APIë¡œ ê·¸ë£¹ ë° ì‚¬ìš©ì ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # Step 1: ê·¸ë£¹ ì „ì²´ ëª©ë¡ ì¡°íšŒ
    group_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups"
    group_resp = requests.get(group_url, headers=headers)
    
    if group_resp.status_code != 200:
        raise Exception(f"ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {group_resp.status_code} - {group_resp.text}")
    
    groups = group_resp.json().get("Resources", [])
    df_groups = pd.DataFrame([{"group_name": g["displayName"], "group_id": g["id"]} for g in groups])
    
    target_groups = df_groups[~df_groups["group_name"].isin(exclude_groups)]
    
    # Step 3: ê° ê·¸ë£¹ì˜ êµ¬ì„±ì› ìˆ˜ì§‘
    user_group_list = []
    exclude_user_id = "6547992203707764"
    
    for _, row in target_groups.iterrows():
        group_name = row["group_name"]
        group_id = row["group_id"]
        group_detail_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups/{group_id}"
        detail_resp = requests.get(group_detail_url, headers=headers)
        
        if detail_resp.status_code != 200:
            print(f"âš ï¸ ê·¸ë£¹ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ (group_id={group_id}): {detail_resp.status_code}")
            continue
        
        group_detail = detail_resp.json()
        members = group_detail.get("members", [])
        
        for m in members:
            user_id = m.get("value")
            if user_id and user_id != exclude_user_id:
                user_group_list.append({"user_id": user_id, "group_name": group_name})
    
    # Step 4: DataFrameìœ¼ë¡œ ë³€í™˜ ë° ê·¸ë£¹í•‘
    df_user_groups = pd.DataFrame(user_group_list).drop_duplicates()
    
    df_user_groups = (
        df_user_groups
        .groupby("user_id")["group_name"]
        .apply(lambda x: sorted(set(x)))
        .reset_index()
    )
    
    print(f"âœ… ì‚¬ìš©ì ê·¸ë£¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_user_groups)} users")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_user_groups', value=df_user_groups.to_json(orient='split'))
    
    return len(df_user_groups)

def enrich_with_groups(**context):
    """
    Task 3: Audit logì— ê·¸ë£¹ ì •ë³´ ë³‘í•©
    """
    ti = context['ti']
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit = pd.read_json(ti.xcom_pull(task_ids='extract_audit_logs', key='df_audit'), orient='split')
    df_user_groups = pd.read_json(ti.xcom_pull(task_ids='get_user_groups', key='df_user_groups'), orient='split')
    
    # ë³‘í•©
    df_audit_with_group = df_audit.merge(df_user_groups, on="user_id", how="left")
    
    # group_nameì´ NaNì¸ í–‰ ì œê±°
    df_audit_with_group = df_audit_with_group.dropna(subset=["group_name"])
    
    print(f"âœ… ê·¸ë£¹ ì •ë³´ ë³‘í•© ì™„ë£Œ: {len(df_audit_with_group)} rows")
    
    context['ti'].xcom_push(key='df_audit_with_group', value=df_audit_with_group.to_json(orient='split', date_format='iso'))
    
    return len(df_audit_with_group)

def get_space_info(**context):
    """
    Task 4: Genie APIë¡œ ìŠ¤í˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
    spaces_url = f"{config['instance']}/api/2.0/genie/spaces"
    resp = requests.get(spaces_url, headers=headers)
    
    if resp.status_code != 200:
        print(f"âš ï¸ ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        space_id_to_name = {}
    else:
        spaces = resp.json().get("spaces", [])
        space_df = pd.DataFrame([
            {"space_id": s.get("space_id"), "space_name": s.get("title")}
            for s in spaces
        ])
        space_id_to_name = dict(zip(space_df["space_id"], space_df["space_name"]))
    
    print(f"âœ… ìŠ¤í˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(space_id_to_name)} spaces")
    
    context['ti'].xcom_push(key='space_id_to_name', value=space_id_to_name)
    
    return len(space_id_to_name)

def get_message_details(**context):
    """
    Task 5: Genie APIë¡œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    """
    ti = context['ti']
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit_with_group = pd.read_json(ti.xcom_pull(task_ids='enrich_with_groups', key='df_audit_with_group'), orient='split')
    space_id_to_name = ti.xcom_pull(task_ids='get_space_info', key='space_id_to_name')
    
    # ìŠ¤í˜ì´ìŠ¤ ì´ë¦„ ì¶”ê°€
    df_audit_with_group["space_name"] = df_audit_with_group["space_id"].map(space_id_to_name)
    
    # group_nameì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë¯€ë¡œ anyë¡œ ì²´í¬
    def should_exclude(group_list):
        if isinstance(group_list, list):
            return any(g in exclude_groups for g in group_list)
        return False
    
    df_target = df_audit_with_group[~df_audit_with_group["group_name"].apply(should_exclude)].copy()
    
    # ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    contents = []
    queries = []
    statement_ids = []
    
    total_rows = len(df_target)
    print(f"ğŸ”„ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {total_rows} rows")
    
    for idx, row in df_target.iterrows():
        if idx % 100 == 0:
            print(f"  Progress: {idx}/{total_rows}")
        
        space_id = row["space_id"]
        conversation_id = row["conversation_id"]
        message_id = row["message_id"]
        
        if None in [space_id, conversation_id, message_id]:
            contents.append(None)
            queries.append(None)
            statement_ids.append(None)
            continue
        
        url = f"{config['instance']}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages/{message_id}"
        
        try:
            resp = requests.get(url, headers=headers)
            if resp.status_code == 200:
                data = resp.json()
                
                # Content ì²˜ë¦¬
                content_raw = data.get("content")
                if isinstance(content_raw, str):
                    content = content_raw.replace("\n", " ")
                else:
                    content = str(content_raw) if content_raw is not None else None
                
                # Query ì²˜ë¦¬
                attachments = data.get("attachments", [])
                if attachments and isinstance(attachments, list):
                    query = attachments[0].get("query", {}).get("query", None)
                else:
                    query = None
                
                # Statement ID ì²˜ë¦¬
                statement_id = data.get("query_result", {}).get("statement_id")
                
            else:
                content, query, statement_id = None, None, None
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ ({idx}í–‰): {e}")
            content, query, statement_id = None, None, None
        
        contents.append(content)
        queries.append(query)
        statement_ids.append(statement_id)
    
    # ë°ì´í„° ì¶”ê°€
    df_target['content'] = contents
    df_target['query'] = queries
    df_target['statement_id'] = statement_ids
    
    print(f"âœ… ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_target)} rows")
    
    context['ti'].xcom_push(key='df_target', value=df_target.to_json(orient='split', date_format='iso'))
    
    return len(df_target)

def merge_query_history(**context):
    """
    Task 6: Query historyì™€ ë³‘í•© ë° ìµœì¢… ë°ì´í„° ì €ì¥
    """
    from databricks import sql
    
    ti = context['ti']
    config = get_databricks_config()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_target = pd.read_json(ti.xcom_pull(task_ids='get_message_details', key='df_target'), orient='split')
    
    # Databricks SQL ì—°ê²°
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )
    
    # Query history ì¡°íšŒ
    query_history_sql = """
    SELECT 
        statement_id, 
        executed_by, 
        execution_status, 
        CAST(total_duration_ms AS DOUBLE) / 1000 AS query_duration_seconds, 
        CAST(result_fetch_duration_ms AS DOUBLE) / 1000 AS query_result_fetch_duration_seconds, 
        CAST(end_time AS TIMESTAMP) + INTERVAL 9 HOURS AS query_end_time_kst
    FROM system.query.history
    WHERE query_source.genie_space_id IS NOT NULL
        AND statement_type = 'SELECT'
        AND DATE(end_time) >= CURRENT_DATE - INTERVAL 7 DAYS
    """
    
    cursor = connection.cursor()
    cursor.execute(query_history_sql)
    query_df = cursor.fetchall_arrow().to_pandas()
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    query_df_renamed = query_df.rename(columns={"executed_by": "user_email"})
    
    # ë³‘í•©
    df_audit_enriched = df_target.merge(
        query_df_renamed[[
            "statement_id", "user_email", "query_end_time_kst", 
            "query_duration_seconds", "query_result_fetch_duration_seconds"
        ]],
        how="left",
        on=["statement_id", "user_email"]
    )
    
    # ì‘ë‹µ ì†Œìš”ì‹œê°„ ê³„ì‚°
    df_audit_enriched["event_time_kst"] = pd.to_datetime(df_audit_enriched["event_time_kst"])
    df_audit_enriched["query_end_time_kst"] = pd.to_datetime(df_audit_enriched["query_end_time_kst"])
    df_audit_enriched["message_response_duration_seconds"] = (
        df_audit_enriched["query_end_time_kst"] - df_audit_enriched["event_time_kst"]
    ).dt.total_seconds()
    
    print(f"âœ… Query history ë³‘í•© ì™„ë£Œ: {len(df_audit_enriched)} rows")
    
    # Spark DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    
    # Pandas to Spark DataFrame
    spark_df = spark.createDataFrame(df_audit_enriched)
    
    # Delta í…Œì´ë¸”ë¡œ ì €ì¥
    spark_df.write.format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable("datahub.injoy_ops_schema.injoy_monitoring_data")
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Delta í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: datahub.injoy_ops_schema.injoy_monitoring_data")
    
    return len(df_audit_enriched)

# Task ì •ì˜
task0 = PythonOperator(
    task_id='tokenize_databricks',
    python_callable=tokenize_databricks,
    op_kwargs={'url': url, 'headers': headers},
    dag=dag,
)

task1 = PythonOperator(
    task_id='extract_audit_logs',
    python_callable=extract_audit_logs,
    dag=dag,
)

task2 = PythonOperator(
    task_id='get_user_groups',
    python_callable=get_user_groups,
    dag=dag,
)

task3 = PythonOperator(
    task_id='enrich_with_groups',
    python_callable=enrich_with_groups,
    dag=dag,
)

task4 = PythonOperator(
    task_id='get_space_info',
    python_callable=get_space_info,
    dag=dag,
)

task5 = PythonOperator(
    task_id='get_message_details',
    python_callable=get_message_details,
    dag=dag,
)

task6 = PythonOperator(
    task_id='merge_query_history',
    python_callable=merge_query_history,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì„¤ì •
task0 >> [task1, task2] >> task3
[task3, task4] >> task5 >> task6