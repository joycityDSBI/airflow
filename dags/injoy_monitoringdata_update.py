from datetime import datetime, timedelta, timezone as dt_timezone
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from airflow.models import Variable
import pandas as pd
import requests
from airflow import Dataset

dataset_injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

# ì œì™¸ ê·¸ë£¹ í•„í„°ë§
# exclude_groups = ["DITeam", "admins", "users", 
#                     "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ë§ˆì¼€íŒ…ì‚¬ì´ì–¸ìŠ¤íŒ€", 
#                     "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ì˜ˆì¸¡ëª¨ë¸ë§íŒ€"]

exclude_groups = []


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='injoy_monitoringdata_producer',
    default_args=default_args,
    description='Process Databricks audit logs for aibiGenie',
    schedule='0 16 * * *',  # ë§¤ì¼ ìƒˆë²½ 1ì‹œ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['databricks', 'audit', 'genie'],
)

def get_databricks_config():
    """Databricks ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'instance': Variable.get('databricks_instance'),
        'token': Variable.get('databricks_token')
    }

# ì„¤ì •
headers = {
    "Authorization": f"Bearer {get_databricks_config()['token']}"
}

url = f"{get_databricks_config()['instance']}/api/2.0/token/list"

def tokenize_databricks(url, headers):
    
    resp = requests.get(url, headers=headers)

    if resp.status_code != 200:
        print(f"âŒ í† í° ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        print(resp.text)
    else:
        tokens = resp.json().get("token_infos", [])
        
        def convert_ms(ms):
            if ms == -1:
                return None
            return datetime.fromtimestamp(ms / 1000)

        for t in tokens:
            creation = convert_ms(t.get("creation_time"))
            expiry = convert_ms(t.get("expiry_time"))
            if creation:
                print(f"   ìƒì„±ì‹œê°„: {creation.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ìƒì„±ì‹œê°„: ì•Œ ìˆ˜ ì—†ìŒ")
            if expiry:
                print(f"   ë§Œë£Œì‹œê°„: {expiry.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ë§Œë£Œì‹œê°„: ë¬´ê¸°í•œ ë˜ëŠ” ì—†ìŒ")



def extract_audit_logs(**context):
    """Databricks audit ë¡œê·¸ ì¶”ì¶œ ë° ì²˜ë¦¬"""
    from databricks import sql
    
    config = get_databricks_config()
    ds = context.get('ds')
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )

    query = f"""
    WITH raw_log AS (
        SELECT 
            'audit_log' as log_type,
            user_identity.email as user_email,
            action_name,
            request_params,
            response.result,
            CAST(event_time AS TIMESTAMP) + INTERVAL 9 HOURS AS event_time_kst
        FROM system.access.audit
        WHERE service_name = 'aibiGenie'
            AND action_name IN ('createConversationMessage', 'updateConversationMessageFeedback', 'getMessageQueryResult')
            AND DATE(event_time) >= CURRENT_DATE - INTERVAL 1 DAYS
            AND DATE(event_time) < CURRENT_DATE
    ),

    message_tb AS (
        SELECT
            'audit_log' as log_type,
            user_email,
            get_json_object(result, '$.user_id') AS user_id,
            action_name,
            event_time_kst,
            get_json_object(result, '$.space_id') AS space_id,
            get_json_object(result, '$.conversation_id') AS conversation_id,
            get_json_object(result, '$.message_id') AS message_id
        FROM raw_log
        WHERE action_name IN ('createConversationMessage')
    ),

    feedback_rb AS (
        SELECT * EXCEPT(rn)
        FROM (
            SELECT 
                request_params.space_id, 
                request_params.conversation_id, 
                request_params.message_id, 
                request_params.feedback_rating,
                ROW_NUMBER() OVER(
                    PARTITION BY request_params.space_id, request_params.conversation_id, request_params.message_id 
                    ORDER BY event_time_kst DESC
                ) as rn
            FROM raw_log
            WHERE action_name = 'updateConversationMessageFeedback'
        )
        WHERE rn = 1
    )

    SELECT 
        a.* EXCEPT(event_time_kst), 
        b.feedback_rating, 
        a.event_time_kst 
    FROM message_tb AS a
    LEFT JOIN feedback_rb AS b
        ON a.space_id = b.space_id
        AND a.conversation_id = b.conversation_id
        AND a.message_id = b.message_id
    """
    cursor = connection.cursor()
    cursor.execute(query)
    df_audit = cursor.fetchall_arrow().to_pandas()
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Audit log ì¶”ì¶œ ì™„ë£Œ: {len(df_audit)} rows")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_audit', value=df_audit.to_json(orient='split', date_format='iso'))
    
    return len(df_audit)

def get_user_groups(**context):
    """
    Task 2: SCIM APIë¡œ ê·¸ë£¹ ë° ì‚¬ìš©ìž ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # Step 1: ê·¸ë£¹ ì „ì²´ ëª©ë¡ ì¡°íšŒ
    group_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups"
    group_resp = requests.get(group_url, headers=headers)
    
    if group_resp.status_code != 200:
        raise Exception(f"ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {group_resp.status_code} - {group_resp.text}")
    
    groups = group_resp.json().get("Resources", [])
    df_groups = pd.DataFrame([{"group_name": g["displayName"], "group_id": g["id"]} for g in groups])
    
    target_groups = df_groups[~df_groups["group_name"].isin(exclude_groups)]
    
    # Step 3: ê° ê·¸ë£¹ì˜ êµ¬ì„±ì› ìˆ˜ì§‘
    user_group_list = []
    exclude_user_id = "6547992203707764"
    
    for _, row in target_groups.iterrows():
        group_name = row["group_name"]
        group_id = row["group_id"]
        group_detail_url = f"{config['instance']}/api/2.0/preview/scim/v2/Groups/{group_id}"
        detail_resp = requests.get(group_detail_url, headers=headers)
        
        if detail_resp.status_code != 200:
            print(f"âš ï¸ ê·¸ë£¹ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ (group_id={group_id}): {detail_resp.status_code}")
            continue
        
        group_detail = detail_resp.json()
        members = group_detail.get("members", [])
        
        for m in members:
            user_id = m.get("value")
            if user_id and user_id != exclude_user_id:
                user_group_list.append({"user_id": user_id, "group_name": group_name})
    
    # Step 4: DataFrameìœ¼ë¡œ ë³€í™˜ ë° ê·¸ë£¹í•‘
    df_user_groups = pd.DataFrame(user_group_list).drop_duplicates()
    
    df_user_groups = (
        df_user_groups
        .groupby("user_id")["group_name"]
        .apply(lambda x: sorted(set(x)))
        .reset_index()
    )
    
    print(f"âœ… ì‚¬ìš©ìž ê·¸ë£¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_user_groups)} users")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_user_groups', value=df_user_groups.to_json(orient='split'))
    
    return len(df_user_groups)

def enrich_with_groups(**context):
    """
    Task 3: Audit logì— ê·¸ë£¹ ì •ë³´ ë³‘í•©
    """
    ti = context['ti']
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit = pd.read_json(ti.xcom_pull(task_ids='extract_audit_logs', key='df_audit'), orient='split')
    df_user_groups = pd.read_json(ti.xcom_pull(task_ids='get_user_groups', key='df_user_groups'), orient='split')
    
    # ë³‘í•©
    df_audit_with_group = df_audit.merge(df_user_groups, on="user_id", how="left")
    
    # group_nameì´ NaNì¸ í–‰ ì œê±°
    df_audit_with_group = df_audit_with_group.dropna(subset=["group_name"])
    
    print(f"âœ… ê·¸ë£¹ ì •ë³´ ë³‘í•© ì™„ë£Œ: {len(df_audit_with_group)} rows")
    
    context['ti'].xcom_push(key='df_audit_with_group', value=df_audit_with_group.to_json(orient='split', date_format='iso'))
    
    return len(df_audit_with_group)

def get_space_info(**context):
    """
    Task 4: Genie APIë¡œ ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
    spaces_url = f"{config['instance']}/api/2.0/genie/spaces"
    resp = requests.get(spaces_url, headers=headers)
    
    if resp.status_code != 200:
        print(f"âš ï¸ ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        space_id_to_name = {}
    else:
        spaces = resp.json().get("spaces", [])
        space_df = pd.DataFrame([
            {"space_id": s.get("space_id"), "space_name": s.get("title")}
            for s in spaces
        ])
        space_id_to_name = dict(zip(space_df["space_id"], space_df["space_name"]))
    
    print(f"âœ… ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(space_id_to_name)} spaces")
    
    context['ti'].xcom_push(key='space_id_to_name', value=space_id_to_name)
    
    return len(space_id_to_name)

def get_message_details(**context):
    """
    Task 5: Genie APIë¡œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    """
    ti = context['ti']
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit_with_group = pd.read_json(ti.xcom_pull(task_ids='enrich_with_groups', key='df_audit_with_group'), orient='split')
    space_id_to_name = ti.xcom_pull(task_ids='get_space_info', key='space_id_to_name')
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ì´ë¦„ ì¶”ê°€
    df_audit_with_group["space_name"] = df_audit_with_group["space_id"].map(space_id_to_name)
    
    # group_nameì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë¯€ë¡œ anyë¡œ ì²´í¬
    def should_exclude(group_list):
        if isinstance(group_list, list):
            return any(g in exclude_groups for g in group_list)
        return False
    
    df_target = df_audit_with_group[~df_audit_with_group["group_name"].apply(should_exclude)].copy()
    
    # ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    contents = []
    queries = []
    statement_ids = []
    
    total_rows = len(df_target)
    print(f"ðŸ”„ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œìž‘: {total_rows} rows")
    
    for idx, row in df_target.iterrows():
        if idx % 100 == 0:
            print(f"  Progress: {idx}/{total_rows}")
        
        space_id = row["space_id"]
        conversation_id = row["conversation_id"]
        message_id = row["message_id"]
        
        if None in [space_id, conversation_id, message_id]:
            contents.append(None)
            queries.append(None)
            statement_ids.append(None)
            continue
        
        url = f"{config['instance']}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages/{message_id}"
        
        try:
            resp = requests.get(url, headers=headers)
            if resp.status_code == 200:
                data = resp.json()
                
                # Content ì²˜ë¦¬
                content_raw = data.get("content")
                if isinstance(content_raw, str):
                    content = content_raw.replace("\n", " ")
                else:
                    content = str(content_raw) if content_raw is not None else None
                
                # Query ì²˜ë¦¬
                attachments = data.get("attachments", [])
                if attachments and isinstance(attachments, list):
                    query = attachments[0].get("query", {}).get("query", None)
                else:
                    query = None
                
                # Statement ID ì²˜ë¦¬
                statement_id = data.get("query_result", {}).get("statement_id")
                
            else:
                content, query, statement_id = None, None, None
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ ({idx}í–‰): {e}")
            content, query, statement_id = None, None, None
        
        contents.append(content)
        queries.append(query)
        statement_ids.append(statement_id)
    
    # ë°ì´í„° ì¶”ê°€
    df_target['content'] = contents
    df_target['query'] = queries
    df_target['statement_id'] = statement_ids
    
    print(f"âœ… ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_target)} rows")
    
    context['ti'].xcom_push(key='df_target', value=df_target.to_json(orient='split', date_format='iso'))
    
    return len(df_target)


def merge_query_history(**context):
    """
    Task 6: Query historyì™€ ë³‘í•© ë° ìµœì¢… ë°ì´í„° ì €ìž¥
    """
    from databricks import sql
    import pandas as pd
    from io import StringIO
    
    ti = context['ti']
    config = get_databricks_config()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    json_data = ti.xcom_pull(task_ids='get_message_details', key='df_target')
    df_target = pd.read_json(StringIO(json_data), orient='split')
    
    # Databricks SQL ì—°ê²°
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )
    
    cursor = connection.cursor()
    
    # Query history ì¡°íšŒ
    query_history_sql = """
    SELECT 
        statement_id, 
        executed_by, 
        execution_status, 
        CAST(total_duration_ms AS DOUBLE) / 1000 AS query_duration_seconds, 
        CAST(result_fetch_duration_ms AS DOUBLE) / 1000 AS query_result_fetch_duration_seconds, 
        CAST(end_time AS TIMESTAMP) + INTERVAL 9 HOURS AS query_end_time_kst
    FROM system.query.history
    WHERE query_source.genie_space_id IS NOT NULL
        AND statement_type = 'SELECT'
        AND DATE(end_time) >= CURRENT_DATE - INTERVAL 1 DAYS
        AND DATE(end_time) < CURRENT_DATE
    """
    
    cursor.execute(query_history_sql)
    query_df = cursor.fetchall_arrow().to_pandas()
    
    print(f"ðŸ“Š Query history ì¡°íšŒ ì™„ë£Œ: {len(query_df)} rows")
    print(f"ðŸ“Š Query history ì»¬ëŸ¼: {query_df.columns.tolist()}")
    
    # ì»¬ëŸ¼ ì¡´ìž¬ í™•ì¸ ë° rename
    if 'executed_by' in query_df.columns:
        query_df_renamed = query_df.rename(columns={"executed_by": "user_email"})
    else:
        print(f"âš ï¸ Warning: 'executed_by' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        cursor.close()
        connection.close()
        raise KeyError(f"'executed_by' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë³‘í•©
    df_audit_enriched = df_target.merge(
        query_df_renamed[[
            "statement_id", "user_email", "query_end_time_kst", 
            "query_duration_seconds", "query_result_fetch_duration_seconds", "execution_status"
        ]],
        how="left",
        on=["statement_id", "user_email"]
    )
    
    # ì‘ë‹µ ì†Œìš”ì‹œê°„ ê³„ì‚°
    df_audit_enriched["event_time_kst"] = pd.to_datetime(df_audit_enriched["event_time_kst"])
    df_audit_enriched["query_end_time_kst"] = pd.to_datetime(df_audit_enriched["query_end_time_kst"])
    df_audit_enriched["message_response_duration_seconds"] = (
        df_audit_enriched["query_end_time_kst"] - df_audit_enriched["event_time_kst"]
    ).dt.total_seconds()
    
    print(f"âœ… Query history ë³‘í•© ì™„ë£Œ: {len(df_audit_enriched)} rows")
    
    # ===== ê¸°ì¡´ í…Œì´ë¸”ì— ë°ì´í„° INSERT =====
    
    # 1. í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ í™•ì¸
    cursor.execute("DESCRIBE datahub.injoy_ops_schema.injoy_monitoring_data")
    table_schema = cursor.fetchall()
    table_columns = [row[0] for row in table_schema]
    
    print(f"ðŸ“Š ê¸°ì¡´ í…Œì´ë¸” ì»¬ëŸ¼: {table_columns}")
    print(f"ðŸ“Š DataFrame ì»¬ëŸ¼: {df_audit_enriched.columns.tolist()}")
    
    # 2. DataFrame ì»¬ëŸ¼ì„ í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œì— ë§žì¶”ê¸°
    # í…Œì´ë¸”ì— ìžˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_columns = [col for col in table_columns if col in df_audit_enriched.columns]
    df_to_insert = df_audit_enriched[available_columns]
    
    print(f"ðŸ“Š INSERTí•  ì»¬ëŸ¼: {available_columns}")
    
    # 3. ë°ì´í„° íƒ€ìž… ë³€í™˜
    def convert_value(val):
        """Pandas íƒ€ìž…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ìž…ìœ¼ë¡œ ë³€í™˜"""
        if pd.isna(val):
            return None
        elif isinstance(val, pd.Timestamp):
            return val.to_pydatetime()
        else:
            return val
    
    # 4. INSERT ë°ì´í„° ì¤€ë¹„
    data_tuples = []
    for _, row in df_to_insert.iterrows():
        row_tuple = tuple(convert_value(row[col]) for col in available_columns)
        data_tuples.append(row_tuple)
        
    # 5. UPSERT (MERGE) ì¿¼ë¦¬
    columns_str = ', '.join([f"`{col}`" for col in available_columns])
    placeholders = ', '.join(['?' for _ in available_columns])

    # MERGE ì¡°ê±´ (Primary Key ì—­í• ì„ í•  ì»¬ëŸ¼ë“¤)
    # statement_idë¡œ ì¤‘ë³µ ì²´í¬
    merge_key = "statement_id"  # ë˜ëŠ” ì—¬ëŸ¬ ì»¬ëŸ¼: ["statement_id", "user_email"]

    # UPDATEí•  ì»¬ëŸ¼ë“¤ (merge_key ì œì™¸)
    update_columns = [col for col in available_columns if col != merge_key]
    update_set = ', '.join([f"target.`{col}` = source.`{col}`" for col in update_columns])

    # INSERTí•  ì»¬ëŸ¼ë“¤
    insert_columns = ', '.join([f"`{col}`" for col in available_columns])
    insert_values = ', '.join([f"source.`{col}`" for col in available_columns])

    merge_sql = f"""
    MERGE INTO datahub.injoy_ops_schema.injoy_monitoring_data AS target
    USING (
        SELECT {placeholders}
    ) AS source ({columns_str})
    ON target.`{merge_key}` = source.`{merge_key}`
    WHEN MATCHED THEN
        UPDATE SET {update_set}
    WHEN NOT MATCHED THEN
        INSERT ({insert_columns})
        VALUES ({insert_values})
    """

    print(f"ðŸ“ MERGE SQL:\n{merge_sql}")
    print(f"ðŸ“ {len(data_tuples)} rows UPSERT ì¤‘...")

    # 6. Batch MERGE ì‹¤í–‰
    try:
        for data_tuple in data_tuples:
            cursor.execute(merge_sql, data_tuple)
        print("âœ… ë°ì´í„° UPSERT ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë°ì´í„° UPSERT ì‹¤íŒ¨: {e}")
        cursor.close()
        connection.close()
        raise
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Delta í…Œì´ë¸” ì €ìž¥ ì™„ë£Œ: datahub.injoy_ops_schema.injoy_monitoring_data")
    print(f"âœ… ì´ {len(data_tuples)} rows ì¶”ê°€ë¨")
    
    return len(data_tuples)

# Task ì •ì˜
task0 = PythonOperator(
    task_id='tokenize_databricks',
    python_callable=tokenize_databricks,
    op_kwargs={'url': url, 'headers': headers},
    dag=dag,
)

task1 = PythonOperator(
    task_id='extract_audit_logs',
    python_callable=extract_audit_logs,
    dag=dag,
)

task2 = PythonOperator(
    task_id='get_user_groups',
    python_callable=get_user_groups,
    dag=dag,
)

task3 = PythonOperator(
    task_id='enrich_with_groups',
    python_callable=enrich_with_groups,
    dag=dag,
)

task4 = PythonOperator(
    task_id='get_space_info',
    python_callable=get_space_info,
    dag=dag,
)

task5 = PythonOperator(
    task_id='get_message_details',
    python_callable=get_message_details,
    dag=dag,
)

task6 = PythonOperator(
    task_id='merge_query_history',
    python_callable=merge_query_history,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì„¤ì •
task0 >> [task1, task2] >> task3
[task3, task4] >> task5 >> task6