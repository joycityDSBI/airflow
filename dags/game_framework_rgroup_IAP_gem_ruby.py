import time
import pandas as pd
from google.cloud import bigquery
from google import genai
from google.genai import types
from google.cloud import storage
import vertexai
from google.genai import Client
from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore

# ì¸ì¦ê´€ë ¨
import google.auth
from google.auth.transport.requests import Request
import logging

# ê·¸ë˜í”„ ê´€ë ¨ íŒ¨í‚¤ì§€
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter, StrMethodFormatter, PercentFormatter, MultipleLocator
import matplotlib as mpl
import matplotlib.font_manager as fm
from matplotlib import cm
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont # 2ê°€ì§€ íŒŒì¼ í•©ì¹˜ê¸°
import matplotlib.dates as mdates
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio
import IPython.display as IPd
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List, Tuple
from matplotlib import rcParams
from matplotlib.patches import Rectangle

# ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€
import numpy as np
import re
import os 
import math
import time
import pandas as pd
from notion_client import Client
import requests
import json
from datetime import datetime, timezone, timedelta
from adjustText import adjust_text
from airflow.models import Variable
from airflow.operators.python import get_current_context
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
from pathlib import Path
from airflow.sdk import get_current_context
from game_framework_util import *

PROJECT_ID = "data-science-division-216308"
LOCATION = "us-central1"


def rev_group_rev_pu(joyplegameid: int, gameidx: str, bigquery_client, bucket, **context):
    query = f"""
    select logdatekst,Week
    , cast(sum(if(rgroup_final = 'R0', pricekrw, null)) as int64)as R0_Sales
    , cast(sum(if(rgroup_final = 'R1', pricekrw, null)) as int64) as R1_Sales
    , cast(sum(if(rgroup_final = 'R2', pricekrw, null)) as int64) as R2_Sales
    , cast(sum(if(rgroup_final = 'R3', pricekrw, null)) as int64) as R3_Sales
    , cast(sum(if(rgroup_final = 'R4', pricekrw, null)) as int64) as R4_Sales
    , cast(sum(if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ', pricekrw, null)) as int64) as `ì „ì›” ë¬´ê³¼ê¸ˆ_Sales`
    , cast(sum(if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì', pricekrw, null)) as int64) as `ë‹¹ì›”ê°€ì…ì_Sales`
    , cast(sum(pricekrw) as int64) as `ì „ì²´ìœ ì €_Sales`

    , count(distinct if(rgroup_final = 'R0' and pricekrw>0 , authaccountname, null)) as R0_PU
    , count(distinct if(rgroup_final = 'R1' and pricekrw>0 , authaccountname, null)) as R1_PU
    , count(distinct if(rgroup_final = 'R2' and pricekrw>0 , authaccountname, null)) as R2_PU
    , count(distinct if(rgroup_final = 'R3' and pricekrw>0 , authaccountname, null)) as R3_PU
    , count(distinct if(rgroup_final = 'R4' and pricekrw>0 , authaccountname, null)) as R4_PU
    , count(distinct if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ' and pricekrw>0 , authaccountname, null)) as `ì „ì›” ë¬´ê³¼ê¸ˆ_PU`
    , count(distinct if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì' and pricekrw>0 , authaccountname, null)) as `ë‹¹ì›”ê°€ì…ì_PU`
    , count(distinct if(pricekrw>0, authaccountname, null)) as `ì „ì²´ìœ ì €_PU`
    from
    (select *, concat(cast(cast(DATE_TRUNC(logdatekst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdatekst,week(Wednesday)) as date), interval 6 day) as string)) as Week
    from `data-science-division-216308.gameInsightFramework.paymentGroup`
    where logdatekst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        and joypleGameID = {joyplegameid}
    )
    group by 1,2
    order by 1
    """

    query_result =query_run_method(service_sub='4_detail_sales', bigquery_client=bigquery_client, query=query)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)
    
    return saved_path


def rev_group_rev_pu_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, rev_group_rev_pu_path, bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)
    rev_group_rev_pu_data = load_df_from_gcs(bucket, rev_group_rev_pu_path)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}

    response4_RgroupSales = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    1. ì´ë²ˆì£¼ ë§¤ì¶œê³¼ PU íŠ¸ë Œë“œì— ëŒ€í•´, ì§€ë‚œì£¼ì™€ ì§€ì§€ë‚œì£¼ì™€ ë¹„êµí•´ì„œ íŠ¹ë³„í•œ ì ì´ ìˆë‹¤ë©´ ì•Œë ¤ì¤˜.
    ì´í•©ì´ë‚˜ í‰ê· ì ì¸ê²ƒ ë§ê³ ë„ íŠ¸ë Œë“œ ë³€í™”ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜
    2. ì¡´ëŒ“ë§ë¡œ ì¨ì¤˜. 10ì¤„ ë‚´ë¡œ ì¨ì¤˜
    3. ë¶„ì„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ í˜¹ì€ ë¶„ì„í•´ë³´ì•˜ìŠµë‹ˆë‹¤ ë“±ì˜ ë§ì„ ì“°ì§€ë§ê³  ë°”ë¡œ ë¶„ì„í•œ ë‚´ìš©ì— ëŒ€í•´ ì•Œë ¤ì¤˜.
    ì œê³µí•´ì£¼ì‹  ë°ì´í„°ëŠ” ì´ëŸ° ë§ ì“°ì§€ë§ì•„ì¤˜ ë°”ë¡œ ë¶„ì„ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜
    4. ë¹„êµí• ë•Œ, ì´ë²ˆì£¼ê°€ ë‹¤ ì§€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì£¼ì°¨ë„ ë™ì¼í•œ ì¼ìˆ˜ë¥¼ ê°€ì§€ê³  ë¹„êµí•´ì£¼ê³ , ì–´ë–»ê²Œ ë™ì¼ê¸°ê°„ ë¹„êµë˜ì—ˆëŠ”ì§€ë„ ëª…ì‹œí•´ì¤˜.
    5. ë§¤ì¶œì€ ì¼ìë³„ ì´í•©ìœ¼ë¡œ ë¹„êµí•´ë„ ë˜ì§€ë§Œ, PU ëŠ” ê·¸ë‚ ì˜ PU ì´ê¸° ë•Œë¬¸ì— ì´í•©ë³´ë‹¤ëŠ” íŠ¸ë Œë“œë¡œ ë¹„êµí•´ì¤˜.
    6. ë¹„êµí•  ë•ŒëŠ” R0, R1,R2 ê·¸ë£¹ì„ ìœ„ì£¼ë¡œ ë§í•´ì¤˜
    7. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    8. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.e.g : 5100ë§Œì› , 1ì–µ 2ì²œë§Œì›
    9. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    10. í–¥í›„ ì–´ë–»ê²Œ í•´ì•¼ëœë‹¤ëŠ” ë§ì€ í•˜ì§€ ë§ì•„ì¤˜.
    11. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    í•µì‹¬ë‚´ìš©ì€ R0,R1,R2 ì˜ ë§¤ì¶œì´ ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ì§€ê°€ í•„ìš”í•´

    <ì¼ìë³„ ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œì•¡>
    {rev_group_rev_pu_data}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_RgroupSales.text


def iap_gem_ruby(joyplegameid:int, gameidx: str, databaseschema: str, bigquery_client, bucket, **context):
    query = f"""
    select logdate_kst,week
    , cast(sum(if(cat_package2='ì „íˆ¬ê¸°', sales_buygem, 0)) as int64) as `ì „íˆ¬ê¸°`
    , cast(sum(if(cat_package2='ì¢…í•©', sales_buygem, 0)) as int64) as `ì¢…í•©`
    , cast(sum(if(cat_package2='ìì›', sales_buygem, 0)) as int64) as `ìì›`
    , cast(sum(if(cat_package2='í•­ê³µëª¨í•¨', sales_buygem, 0)) as int64) as `í•­ê³µëª¨í•¨`
    , cast(sum(if(cat_package2='ì˜ì›…', sales_buygem, 0)) as int64) as `ì˜ì›…`
    , cast(sum(if(cat_package2='êµ°í•¨', sales_buygem, 0)) as int64) as `êµ°í•¨`
    , cast(sum(if(cat_package2='ë°°í‹€íŒ¨ìŠ¤', sales_buygem, 0)) as int64) as `ë°°í‹€íŒ¨ìŠ¤`
    , cast(sum(if(cat_package2='ì—°êµ¬', sales_buygem, 0)) as int64) as `ì—°êµ¬`
    , cast(sum(if(cat_package2='ì¥ë¹„', sales_buygem, 0)) as int64) as `ì¥ë¹„`
    , cast(sum(if(cat_package2 not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„'), sales_buygem, null)) as int64) as `ê¸°íƒ€`
    from
    (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (
    ### IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        )
    union all
    ### GEM
        (
        select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        )
    union all
    ### RUBY
        (
        select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')
        )
    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1,2 order by 1
    """
    query_result = query_run_method('4_detail_sales', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path


def iap_gem_ruby_history(gameidx: str, bigquery_client, bucket, **context):
    query = f"""
    select *
    from (
        select distinct updateDate `ì—…ë°ì´íŠ¸ì¼`
                        , case when category is null then 'ê¸°íƒ€'
                                when category = 'ì´ë²¤íŠ¸ (ìš´ì˜íˆ´)' then 'ì´ë²¤íŠ¸'
                                else category end as `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜`
                        , title as `ì—…ë°ì´íŠ¸ ë‚´ìš©`
        from `data-science-division-216308.gameInsightFramework.{gameidx}_history`
        where date(updateDate)>= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 32 DAY)
        and date(updateDate)<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        and (title is not null or title != '' or title != ' ')
        )
    where `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜` not in ('ê¸°íƒ€', 'ì ê²€ ê¸°ë³¸ ì •ë³´', 'LQA', 'ë²„ê·¸ ìˆ˜ì • ë° ì‚¬ìš©ì„± ê°œì„ ', 'BM_ìƒì ')
    order by `ì—…ë°ì´íŠ¸ì¼`desc
    """

    query_result = query_run_method('4_detail_sales', bigquery_client=bigquery_client, query=query)
    # 1ì£¼ì „ ìˆ˜ìš”ì¼ë¶€í„° ì–´ì œì¼ìê¹Œì§€ì˜ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ (ì¿¼ë¦¬ì—ì„œ ì „ì²˜ë¦¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”í›„ ìˆ˜ì • í•„ìš”)

    # ë¬¸ìì—´ ì»¬ëŸ¼ -> datetimeìœ¼ë¡œ íŒŒì‹± (ì‹¤íŒ¨í•œ ê°’ì€ NaT)
    s = pd.to_datetime(query_result['ì—…ë°ì´íŠ¸ì¼'], errors='coerce')

    # í•œêµ­ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜/ì „ì¼
    today = pd.Timestamp.now(tz='Asia/Seoul').normalize().date()
    yesterday = today - pd.Timedelta(days=1)

    # ì˜¤ëŠ˜ ê¸°ì¤€ 'ì§ì „ ìˆ˜ìš”ì¼'(ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ ì˜¤ëŠ˜ ì œì™¸) ê³„ì‚°
    # ì›”=0, í™”=1, ìˆ˜=2, ... ì¼=6
    w = today.weekday()
    delta_to_last_wed = (w - 2) % 7
    if delta_to_last_wed == 0:  # ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ 7ì¼ ì „ì„ 'ì§ì „ ìˆ˜ìš”ì¼'ë¡œ
        delta_to_last_wed = 7
    last_wed = today - pd.Timedelta(days=delta_to_last_wed)

    # ğŸ‘‰ "1ì£¼ ì „ ìˆ˜ìš”ì¼"ì„ ì‹œì‘ì¼ë¡œ
    start_date = last_wed

    # êµ¬ê°„: [1ì£¼ ì „ ìˆ˜ìš”ì¼, ì „ì¼] (ì–‘ë í¬í•¨)
    mask = (s.dt.date >= start_date) & (s.dt.date <= yesterday)
    query_result4_ingameHistory = query_result.loc[mask].copy()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result4_ingameHistory, bucket, gcs_path)

    return saved_path


def iap_gem_ruby_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_iapgemruby, path_iapgemruby_history, bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    query_result4_salesByPackage = load_df_from_gcs(bucket, path_iapgemruby)
    query_result4_ingameHistory = load_df_from_gcs(bucket, path_iapgemruby_history)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}

    response4_salesByPackage = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{query_result4_salesByPackage.to_csv(index=False)}

    ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {query_result4_ingameHistory.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage.text


def iap_df(gameidx:str, databaseschema: str, bigquery_client, bucket, **context):
    # IAP
    query = f"""
    WITH base AS (
    SELECT *
    FROM `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    WHERE logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(pricekrw) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (
            SELECT cat_package, sum(rev) AS peak_rev
            FROM daily
            GROUP BY 1
        )
    ORDER BY peak_rev DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
        d.logdate_kst,
        IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4

    """

    query_result =query_run_method('4_detail_sales', bigquery_client, query)
    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ Pivot

    query_result4_salesByPackage_IAP = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result4_salesByPackage_IAP, bucket, gcs_path)

    return saved_path


def gem_df(joyplegameid: int, gameidx:str, bigquery_client, bucket, **context):
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='gem' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS usegem
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(usegem) AS peak_usegem
        FROM daily
        GROUP BY 1)
    ORDER BY peak_usegem DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.usegem) AS usegem
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result = query_run_method('4_detail_sales', bigquery_client, query)

    query_result4_salesByPackage_GEM = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="usegem",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result4_salesByPackage_GEM, bucket, gcs_path)

    return saved_path


def ruby_df(joyplegameid: int, gameidx:str, bigquery_client, bucket, **context):
    
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='ruby' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS useruby
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(useruby) AS peak_useruby
        FROM daily
        GROUP BY 1)
    ORDER BY peak_useruby DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.useruby) AS useruby
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result =query_run_method('4_detail_sales', bigquery_client, query)

    query_result4_salesByPackage_RUBY = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="useruby",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result4_salesByPackage_RUBY, bucket, gcs_path)

    return saved_path


def iap_df_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_iap_df, path_iapgemruby_history, bucket, PROJECT_ID, LOCATION, **context):
    
    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    iap_df = load_df_from_gcs(bucket, path_iap_df)
    iap_gem_ruby_history = load_df_from_gcs(bucket, path_iapgemruby_history)
    
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response4_salesByPackage_IAP = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{iap_df.to_csv(index=False)}

    ì´ë²ˆì£¼ IAP ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_IAP.text


def gem_df_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_gem_df, path_iapgemruby_history, bucket, PROJECT_ID, LOCATION, **context):
    
    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    gem_df = load_df_from_gcs(bucket, path_gem_df)
    iap_gem_ruby_history = load_df_from_gcs(bucket, path_iapgemruby_history)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response4_salesByPackage_GEM = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ì ¬ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{gem_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ì ¬ ì†Œëª¨ëŸ‰ì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ì ¬ì†Œë¹„ëŸ‰ì„ ì²«ë²ˆì§¸ ìë¦¬ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ì ¬ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ì ¬ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_GEM.text


def ruby_df_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_ruby_df, path_iapgemruby_history, bucket, PROJECT_ID, LOCATION, **context):
    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    ruby_df = load_df_from_gcs(bucket, path_ruby_df)
    iap_gem_ruby_history = load_df_from_gcs(bucket, path_iapgemruby_history)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response4_salesByPackage_RUBY = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ë£¨ë¹„ ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{ruby_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë£¨ë¹„ ì†Œëª¨ëŸ‰ ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
    ë‹¨ìœ„ëŠ” ì›ì´ ì•„ë‹ˆë¼ ë£¨ë¹„ë¡œ í‘œê¸°í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ í•œìë¦¬ ìˆ˜ ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë£¨ë¹„ ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_RUBY.text


def weekly_iapcategory_rev(joyplegameid: int, gameidx: str, databaseschema:str, bigquery_client, bucket, **context):
    
    query = f"""
    with base as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (
        ## IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
        )
    )

    , daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package2, SUM(sales_buygem) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package2
    FROM
        (SELECT cat_package2, sum(rev) AS peak_rev
        FROM daily
        GROUP BY 1)
    ORDER BY peak_rev DESC, cat_package2 ASC
    LIMIT 15
    )

    SELECT d.logdate_kst,
        IF(d.cat_package2 IN (SELECT cat_package2 FROM top_cat), d.cat_package2, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2
    ORDER BY 1,2


    """

    query_result =query_run_method('4_detail_sales', bigquery_client, query)

    query_result4_salesByCategory = query_result.pivot_table(
        index=["logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    exclude = {'logdate_kst', 'ê¸°íƒ€'}
    cols = [c for c in query_result4_salesByCategory.columns if c not in exclude]

    # ì‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì•ˆì „ ì²˜ë¦¬
    def sq(c: str) -> str:
        return "'" + c.replace("'", "''") + "'"

    query_result4_salesByCategory_Cols = ", ".join(sq(c) for c in cols)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path_1 = f"{gameidx}/{timestamp}_1.parquet"
    gcs_path_2 = f"{gameidx}/{timestamp}_2.parquet"
        
    saved_path_1 = save_df_to_gcs(query_result4_salesByCategory, bucket, gcs_path_1)

    cols_df = pd.DataFrame({'columns': cols})
    saved_path_2 = save_df_to_gcs(cols_df, bucket, gcs_path_2)

    return saved_path_1, saved_path_2


def iapcategory_rev_df_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_weekly_iapcategory_rev, bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    weekly_iapcategory_rev = load_df_from_gcs(bucket, path_weekly_iapcategory_rev)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}

    response4_salesByCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ì§€ë‚œ ì—…ë°ì´íŠ¸ì¼ë¶€í„° ì „ì¼ìê¹Œì§€ì˜ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ë‹¤ìŒì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•´ì„œ ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ê³ , ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í–ˆëŠ”ì§€ë¥¼ í™•ì¸í•´ì¤˜.\n{weekly_iapcategory_rev.to_csv(index=False)}
    ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ëŠ”ì§€ëŠ” ìƒìœ„ 3ê°œë§Œ ì•Œë ¤ì£¼ê³ , ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í•œ ì¹´í…Œê³ ë¦¬ì—ì„œëŠ” ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸í•˜ê³  ì•Œë ¤ì¤˜.
    ì œì–¸ì€ í•˜ì§€ ë§ì•„ì¤˜.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    response4_CategoryListUp = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {response4_salesByCategory.text}\nìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ì—ì„œ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜.
    ('ê³¨ë“œ', 'ì€í™”', 'ì²­ì‚¬ì§„', 'ì „ìˆ  êµë³¸', 'ìì›')
    """,
        config=types.GenerateContentConfig(
            temperature=0
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # evalë¡œ ë¬¸ìì—´ì„ ì‹¤ì œ tupleë¡œ ë³€í™˜
        CategoryListUp_2 = eval(CategoryListUp.group(0))
        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_2])
    # SQL order by ì‹œ, ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•œ ì½”ë“œ
    case_when_str = "\n".join(
        [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_2)]
    )

    return CategoryListUp_SQL, case_when_str, response4_salesByCategory, response4_CategoryListUp



def top3_items_by_category(joyplegameid: int, gameidx:str, service_sub: str, databaseschema: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, 
                           path_weekly_iapcategory_rev_cols, path_weekly_iapcategory_rev, bigquery_client,
                           bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)
    
    weekly_iapcategory_rev_cols = load_df_from_gcs(bucket, path_weekly_iapcategory_rev_cols)
    print(f"top3 items by category ì—ì„œ weekly_iapcategory_rev_cols : ", weekly_iapcategory_rev_cols)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    

    print(f"top3 items by category ì—ì„œ íƒ€ì… : ", type(service_sub))
    CategoryListUp_SQL, case_when_str, _, _ = iapcategory_rev_df_gemini(service_sub=str(service_sub), 
                                                                        genai_client=genai_client, 
                                                                        MODEL_NAME=MODEL_NAME, 
                                                                        SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION, 
                                                                        path_weekly_iapcategory_rev=path_weekly_iapcategory_rev,
                                                                        bucket=bucket, 
                                                                        PROJECT_ID=PROJECT_ID, 
                                                                        LOCATION=LOCATION,
                                                                        **context)
    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 3 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 3 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ({weekly_iapcategory_rev_cols}) then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.GW.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
            )
            )
    where cat_package2 in ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    where `ìƒí’ˆ ì´ë¦„` != 'ê·¸ ì™¸ ìƒí’ˆë“¤'
    group by `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc

    """

    query_result=query_run_method(service_sub, bigquery_client, query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path



def top3_items_by_category_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, 
                                  path_top3_items_by_category, path_weekly_iapcategory_rev, path_iapgemruby_history,
                                  bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)
    query_result4_salesByPackage_ListedCategory = load_df_from_gcs(bucket, path_top3_items_by_category)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}

    _, _, response4_salesByCategory = iapcategory_rev_df_gemini(service_sub, 
                                                                        genai_client, 
                                                                        MODEL_NAME, 
                                                                        SYSTEM_INSTRUCTION, 
                                                                        path_weekly_iapcategory_rev,
                                                                        bucket, 
                                                                        PROJECT_ID, 
                                                                        LOCATION,
                                                                        **context)

    response4_salesByPackage_ListedCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {query_result4_salesByPackage_ListedCategory.to_csv(index=False)}
    ìœ„ì˜ ë°ì´í„°ëŠ” ë§¤ì¶œ ìƒìœ„ 3ìœ„ ì¹´í…Œê³ ë¦¬ ë° ë§¤ì¶œ ë³€ë™ì´ ë†’ì•˜ë˜ ì¹´í…Œê³ ë¦¬ë“¤ì˜ ìƒí’ˆë³„ ë§¤ì¶œ ë°ì´í„°ì•¼.
    \n{response4_salesByCategory.text}
    ê·¸ë¦¬ê³  ìœ„ì˜ ë°ì´í„°ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ë³€í™” ìš”ì•½í•œ ë‚´ìš©ì´ì•¼.
    ë‘ ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œ,
    ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ê° ì¼ìë³„ë¡œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€(ëª¨ë“  ë‚ ì§œë¥¼ ì°¸ì¡°í•´ì¤˜),
    ê·¸ ì™¸ ì¹´í…Œê³ ë¦¬ë“¤ì€ ë§¤ì¶œ ë³€ë™ì´ í° ë‚ ì§œì—ë§Œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€ ë¶„ì„í•´ì¤˜.
    ì œì‹œëœ ë°ì´í„°ë§Œìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ì•„ì¤˜.
    ì£¼ì–´ì§„ ë°ì´í„°ê°€ í•˜ë£¨ë§Œ ìˆë‹¤ëŠ” ìœ ì˜ì‚¬í•­ì€ ë§í•˜ì§€ë§ˆ.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.1
            ,labels=LABELS
            # max_output_tokens=9000
        )
    )

    response4_WeeklySales_Draft1 = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
        ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì§€í‘œì— ëŒ€í•œ ìš”ì•½ê¸€ì´ì•¼.\n{response4_salesByCategory.text}
        ê·¸ë¦¬ê³  ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¶œì— ê¸°ì—¬í•œ ìƒí’ˆì— ëŒ€í•œ ì •ë³´ê¸€ì´ì•¼.\n{response4_salesByPackage_ListedCategory.text}
        ë‘ ê¸€ì„ ì¢…í•©í•´ì„œ, ê¸°ê°„ë™ì•ˆ ê²Œì„ì˜ ë§¤ì¶œ ë³€í™”ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.
        ë¦¬í¬íŠ¸ ì‘ì„±ì‹œ ë°˜ë“œì‹œ ì•„ë˜ì˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.

        ì„œë‘ì—ëŠ” 'ê¸ˆì£¼ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸ ë¦¬í¬íŠ¸ (ë°ì´í„° ê¸°ê°„)'ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. Bold ì²˜ë¦¬í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬(ë§¤ì¶œ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì¸ ê²½ìš°, ìˆœìœ„ ì–¸ê¸‰. e.g. ì „íˆ¬ê¸° (ë§¤ì¶œ 1ìœ„). Bold ì²˜ë¦¬í•´ì¤˜.)
        * ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”. ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. í° ë³€í™”ê°€ ìˆê±°ë‚˜ ë§¤ì¶œì´ ë†’ì•˜ë˜ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        * í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ, ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. ì£¼ìš” ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        'ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”'ì™€ 'í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ'ì€ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ 6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

        'ì ¬'ê³¼ 'ë£¨ë¹„'ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì•¼. ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì·¨ê¸‰í•˜ì§€ë§ˆ.
        ìƒí’ˆëª…ì— ìˆëŠ” ë‚ ì§œë¡œ ìƒí’ˆëª…ì˜ ì¶œì‹œëœ ë‚ ì§œë¥¼ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì´ ë§¤ì¶œì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì œì™¸í•œ ì´ë²¤íŠ¸ ë° í”„ë¡œëª¨ì…˜ì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.5
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    iap_gem_ruby_history = load_df_from_gcs(bucket, path_iapgemruby_history)

    response4_WeeklySales_Report = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ë‹¤ìŒì€ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ì•¼.\n{iap_gem_ruby_history.to_csv(index=False)}
    ë‹¤ìŒì˜ ì§€ë‚œ ì—…ë°ì´íŠ¸ ì´í›„ ë§¤ì¶œ ë¶„ì„ ë¦¬í¬íŠ¸ì—ì„œ, ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì´ ìˆëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë‚˜ ìƒí’ˆì´ ìˆë‹¤ë©´ í•´ë‹¹ ì—…ë°ì´íŠ¸ ë‚´ìš©ê³¼ ì—°ê´€ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ì–¸ê¸‰í•´ì¤˜.\n{response4_WeeklySales_Draft1.text}
    ì£¼ì–´ì§„ ë¶„ì„ ë¦¬í¬íŠ¸ì˜ í˜•ì‹ì— ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì—…ë°ì´íŠ¸ ê´€ë ¨ ë‚´ìš©ë§Œ ì–¸ê¸‰ì„ ì¶”ê°€í•˜ëŠ” ì‹ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜. e.g.*   **ì—…ë°ì´íŠ¸ ì—°ê´€ì„±:**
    ìƒí’ˆëª…ì˜ ë‚ ì§œê°€ ë“¤ì–´ê°€ ìˆë‹¤ëŠ” ì‚¬ì‹¤ë§Œìœ¼ë¡œ ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì„±ì´ ìˆë‹¤ê³  ì¶”ë¡ í•˜ì§€ ë§ˆ.
    ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ë§ì€ í•˜ì§€ë§ˆ.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    return response4_WeeklySales_Report.text


def rgroup_top3_pu(joyplegameid:int, gameidx:str, databaseschema:str, bigquery_client, bucket, **context):
    query = f"""
        with raw as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
        , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

        , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
        , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
        , DATE_SUB(
                    date_add(current_date('Asia/Seoul'),interval -1 day),
                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                    ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
        from
        (
        ### IAP
        (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , cast(price_sheet as int64) as price_sheet
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
        union all
        ### GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , cast(price_sheet as int64)*(1500/40) as price_sheet
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend')
        union all
        ### RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')

        )
        where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        ),

        sales_raw as ( ## 5331039
        select *  , format_date('%Y-%m',  logdatekst ) as month
        from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
        where joyplegameid = {joyplegameid}
        and logdatekst>='2025-01-01'
        ),


        monthly_rev as (
        select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
        from
        (select *
        , format_date('%Y-%m-01',  logdatekst ) as logmonth
        , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
        from sales_raw
        where logdatekst>='2025-01-01')
        group by 1,2,3
        ),

        r_group as (
        select *
        , case
        when rev>=10000000 then 'R0'
        when rev>=1000000  then 'R1'
        when rev>=100000   then 'R2'
        when rev>=10000    then 'R3'
        when rev>=1        then 'R4'
        # when rev=0         then 'nonPU'
        else 'ETC' end as rgroup
        from monthly_rev
        where rev>0
        ),

        raw2 as (
        select a.*,month_key
        , case
        when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
        when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
        else b.rgroup end as rgroup_final

        from
        (select * , format_date('%Y-%m',  week_start ) as month2
        from raw) as a

        left join
        (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
        from r_group
        ) as b

        on a.authaccountname = b.authaccountname
        and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
        ),

        raw3 as (

        select *
        , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
        , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
        from
        (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
        , count(distinct authaccountname) as PU
        , cast(sum(sales_buygem) as int64) as sales
        from raw2
        where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
        and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
        group by 1,2,3,4,5,6)
        )

        select *
        from raw3
        where rgroup_final is not null
        and pu_rank in (1,2,3)
        order by rgroup_final, pu_rank
    """

    query_result = query_run_method('4_detail_sales', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path


def rgroup_top3_rev(joyplegameid:int, gameidx:str, databaseschema:str, bigquery_client, bucket, **context):
    query = f"""

    with raw as (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    , DATE_SUB(
                date_add(current_date('Asia/Seoul'),interval -1 day),
                INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
    from
    (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , cast(price_sheet as int64) as price_sheet
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
    union all
    ### GEM
    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , cast(price_sheet as int64)*(1500/40) as price_sheet
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend')
    union all
    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend')

    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    sales_raw as ( ## 5331039
    select *  , format_date('%Y-%m',  logdatekst ) as month
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2025-01-01'
    ),


    monthly_rev as (
    select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
    from
    (select *
    , format_date('%Y-%m-01',  logdatekst ) as logmonth
    , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    from sales_raw
    where logdatekst>='2025-01-01')
    group by 1,2,3
    ),

    r_group as (
    select *
    , case
    when rev>=10000000 then 'R0'
    when rev>=1000000  then 'R1'
    when rev>=100000   then 'R2'
    when rev>=10000    then 'R3'
    when rev>=1        then 'R4'
    # when rev=0         then 'nonPU'
    else 'ETC' end as rgroup
    from monthly_rev
    where rev>0
    ),

    raw2 as (
    select a.*,month_key
    , case
    when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
    when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
    else b.rgroup end as rgroup_final

    from
    (select * , format_date('%Y-%m',  week_start ) as month2
    from raw) as a

    left join
    (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
    from r_group
    ) as b

    on a.authaccountname = b.authaccountname
    and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
    ),

    raw3 as (

    select *
    , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
    , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
    from
    (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
    , count(distinct authaccountname) as PU
    , cast(sum(sales_buygem) as int64) as sales
    from raw2
    where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
    and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
    group by 1,2,3,4,5,6)
    )

    select *
    from raw3
    where rgroup_final is not null
    and sales_rank in (1,2,3)
    order by rgroup_final, sales_rank
    """

    query_result = query_run_method('4_detail_sales', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path


def rgroup_top3_gemini(service_sub: str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list, path_rgroup_top3_rev, path_rgroup_top3_pu, bucket, PROJECT_ID, LOCATION, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    query_result4_thisWeekSalesTop3 = load_df_from_gcs(bucket, path_rgroup_top3_rev)
    query_result4_thisWeekPUTop3 = load_df_from_gcs(bucket, path_rgroup_top3_pu)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}


    response4_thisWeekRgroup = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    ì´ë²ˆì£¼ Rê·¸ë£¹ë³„ PU top3 , ë§¤ì¶œ top3 ìƒí’ˆë“¤ ì •ë³´ë¥¼ ì¤„ê²Œ
    ìƒìœ„ ê³¼ê¸ˆê·¸ë£¹ê³¼ í•˜ìœ„ê³¼ê¸ˆê·¸ë£¹ ê°„ì˜ ì°¨ì´ì—ëŒ€í•´ì„œë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜

    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ


    < Rê·¸ë£¹ë³„ PU top3 ìƒí’ˆ>
    {query_result4_thisWeekPUTop3}


    < Rê·¸ë£¹ë³„ ë§¤ì¶œ top3 ìƒí’ˆ>
    {query_result4_thisWeekSalesTop3}



    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_thisWeekRgroup.text



def category_for_bigquery_sql(service_sub:str, path_weekly_iapcategory_rev:str, 
                              genai_client, MODEL_NAME, SYSTEM_INSTRUCTION, bucket, **context):

    _, _, _, response4_CategoryListUp = iapcategory_rev_df_gemini(service_sub, 
                                                                        genai_client, 
                                                                        MODEL_NAME, 
                                                                        SYSTEM_INSTRUCTION, 
                                                                        path_weekly_iapcategory_rev,
                                                                        bucket, 
                                                                        PROJECT_ID, 
                                                                        LOCATION,
                                                                        **context)

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # ë¬¸ìì—´ì„ ì‹¤ì œ tuple/listë¡œ ë³€í™˜ (ì•ˆì „)
        CategoryListUp_2 = eval(CategoryListUp.group(0))

        # ì• 3ê°œë§Œ ì¶”ì¶œ
        CategoryListUp_Top3 = list(CategoryListUp_2)[:3]

        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜: 'ë°°í‹€íŒ¨ìŠ¤', 'êµ°í•¨', 'ì „íˆ¬ê¸°'
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_Top3])

        # SQL ORDER BYìš© CASE WHEN ... THEN ...
        case_when_str = "\n".join(
            [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_Top3)]
        )
    return CategoryListUp_SQL, case_when_str, CategoryListUp_Top3


def top3_items_rev(joyplegameid:int, gameidx:str, databaseschema:str, service_sub:str, 
                   path_weekly_iapcategory_rev:str, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION:list,
                   bigquery_client, bucket, **context):
    
    CategoryListUp_SQL, case_when_str, _ = category_for_bigquery_sql(service_sub=service_sub,
                                                                    path_weekly_iapcategory_rev=path_weekly_iapcategory_rev,
                                                                    genai_client=genai_client,
                                                                    MODEL_NAME=MODEL_NAME,
                                                                    SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                                                                    bucket=bucket,
                                                                    **context)

    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 5 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 5 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , case when CountryCode = 'KR' then '1.KR' when CountryCode = 'US' then '2.US' else '3.ETC' end as CountryCat
            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„') then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
                (
                    select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst, CountryCode
                        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
                        , cast(price_sheet as int64) as price_sheet
                        , pricekrw as sales_usegem, pricekrw as sales_buygem
                from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
                where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind
                        , cast(price_sheet as int64)*(1500/40) as price_sheet
                        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='gem' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
                        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='ruby' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )
            )
            )
    where cat_package2 in  ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    group by `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc
    """
    query_result = query_run_method('4_detail_sales', bigquery_client, query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    cats = [re.sub(r"^[\"'â€™â€˜`]+|[\"'â€™â€˜`]+$", "", t.strip())
        for t in CategoryListUp_SQL.split(",") if t.strip()]

    # 2) ìˆœì„œëŒ€ë¡œ í•„í„°ë§í•´ì„œ ìƒˆ DF ìƒì„±
    category_col = "ìƒí’ˆ ì¹´í…Œê³ ë¦¬"

    dfs = {}  # ì‚¬ì „ìœ¼ë¡œ ë³´ê´€: {"query_result4_salesByPackage_forGraph_1": df1, ...}
    for i, c in enumerate(cats, start=1):
        key = f"query_result4_salesByPackage_forCategoryGraph_{i}"
        dfs[key] = query_result[
            query_result[category_col] == c
        ].copy()

    return dfs, saved_path


def rgroup_rev_draw(gameidx: str, gcs_path:str, bucket, **context):
    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales = load_df_from_gcs(bucket, gcs_path)

    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales.iloc[:, [0,2,3,4,5,6,7,8]]

    ##
    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales2_salesGraph.rename(
        columns = {"R0_Sales" : "R0",
                "R1_Sales" : "R1",
                "R2_Sales" : "R2",
                "R3_Sales" : "R3",
                "R4_Sales" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_Sales" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_Sales" : "ë‹¹ì›”ê°€ì…ì"}
    )


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_salesGraph["logdatekst"]
    y = query_result4_RgroupSales2_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" Rê·¸ë£¹ë³„ ë§¤ì¶œ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_salesGraph = "graph4_RgroupSales_salesGraph.png"
    
    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    plt.savefig(file_path4_RgroupSales_salesGraph, dpi=160, bbox_inches='tight') # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_salesGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_salesGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_salesGraph)

    return f'{gameidx}/{file_path4_RgroupSales_salesGraph}'


def rgroup_pu_draw(gameidx: str, path_rgroup_pu_rev:str, bucket, **context):
    
    query_result4_RgroupSales =load_df_from_gcs(bucket, path_rgroup_pu_rev)

    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales.iloc[:, [0,10,11,12,13,14,15,16]]

    ##
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales2_puGraph.rename(
        columns = {"R0_PU" : "R0",
                "R1_PU" : "R1",
                "R2_PU" : "R2",
                "R3_PU" : "R3",
                "R4_PU" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_PU" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_PU" : "ë‹¹ì›”ê°€ì…ì"}
    )

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_puGraph["logdatekst"]
    y = query_result4_RgroupSales2_puGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_puGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" (Rê·¸ë£¹ë³„ PU ìˆ˜ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_puGraph = "graph4_RgroupSales_puGraph.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_RgroupSales_puGraph, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_puGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_puGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_puGraph)

    return f'{gameidx}/{file_path4_RgroupSales_puGraph}'


def merge_rgroup_graph(gameidx: str, path_group_rev_pu:str, bucket, **context):
    p1 = rgroup_rev_draw(gameidx, path_group_rev_pu, bucket)
    p2 = rgroup_pu_draw(gameidx, path_group_rev_pu, bucket)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA")


    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph4_RgroupSales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path



def iap_gem_ruby_graph_draw(gameidx: str, path_iap_gem_ruby:str, bucket, **context):
    
    query_result4_salesByPackage = load_df_from_gcs(bucket, path_iap_gem_ruby)
    query_result4_salesByPackage_salesGraph = query_result4_salesByPackage.iloc[:, [0,2,3,4,5,6,7,8,9,10,11]]

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP+ìœ ê°€ì ¬+ìœ ê°€ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage = "graph4_salesByPackage.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage}')
    blob.upload_from_filename(file_path4_salesByPackage)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage)

    return f'{gameidx}/{file_path4_salesByPackage}'



def iap_gem_ruby_IAP_graph_draw(gameidx: str, path_iap_df:str, bucket, **context):
    
    query_result4_salesByPackage_IAP = load_df_from_gcs(bucket, path_iap_df)

    # ... (ìœ„ ë°ì´í„° ì¤€ë¹„Â·í°íŠ¸ ë¶€ë¶„ ë™ì¼)
    query_result4_salesByPackage_IAP_salesGraph = query_result4_salesByPackage_IAP.iloc[:, (query_result4_salesByPackage_IAP.columns != 'month') & (query_result4_salesByPackage_IAP.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_IAP_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_IAP_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_IAP_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_IAP = "graph4_salesByPackage_IAP.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_IAP, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_IAP}')
    blob.upload_from_filename(file_path4_salesByPackage_IAP)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_IAP)

    return f'{gameidx}/{file_path4_salesByPackage_IAP}'


def iap_gem_ruby_GEM_graph_draw(gameidx: str, path_gem_df:str, bucket, **context):

    query_result4_salesByPackage_GEM = load_df_from_gcs(bucket, path_gem_df)
    query_result4_salesByPackage_GEM_salesGraph = query_result4_salesByPackage_GEM.iloc[:, (query_result4_salesByPackage_GEM.columns != 'month') & (query_result4_salesByPackage_GEM.columns != 'week')]

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_GEM_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_GEM_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_GEM_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ì ¬) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_GEM = "graph4_salesByPackage_GEM.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_GEM, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_GEM}')
    blob.upload_from_filename(file_path4_salesByPackage_GEM)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_GEM)

    return f'{gameidx}/{file_path4_salesByPackage_GEM}'
    


def iap_gem_ruby_RUBY_graph_draw(gameidx: str, path_ruby_df:str, bucket, **context):

    query_result4_salesByPackage_RUBY = load_df_from_gcs(bucket, path_ruby_df)
    query_result4_salesByPackage_RUBY_salesGraph = query_result4_salesByPackage_RUBY.iloc[:, (query_result4_salesByPackage_RUBY.columns != 'month') & (query_result4_salesByPackage_RUBY.columns != 'week')]

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_RUBY_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_RUBY_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_RUBY_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_RUBY = "graph4_salesByPackage_RUBY.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_RUBY, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_RUBY}')
    blob.upload_from_filename(file_path4_salesByPackage_RUBY)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_RUBY)

    return f'{gameidx}/{file_path4_salesByPackage_RUBY}'


### 1ìœ„
def top1_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, bigquery_client, bucket, **context):

    dfs, _ = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category1 = "graph4_salesByPackage_Category1.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category1, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category1}')
    blob.upload_from_filename(salesByPackage_Category1)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category1)

    return f'{gameidx}/{salesByPackage_Category1}'


### 2ìœ„
def top2_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, bigquery_client, bucket, **context):

    dfs, _ = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category2 = "graph4_salesByPackage_Category2.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category2, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category2}')
    blob.upload_from_filename(salesByPackage_Category2)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category2)

    return f'{gameidx}/{salesByPackage_Category2}'



### 3ìœ„
def top3_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, bigquery_client, bucket, **context):

    dfs, _ = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category3 = "graph4_salesByPackage_Category3.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category3, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category3}')
    blob.upload_from_filename(salesByPackage_Category3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category3)

    return f'{gameidx}/{salesByPackage_Category3}'


def rgroup_pu_top3_graph_draw(gameidx: str, path_rgroup_top3_pu:str, bucket, **context):
    
    # rgroup_top3_pu
    query_result4_thisWeekPUTop3 =load_df_from_gcs(bucket, path_rgroup_top3_pu)

    df = query_result4_thisWeekPUTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekPUTop3 = "graph4_thisWeekPUTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekPUTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekPUTop3}')
    blob.upload_from_filename(file_path4_thisWeekPUTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekPUTop3)

    return f'{gameidx}/{file_path4_thisWeekPUTop3}'


def rgroup_rev_top3_graph_draw(gameidx: str, path_rgroup_top3_rev:str, bucket, **context):

    query_result4_thisWeekRevTop3 = load_df_from_gcs(bucket, path_rgroup_top3_rev)

    df = query_result4_thisWeekRevTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekSalesTop3 = "graph4_thisWeekRevTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekSalesTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekSalesTop3}')
    blob.upload_from_filename(file_path4_thisWeekSalesTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekSalesTop3)

    return f'{gameidx}/{file_path4_thisWeekSalesTop3}'


def rgroup_rev_upload_notion(gameidx: str, path_rev_group_rev_pu, rev_group_rev_pu_path, service_sub, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION, notion, bucket, headers_json, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "4. ì´ë²ˆì£¼ ìƒì„¸ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(1) Rê·¸ë£¹ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": " ** ì „ì›” ê³¼ê¸ˆì•¡ ê¸°ì¤€ Rê·¸ë£¹ ì…ë‹ˆë‹¤. \n ** ì£¼ì°¨ë³„ ê¸°ì¤€ì€ ìˆ˜ìš”ì¼~í™”ìš”ì¼ ì…ë‹ˆë‹¤. " }}]
                },
            }
        ],
    )

    try:
        gcs_path = f'{gameidx}/graph4_RgroupSales.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_RgroupSales.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = headers_json
    try:
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
 
        # 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        try:
            # [ìˆ˜ì •] headers=headers_upload ëŒ€ì‹  headers=headers_send ë¥¼ ì‚¬ìš©
            send_resp = requests.post(send_url, headers=headers_send, files=files) 
            send_resp.raise_for_status()
            print(f"âœ… NOTION ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ì‘ì—… ì‹¤íŒ¨ : {e}")
            # ì‹¤íŒ¨ ì‹œ ì‘ë‹µ ë‚´ìš©ì„ í™•ì¸í•˜ë©´ ë””ë²„ê¹…ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.
            if hasattr(e, 'response') and e.response is not None:
                print(f"ì˜¤ë¥˜ ì‘ë‹µ: {e.response.text}")
            raise e

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_RgroupSales = load_df_from_gcs(bucket, path_rev_group_rev_pu)

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_RgroupSales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„

    blocks = md_to_notion_blocks(rev_group_rev_pu_gemini(service_sub,
                                                         genai_client,
                                                         MODEL_NAME,
                                                         SYSTEM_INSTRUCTION,
                                                         rev_group_rev_pu_path,
                                                         bucket,
                                                         PROJECT_ID,
                                                         LOCATION,
                                                         **context
                                                        )
                                                    )

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(2) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ë°ì´í„° ê¸°ì¤€ : IAP êµ¬ë§¤ - IAP ì ¬êµ¬ë§¤ - IAP ë£¨ë¹„êµ¬ë§¤ + ìœ ê°€ì ¬ ì‚¬ìš©ë‚´ì—­ + ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ì ¬ê³¼ ë£¨ë¹„ë¡œ ì–´ë–¤ ìƒí’ˆì„ êµ¬ë§¤í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´, IAPë¡œ ì ¬ê³¼ ë£¨ë¹„ë¥¼ êµ¬ë§¤í•œ ê²ƒì€ ì œê±°í•œ í›„ ìœ ê°€ì ¬/ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ì„ ë§¤ì¶œë¡œ ì§‘ê³„í•˜ì˜€ìŠµë‹ˆë‹¤." }}]
                },
            }
        ],
    )



def iap_gem_ruby_upload_notion(gameidx: str, joyplegameid: int, databaseschema: str,
                               path_iapgemruby, path_iapgemruby_history, 
                               path_top3_items_by_category,path_weekly_iapcategory_rev,
                               service_sub, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION, bigquery_client, notion, bucket, headers_json, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    try:
        gcs_path = iap_gem_ruby_graph_draw(gameidx, path_iap_gem_ruby=path_iapgemruby, bucket=bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


    # ê³µí†µ í—¤ë”
    headers_json = headers_json
    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage = load_df_from_gcs(bucket, path_iapgemruby)

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_salesByPackage,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
        )
    
    blocks = md_to_notion_blocks(iap_gem_ruby_gemini(
        service_sub=service_sub,
        genai_client=genai_client,
        MODEL_NAME=MODEL_NAME,
        SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
        path_iapgemruby=path_iapgemruby,
        path_iapgemruby_history=path_iapgemruby_history,
        bucket=bucket,
        PROJECT_ID=PROJECT_ID,
        LOCATION=LOCATION))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    # í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ì¤‘ê°„ì— ê·¸ë˜í”„ ì‚½ì…ì„ ìœ„í•œ ê²°ê³¼ í…ìŠ¤íŠ¸ 5ë¶„í• 

    text = top3_items_by_category_gemini(
        service_sub=service_sub,
        genai_client=genai_client,
        MODEL_NAME=MODEL_NAME,
        SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
        path_top3_items_by_category=path_top3_items_by_category,
        path_weekly_iapcategory_rev=path_weekly_iapcategory_rev,
        bucket=bucket,
        PROJECT_ID=PROJECT_ID,
        LOCATION=LOCATION,
        **context)

    # ì¤„ ë‹¨ìœ„ ë¶„ë¦¬
    lines = [line.strip() for line in text.split("\n") if line.strip()]

    # ë¸”ë¡ ë‹¨ìœ„ ë¶„ë¦¬
    blocks_raw = []
    current_block = []

    for line in lines:
        if line.startswith("**") and line.endswith("**"):
            # ìƒˆë¡œìš´ ë¸”ë¡ ì‹œì‘ â†’ ê¸°ì¡´ ë¸”ë¡ ì €ì¥
            if current_block:
                blocks_raw.append(current_block)
            current_block = [line]
        else:
            current_block.append(line)

    # ë§ˆì§€ë§‰ ë¸”ë¡ ì €ì¥
    if current_block:
        blocks_raw.append(current_block)

    # ì´ì œ blocks_raw = [[í—¤ë”, ë‚´ìš©...], [í—¤ë”, ë‚´ìš©...], ...]

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    blocks_bucket = {"blocks_1": [], "blocks_2": [], "blocks_3": [], "blocks_4": [], "blocks_5": []}

    found_first = False
    for block in blocks_raw:
        header = block[0]

        # ë§¤ì¶œ 1ìœ„ ì „ê¹Œì§€ëŠ” blocks_1
        if not found_first:
            if "(ë§¤ì¶œ 1ìœ„)" in header:
                found_first = True
                blocks_bucket["blocks_2"] = block
            else:
                blocks_bucket["blocks_1"].extend(block)
            continue

        # ì´í›„ ë§¤ì¶œ 2ìœ„, 3ìœ„, ë‚˜ë¨¸ì§€ êµ¬ë¶„
        if "(ë§¤ì¶œ 2ìœ„)" in header:
            blocks_bucket["blocks_3"] = block
        elif "(ë§¤ì¶œ 3ìœ„)" in header:
            blocks_bucket["blocks_4"] = block
        else:
            blocks_bucket["blocks_5"].extend(block)

    for k, v in blocks_bucket.items():
        if isinstance(v, list):
            blocks_bucket[k] = "\n".join(v)  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜

    blocks = md_to_notion_blocks(blocks_bucket["blocks_1"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 1ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = top1_graph_draw(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category1.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = headers_json

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_2"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 2ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = top2_graph_draw(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category2.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_3"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 3ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = top3_graph_draw(joyplegameid, gameidx, databaseschema, service_sub, bigquery_client, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_4"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    blocks = md_to_notion_blocks(blocks_bucket["blocks_5"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True



def iap_toggle_add(gameidx: str, service_sub:str, MODEL_NAME:str, SYSTEM_INSTRUCTION:list, 
                   path_iap_df:str, path_iapgemruby_history:str, PROJECT_ID: str, LOCATION:str, bucket, notion, **context):
    
    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

        # ê³µí†µ í—¤ë”
    headers_json = headers_json

    try:
        gcs_path = iap_gem_ruby_IAP_graph_draw(gameidx, path_iap_df, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_IAP.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_IAP = context['task_instance'].xcom_pull(
        task_ids='iap_df',
        key='iap_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_IAP,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    blocks = md_to_notion_blocks(iap_df_gemini(service_sub=service_sub,
                                               genai_client=genai_client,
                                               MODEL_NAME=MODEL_NAME,
                                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                                               path_iap_df=path_iap_df,
                                               path_iapgemruby_history=path_iapgemruby_history,
                                               bucket=bucket,
                                               PROJECT_ID=PROJECT_ID,
                                               LOCATION=LOCATION,
                                               **context))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True



def gem_toggle_add(gameidx: str, service_sub:str, MODEL_NAME:str, SYSTEM_INSTRUCTION:list, 
                   path_gem_df:str, path_iapgemruby_history:str, PROJECT_ID: str, LOCATION:str, bucket, notion, **context):
    
    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = headers_json
    try:
        gcs_path = iap_gem_ruby_GEM_graph_draw(gameidx, path_gem_df, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_GEM.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

###########
    query_result4_salesByPackage_GEM = context['task_instance'].xcom_pull(
        task_ids='gem_df',
        key='gem_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_GEM,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    blocks = md_to_notion_blocks(gem_df_gemini(service_sub=service_sub,
                                               genai_client=genai_client,
                                               MODEL_NAME=MODEL_NAME,
                                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                                               path_gem_df=path_gem_df,
                                               path_iapgemruby_history=path_iapgemruby_history,
                                               bucket=bucket,
                                               PROJECT_ID=PROJECT_ID,
                                               LOCATION=LOCATION,
                                               **context))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def ruby_toggle_add(gameidx: str, service_sub:str, MODEL_NAME:str, SYSTEM_INSTRUCTION:list, 
                   path_ruby_df:str, path_iapgemruby_history:str, PROJECT_ID: str, LOCATION:str, bucket, notion, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = headers_json

    try:
        gcs_path = iap_gem_ruby_RUBY_graph_draw(gameidx, path_ruby_df, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_RUBY.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_RUBY = context['task_instance'].xcom_pull(
        task_ids='ruby_df',
        key='ruby_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_RUBY,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    blocks = md_to_notion_blocks(ruby_df_gemini(service_sub=service_sub,
                                               genai_client=genai_client,
                                               MODEL_NAME=MODEL_NAME,
                                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                                               path_ruby_df=path_ruby_df,
                                               path_iapgemruby_history=path_iapgemruby_history,
                                               bucket=bucket,
                                               PROJECT_ID=PROJECT_ID,
                                               LOCATION=LOCATION,
                                               **context))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def rgroup_top3_upload_notion(gameidx: str, service_sub:str, MODEL_NAME:str, SYSTEM_INSTRUCTION:list, 
                   path_rgroup_top3_pu:str, path_rgroup_top3_rev:str, PROJECT_ID: str, LOCATION:str, bucket, notion, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(3) ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œ/PU ìƒìœ„ 3ê°œ ìƒí’ˆ \n" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = headers_json
    try:
        gcs_path = rgroup_pu_top3_graph_draw(gameidx, path_rgroup_top3_pu, bucket, **context)
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekPUTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    ### íŒŒì¼ ì—…ë¡œë“œ ê°ì²´ 
    try:
        gcs_path = f'{gameidx}/graph4_thisWeekRevTop3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekRevTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": headers_json.get("Authorization"),
            "Notion-Version": headers_json.get("Notion-Version")
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    query_result4_thisWeekSalesTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekPUTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(PU)",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekSalesTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(ë§¤ì¶œ) ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    blocks = md_to_notion_blocks(rgroup_top3_gemini(service_sub=service_sub,
                                               genai_client=genai_client,
                                               MODEL_NAME=MODEL_NAME,
                                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                                               path_rgroup_top3_pu=path_rgroup_top3_pu,
                                               path_rgroup_top3_rev=path_rgroup_top3_rev,
                                               bucket=bucket,
                                               PROJECT_ID=PROJECT_ID,
                                               LOCATION=LOCATION,
                                               **context))


    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

