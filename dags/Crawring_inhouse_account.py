import gspread
import pandas as pd
import os
from airflow.models import Variable
from google.oauth2.service_account import Credentials
from google.oauth2 import service_account
from google.cloud import bigquery
import json
import pandas_gbq # ìµœì‹  ë°©ì‹ ê¶Œìž¥
from datetime import datetime, timedelta
from airflow import DAG, Dataset
from airflow.operators.python import PythonOperator
import requests
import time
import logging

def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)


WWMC_SPREADSHEET_ID = '1D7WghN05AOW6HRNscOnjW9JJ4P2-uWlGDK8bMcoAqKk'
WWMC_SHEET_NAME = 'TEST_ACCOUNT'

DRSG_SPREADSHEET_ID = '1CRbDxfF8pdGPxcvY-1-LHwsrN4xfXu-7LoEfce6_6-U'
DRSG_SHEET_NAME = 'TEST_ACCOUNT'

## POTCëŠ” ì‹œíŠ¸ ìž ê¸ˆìœ¼ë¡œ ì§„í–‰ ë¶ˆê°€ > ìž ê¸ˆ í•´ì œ
POTC_SPREADSHEET_ID = '16nZ8P-cxlARLoHwtXxDCr_awpqi9mCKG1R2s9AyYKkk'
POTC_SHEET_NAME = 'ë¹…ì¿¼ë¦¬(ì§€ì›ëŒ€ìƒ)' 

GBTW_SPREADSHEET_ID = '1kLyYB1xZUzj1VPq8u123gMrWtB4GGYsVhGMQYft-b30'
GBTW_SHEET_NAME_1 = 'GW 1,2ì›”ë“œ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬'
GBTW_SHEET_NAME_2 = 'GW 3ì›”ë“œ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬'
GBTW_SHEET_NAME_3 = 'GW ì™¸ì£¼ì‚¬ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬'
GBTW_SHEET_NAME_4 = 'ë¹„ì •ìƒ ì´ìš©ìž ì œìž¬ ì¡°ì¹˜'

NOTION_TOKEN = get_var("NOTION_TOKEN")
DBID = get_var("NOTION_DBID")
DATASET_ID = "Account_Info"
TABLE_ID = "RESU_account_info"
NOTION_API_VERSION = get_var("NOTION_API_VERSION", "2022-06-28")
FULL_TABLE_ID = f"data-science-division-216308.{DATASET_ID}.{TABLE_ID}"

PROJECT_ID = "datahub-478802"
LOCATION = "US"



################### ìœ í‹¸í•¨ìˆ˜ #####################

def get_gcp_credentials():
    """Airflow Variableì—ì„œ GCP ìžê²© ì¦ëª…ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    credentials_json = Variable.get('GOOGLE_CREDENTIAL_JSON')
    cred_dict = json.loads(credentials_json)
    if 'private_key' in cred_dict:
        cred_dict['private_key'] = cred_dict['private_key'].replace('\\n', '\n')
    
    # [ìˆ˜ì •] ìŠ¤ì½”í”„(Scopes)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œ ì¶”ê°€í•©ë‹ˆë‹¤.
    SCOPES = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive',
        "https://www.googleapis.com/auth/cloud-platform",
    ]
    
    return service_account.Credentials.from_service_account_info(
        cred_dict,
        scopes=SCOPES
    )

def init_clients():
    """Task ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ì–´ í•„ìš”í•œ í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    creds = get_gcp_credentials()
    
    # 1. GCP Clients
    bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)
    
    return {
        "bq_client": bq_client
    }


#################### RESU account info ê°€ì ¸ì˜¤ê¸° #######################
def extract_property_value(value):
    """Notion ì†ì„±ê°’ ì¶”ì¶œ"""
    prop_type = value["type"]
    mapping = {
        "title": lambda v: v.get("title", [{}])[0].get("text", {}).get("content"),
        "rich_text": lambda v: v.get("rich_text", [{}])[0].get("text", {}).get("content"),
        "select": lambda v: v.get("select", {}).get("name"),
        "multi_select": lambda v: [m["name"] for m in v.get("multi_select", [])],
        "status": lambda v: v.get("status", {}).get("name"),
        "date": lambda v: v.get("date", {}).get("start"),
        "checkbox": lambda v: v.get("checkbox"),
        "number": lambda v: v.get("number"),
    }
    try:
        return mapping.get(prop_type, lambda v: None)(value)
    except:
        return None
    
def query_notion_database(**context):
    """Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ì¡°íšŒ"""
    url = f"https://api.notion.com/v1/databases/{DBID}/query"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_API_VERSION,
        "Content-Type": "application/json"
    }

    results = []
    has_more = True
    next_cursor = None

    logging.info(f"ðŸ” Notion ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹œìž‘: {DBID}")
    
    while has_more:
        payload = {"start_cursor": next_cursor} if next_cursor else {}
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        results.extend(data["results"])
        has_more = data.get("has_more", False)
        next_cursor = data.get("next_cursor")

    logging.info(f"âœ… Notion ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(results)}ê°œ í–‰")
    
    context['task_instance'].xcom_push(key='notion_raw_data', value=results)
    return len(results)


def parse_and_transform_data(**context):
    """Notion ë°ì´í„° íŒŒì‹± ë° ë³€í™˜"""
    ti = context['task_instance']
    rows = ti.xcom_pull(task_ids='query_notion_database', key='notion_raw_data')
    
    logging.info(f"ðŸ“Š ë°ì´í„° íŒŒì‹± ì‹œìž‘: {len(rows)}ê°œ í–‰")
    
    # ë°ì´í„° íŒŒì‹±
    parsed = []
    for row in rows:
        row_data = {"notion_page_id": row.get("id")}
        props = row.get("properties", {})
        for key, value in props.items():
            row_data[key] = extract_property_value(value)
        parsed.append(row_data)
    
    # DataFrame ìƒì„± ë° ë³€í™˜
    df = pd.DataFrame(parsed)
    print(df.head(5))
    df.columns = df.columns.str.strip()
    df = df[['UserKey', 'UserID', 'êµ¬ë¶„']]
    df = df.assign(build='RESU').rename(
        columns={'UserKey': 'userKey', 'UserID': 'charid', 'êµ¬ë¶„': 'class'}
    )
    
    logging.info(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
    
    ti.xcom_push(key='transformed_data', value=df.to_dict('records'))
    return len(df)

def upload_to_bigquery(**context):
    """BigQueryì— ë°ì´í„° ì—…ë¡œë“œ"""
    ti = context['task_instance']
    data_dict = ti.xcom_pull(task_ids='parse_and_transform_data', key='transformed_data')
    df = pd.DataFrame(data_dict)
    
    logging.info(f"ðŸ“¤ BigQuery ì—…ë¡œë“œ ì‹œìž‘: {len(df)}ê°œ í–‰")
    
    try:
        credentials_json = Variable.get('GOOGLE_CREDENTIAL_JSON')
        cred_dict = json.loads(credentials_json)
        
        # 2. private_key ì¤„ë°”ê¿ˆ ë¬¸ìž ì²˜ë¦¬
        if 'private_key' in cred_dict:
            if '\\n' in cred_dict['private_key']:
                cred_dict['private_key'] = cred_dict['private_key'].replace('\\n', '\n')

        # 3. Credentials ê°ì²´ ìƒì„±
        credentials = service_account.Credentials.from_service_account_info(
            cred_dict,
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )
        
        # 4. Client ìƒì„± ì‹œ credentials ì „ë‹¬ (ì—¬ê¸°ê°€ í•µì‹¬!)
        client = bigquery.Client(project=PROJECT_ID, credentials=credentials)
        
    except Exception as e:
        print(f"âŒ ì¸ì¦ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise e
    
    job = client.load_table_from_dataframe(
        dataframe=df,
        destination=FULL_TABLE_ID,
        job_config=bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE",
            autodetect=True
        )
    )
    job.result()
    
    logging.info(f"âœ… BigQuery í…Œì´ë¸” ì—…ë¡œë“œ ì™„ë£Œ: {FULL_TABLE_ID}")
    
    ti.xcom_push(key='result', value={'table': FULL_TABLE_ID, 'rows': len(df)})
    return len(df)

#################### WWMC ì¸í•˜ìš°ìŠ¤ ê³„ì • ETL í•¨ìˆ˜ #####################
def WWMC_from_spreadsheet_df(spreadsheet_id, sheet_name):

    creds = get_gcp_credentials()
    client = gspread.authorize(creds)

    doc = client.open_by_key(spreadsheet_id)
    sheet = doc.worksheet(sheet_name)
    all_data = sheet.get('A:D')

    if not all_data:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data[1]
    data = all_data[2:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]

    df = pd.DataFrame(data, columns=header)

    df = df.rename(columns={
        "build":"build",
        "userkey":"userkey",
        "charid":"charid",
        "type":"class"
        })

    selected_df = df[["build",
                      "userkey",
                      "charid",
                      "class"
                      ]]
    
    return selected_df


def WWMC_merge_to_bigquery(project_id, dataset_id, table_id):

    df = WWMC_from_spreadsheet_df(WWMC_SPREADSHEET_ID, WWMC_SHEET_NAME)
    credentials = get_gcp_credentials()
    client = bigquery.Client(project=project_id, credentials=credentials)
    table_full_id = f"{project_id}.{dataset_id}.{table_id}"

    # 1. ë°ì´í„° ë¹„ìš°ê¸° (í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ/ì„¤ì • ìœ ì§€)
    truncate_query = f"TRUNCATE TABLE `{table_full_id}`"
    client.query(truncate_query, location=LOCATION).result()
    print(f"ðŸ—‘ï¸ {table_full_id} ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒ€ìž… í´ë¦¬ë‹ (Parquet ë³€í™˜ ì—ëŸ¬ ë°©ì§€)
    df_final = df.astype(str)
    
    # 3. ë°ì´í„° ì‚½ìž…
    try:
        # TRUNCATEë¥¼ ë¯¸ë¦¬ í–ˆìœ¼ë¯€ë¡œ 'append'ë¥¼ ì¨ì•¼ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ/íŒŒí‹°ì…˜ ì„¤ì •ì´ ìœ ì§€ë©ë‹ˆë‹¤.
        # df.to_gbq ëŒ€ì‹  pandas_gbq.to_gbq ì‚¬ìš© ê¶Œìž¥
        pandas_gbq.to_gbq(
            df_final,
            destination_table=f"{dataset_id}.{table_id}",
            project_id=project_id,
            if_exists='append', 
            progress_bar=True,
            credentials=credentials
        )
        print(f"âœ… {len(df_final)}í–‰ ë°ì´í„°ê°€ {table_full_id}ì— ì„±ê³µì ìœ¼ë¡œ Insert ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e # ì—ëŸ¬ ì¶”ì ì„ ìœ„í•´ raise ì¶”ê°€
    

#################### DS ì¸í•˜ìš°ìŠ¤ ê³„ì • ETL í•¨ìˆ˜ #####################
def DRSG_from_spreadsheet_df(spreadsheet_id, sheet_name):

    creds = get_gcp_credentials()
    client = gspread.authorize(creds)

    doc = client.open_by_key(spreadsheet_id)
    sheet = doc.worksheet(sheet_name)
    all_data = sheet.get('A:E')

    if not all_data:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data[0]
    data = all_data[1:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]

    df = pd.DataFrame(data, columns=header)

    df = df.rename(columns={
        "ë¹Œë“œ":"build",
        "ì„œë²„ëª…":"worldid",
        "ê³„ì •ë²ˆí˜¸":"charid",
        "êµ¬ë¶„":"class",
        "íšŒì›ë²ˆí˜¸":"userkey"
        })

    selected_df = df[["build",
                      "userkey",
                      "charid",
                      "class",
                      "worldid"
                      ]]
    
    return selected_df


def DRSG_merge_to_bigquery(project_id, dataset_id, table_id):

    df = DRSG_from_spreadsheet_df(DRSG_SPREADSHEET_ID, DRSG_SHEET_NAME)
    credentials = get_gcp_credentials()
    client = bigquery.Client(project=project_id, credentials=credentials)
    table_full_id = f"{project_id}.{dataset_id}.{table_id}"

    # 1. ë°ì´í„° ë¹„ìš°ê¸° (í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ/ì„¤ì • ìœ ì§€)
    truncate_query = f"TRUNCATE TABLE `{table_full_id}`"
    client.query(truncate_query, location=LOCATION).result()
    print(f"ðŸ—‘ï¸ {table_full_id} ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒ€ìž… í´ë¦¬ë‹ (Parquet ë³€í™˜ ì—ëŸ¬ ë°©ì§€)
    df_final = df.astype(str)
    
    # 3. ë°ì´í„° ì‚½ìž…
    try:
        # TRUNCATEë¥¼ ë¯¸ë¦¬ í–ˆìœ¼ë¯€ë¡œ 'append'ë¥¼ ì¨ì•¼ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ/íŒŒí‹°ì…˜ ì„¤ì •ì´ ìœ ì§€ë©ë‹ˆë‹¤.
        # df.to_gbq ëŒ€ì‹  pandas_gbq.to_gbq ì‚¬ìš© ê¶Œìž¥
        pandas_gbq.to_gbq(
            df_final,
            destination_table=f"{dataset_id}.{table_id}",
            project_id=project_id,
            if_exists='append', 
            progress_bar=True,
            credentials=credentials
        )
        print(f"âœ… {len(df_final)}í–‰ ë°ì´í„°ê°€ {table_full_id}ì— ì„±ê³µì ìœ¼ë¡œ Insert ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e # ì—ëŸ¬ ì¶”ì ì„ ìœ„í•´ raise ì¶”ê°€


#################### GBTW ì¸í•˜ìš°ìŠ¤ ê³„ì • ETL í•¨ìˆ˜ #####################
def GBTW_from_spreadsheet_df(spreadsheet_id, sheet_name_1, sheet_name_2, sheet_name_3, sheet_name_4):

    creds = get_gcp_credentials()
    client = gspread.authorize(creds)

    #### ì‹œíŠ¸ 1 : GW 1,2ì›”ë“œ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬
    doc = client.open_by_key(spreadsheet_id)
    sheet1 = doc.worksheet(sheet_name_1)
    all_data_1 = sheet1.get('C:E')

    if not all_data_1:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data_1[1]
    data = all_data_1[2:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]

    df1 = pd.DataFrame(data, columns=header)

    df1 = df1.rename(columns={
        "íšŒì›ë²ˆí˜¸(Userkey) ":"userkey",
        "ê³„ì •ë²ˆí˜¸(UserID)":"charid",
        "êµ¬ë¶„":"class"
        })
    df1['world'] = 'GBTW'

    selected_df1 = df1[["world",
                    "userkey",
                    "charid",
                    "class"
                    ]]
    
    #### ì‹œíŠ¸ 2 : GW 3ì›”ë“œ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬
    sheet2 = doc.worksheet(sheet_name_2)
    all_data_2 = sheet2.get('C:E')

    if not all_data_2:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data_2[1]
    data = all_data_2[2:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]
    
    df2 = pd.DataFrame(data, columns=header)

    df2 = df2.rename(columns={
        "íšŒì›ë²ˆí˜¸(Userkey) ":"userkey",
        "ê³„ì •ë²ˆí˜¸(UserID)":"charid",
        "êµ¬ë¶„":"class"
        })
    df2['world'] = 'GBTW'

    selected_df2 = df2[["world",
                "userkey",
                "charid",
                "class"
                ]]


    #### ì‹œíŠ¸ 3 : GW ì™¸ì£¼ì‚¬ ë§ˆìŠ¤í„°ì¦ˆ ê´€ë¦¬
    sheet3 = doc.worksheet(sheet_name_3)
    all_data_3 = sheet3.get('C:E')

    if not all_data_3:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data_3[1]
    data = all_data_3[2:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]
    
    df3 = pd.DataFrame(data, columns=header)

    df3 = df3.rename(columns={
        "íšŒì›ë²ˆí˜¸(Userkey) ":"userkey",
        "ê³„ì •ë²ˆí˜¸(UserID)":"charid",
        "êµ¬ë¶„":"class"
        })
    df3['world'] = 'GBTW'

    selected_df3 = df3[["world",
            "userkey",
            "charid",
            "class"
            ]]


    #### ì‹œíŠ¸ 4 : ë¹„ì •ìƒ ì´ìš©ìž ì œìž¬ ì¡°ì¹˜
    sheet4 = doc.worksheet(sheet_name_4)
    all_data_4 = sheet4.get('A:F')

    if not all_data_4:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data_4[0]
    data = all_data_4[1:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]
    
    df4 = pd.DataFrame(data, columns=header)

    df4 = df4.rename(columns={
        "íšŒì›ë²ˆí˜¸":"userkey",
        "ì œë…ë²ˆí˜¸":"charid",
        "ì œìž¬ ì²˜ë¦¬ ìœ í˜•":"class"
        })
    df4['world'] = 'GBTW'

    selected_df4 = df4[["world",
        "userkey",
        "charid",
        "class"
        ]]


    selected_df = pd.concat([selected_df1, selected_df2, selected_df3, selected_df4], ignore_index=True)
    
    return selected_df


def GBTW_merge_to_bigquery(project_id, dataset_id, table_id):

    df = GBTW_from_spreadsheet_df(GBTW_SPREADSHEET_ID, GBTW_SHEET_NAME_1, GBTW_SHEET_NAME_2, GBTW_SHEET_NAME_3, GBTW_SHEET_NAME_4)
    credentials = get_gcp_credentials()
    client = bigquery.Client(project=project_id, credentials=credentials)
    table_full_id = f"{project_id}.{dataset_id}.{table_id}"

    # 1. ë°ì´í„° ë¹„ìš°ê¸° (í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ/ì„¤ì • ìœ ì§€)
    truncate_query = f"TRUNCATE TABLE `{table_full_id}`"
    client.query(truncate_query, location=LOCATION).result()
    print(f"ðŸ—‘ï¸ {table_full_id} ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒ€ìž… í´ë¦¬ë‹ (Parquet ë³€í™˜ ì—ëŸ¬ ë°©ì§€)
    df_final = df.astype(str)
    
    # 3. ë°ì´í„° ì‚½ìž…
    try:
        # TRUNCATEë¥¼ ë¯¸ë¦¬ í–ˆìœ¼ë¯€ë¡œ 'append'ë¥¼ ì¨ì•¼ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ/íŒŒí‹°ì…˜ ì„¤ì •ì´ ìœ ì§€ë©ë‹ˆë‹¤.
        # df.to_gbq ëŒ€ì‹  pandas_gbq.to_gbq ì‚¬ìš© ê¶Œìž¥
        pandas_gbq.to_gbq(
            df_final,
            destination_table=f"{dataset_id}.{table_id}",
            project_id=project_id,
            if_exists='append', 
            progress_bar=True,
            credentials=credentials
        )
        print(f"âœ… {len(df_final)}í–‰ ë°ì´í„°ê°€ {table_full_id}ì— ì„±ê³µì ìœ¼ë¡œ Insert ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e # ì—ëŸ¬ ì¶”ì ì„ ìœ„í•´ raise ì¶”ê°€



#################### POTC ì¸í•˜ìš°ìŠ¤ ê³„ì • ETL í•¨ìˆ˜ #####################
def POTC_from_spreadsheet_df(spreadsheet_id, sheet_name):

    creds = get_gcp_credentials()
    client = gspread.authorize(creds)

    doc = client.open_by_key(spreadsheet_id)
    sheet = doc.worksheet(sheet_name)
    all_data = sheet.get('A:D')

    if not all_data:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    header = all_data[1]
    data = all_data[2:]

    if len(header) == 1:
        header = [f'col_{i}' for i in range(len(data[0]))]

    df = pd.DataFrame(data, columns=header)

    df = df.rename(columns={
        "ë¹Œë“œ":"build",
        "íšŒì›ë²ˆí˜¸(Userkey) ":"userkey",
        "ê³„ì •ë²ˆí˜¸(UserID)":"charid",
        "êµ¬ë¶„":"class",
        })

    selected_df = df[["build",
                      "userkey",
                      "charid",
                      "class",
                      ]]
    
    return selected_df


def POTC_merge_to_bigquery(project_id, dataset_id, table_id):

    df = POTC_from_spreadsheet_df(POTC_SPREADSHEET_ID, POTC_SHEET_NAME)
    credentials = get_gcp_credentials()
    client = bigquery.Client(project=project_id, credentials=credentials)
    table_full_id = f"{project_id}.{dataset_id}.{table_id}"

    # 1. ë°ì´í„° ë¹„ìš°ê¸° (í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ/ì„¤ì • ìœ ì§€)
    truncate_query = f"TRUNCATE TABLE `{table_full_id}`"
    client.query(truncate_query, location=LOCATION).result()
    print(f"ðŸ—‘ï¸ {table_full_id} ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒ€ìž… í´ë¦¬ë‹ (Parquet ë³€í™˜ ì—ëŸ¬ ë°©ì§€)
    df_final = df.astype(str)
    
    # 3. ë°ì´í„° ì‚½ìž…
    try:
        # TRUNCATEë¥¼ ë¯¸ë¦¬ í–ˆìœ¼ë¯€ë¡œ 'append'ë¥¼ ì¨ì•¼ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ/íŒŒí‹°ì…˜ ì„¤ì •ì´ ìœ ì§€ë©ë‹ˆë‹¤.
        # df.to_gbq ëŒ€ì‹  pandas_gbq.to_gbq ì‚¬ìš© ê¶Œìž¥
        pandas_gbq.to_gbq(
            df_final,
            destination_table=f"{dataset_id}.{table_id}",
            project_id=project_id,
            if_exists='append', 
            progress_bar=True,
            credentials=credentials
        )
        print(f"âœ… {len(df_final)}í–‰ ë°ì´í„°ê°€ {table_full_id}ì— ì„±ê³µì ìœ¼ë¡œ Insert ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e # ì—ëŸ¬ ì¶”ì ì„ ìœ„í•´ raise ì¶”ê°€




# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(seconds=15),
}

with DAG(
    dag_id='inhouse_account_ETL',
    default_args=default_args,
    description='Inhouse Account ETL',
    schedule='30 19 * * *',  # ë§¤ì¼ ì˜¤ì „ 04ì‹œ 50ë¶„ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['ETL', 'inhouse_account', 'bigquery'],
) as dag:

    WWMC_inhouse_account_task = PythonOperator(
        task_id='WWMC_inhouse_account_task',
        python_callable=WWMC_merge_to_bigquery,
        op_kwargs={
            "project_id": "data-science-division-216308",
            "dataset_id": "Account_Info",
            "table_id": "WWM_account_info"
        },
        dag=dag,
    )

    DRSG_inhouse_account_task = PythonOperator(
        task_id='DRSG_inhouse_account_task',
        python_callable=DRSG_merge_to_bigquery,
        op_kwargs={
            "project_id": "data-science-division-216308",
            "dataset_id": "Account_Info",
            "table_id": "DS_account_info"
        },
        dag=dag,
    )

    GBTW_inhouse_account_task = PythonOperator(
        task_id='GBTW_inhouse_account_task',
        python_callable=GBTW_merge_to_bigquery,
        op_kwargs={
            "project_id": "data-science-division-216308",
            "dataset_id": "Account_Info",
            "table_id": "GW_account_info"
        },
        dag=dag,
    )

    POTC_inhouse_account_task = PythonOperator(
        task_id='POTC_inhouse_account_task',
        python_callable=POTC_merge_to_bigquery,
        op_kwargs={
            "project_id": "data-science-division-216308",
            "dataset_id": "Account_Info",
            "table_id": "POTC_account_info"
        },
        dag=dag,
    )

    # ETL Tasks
    RESU_query_task = PythonOperator(
        task_id='query_notion_database',
        python_callable=query_notion_database,
    )
    
    RESU_transform_task = PythonOperator(
        task_id='parse_and_transform_data',
        python_callable=parse_and_transform_data,
    )
    
    RESU_load_task = PythonOperator(
        task_id='upload_to_bigquery',
        python_callable=upload_to_bigquery,
    )

    WWMC_inhouse_account_task >> DRSG_inhouse_account_task >> GBTW_inhouse_account_task >> POTC_inhouse_account_task >> RESU_query_task >> RESU_transform_task >> RESU_load_task