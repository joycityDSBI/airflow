"""
Notion DB ë™ê¸°í™” DAG
- Databricksì—ì„œ ë°ì´í„° ì¡°íšŒ
- Pandasë¡œ ë°ì´í„° ì§‘ê³„
- Notion DBì™€ ë™ê¸°í™” (INSERT/UPDATE/DELETE)
"""

from airflow import DAG, Dataset
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import time
import json
import os

# Airflow Variable import (ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬)
try:
    from airflow.sdk import Variable
except ImportError:
    from airflow.models import Variable


# ============================================================
# ê¸°ë³¸ ì„¤ì •
# ============================================================
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def get_var(key: str, default: str = None, required: bool = False) -> str:
    """í™˜ê²½ ë³€ìˆ˜ â†’ Airflow Variable ìˆœì„œë¡œ ì¡°íšŒ
    
    Args:
        key: ë³€ìˆ˜ ì´ë¦„
        default: ê¸°ë³¸ê°’ (ì—†ìœ¼ë©´ None)
        required: í•„ìˆ˜ ë³€ìˆ˜ ì—¬ë¶€
    
    Returns:
        í™˜ê²½ ë³€ìˆ˜ ê°’ ë˜ëŠ” Airflow Variable ê°’
    """
    # 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    env_value = os.environ.get(key)
    if env_value:
        print(f"âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ {key} ë¡œë“œë¨")
        return env_value
    
    # 2ë‹¨ê³„: Airflow Variable í™•ì¸
    try:
        # airflow.sdk.Variableì€ default íŒŒë¼ë¯¸í„° ì‚¬ìš©
        # airflow.models.Variableì€ default_var íŒŒë¼ë¯¸í„° ì‚¬ìš©
        try:
            var_value = Variable.get(key, default=None)
        except TypeError:
            # fallback for older Airflow versions
            var_value = Variable.get(key, default_var=None)
        
        if var_value:
            print(f"âœ“ Airflow Variableì—ì„œ {key} ë¡œë“œë¨")
            return var_value
    except Exception as e:
        print(f"âš ï¸  Variable.get({key}) ì˜¤ë¥˜: {str(e)}")
    
    # 3ë‹¨ê³„: ê¸°ë³¸ê°’ ë°˜í™˜
    if default is not None:
        print(f"â„¹ï¸  ê¸°ë³¸ê°’ìœ¼ë¡œ {key} ì„¤ì •ë¨: {default}")
        return default
    
    # 4ë‹¨ê³„: í•„ìˆ˜ ë³€ìˆ˜ì¸ ê²½ìš° ì—ëŸ¬, ì•„ë‹ˆë©´ None ë°˜í™˜
    if required:
        raise ValueError(f"í•„ìˆ˜ ì„¤ì • {key}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                         f"í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variableì—ì„œ ì„¤ì •í•˜ì„¸ìš”.")
    
    print(f"â„¹ï¸  {key} ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)")
    return None


def get_notion_headers():
    """Notion API í—¤ë” ìƒì„±"""
    return {
        "Authorization": f"Bearer {get_var('NOTION_TOKEN', required=True)}",
        "Notion-Version": get_var("NOTION_API_VERSION", "2022-06-28"),
        "Content-Type": "application/json"
    }


# ============================================================
# Notion API í•¨ìˆ˜ë“¤
# ============================================================
def get_all_notion_pages(database_id: str, headers: dict) -> list:
    """Notion DBì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ì¡°íšŒí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/databases/{database_id}/query"
    results = []
    has_more = True
    next_cursor = None
    
    while has_more:
        payload = {"page_size": 100}
        if next_cursor:
            payload["start_cursor"] = next_cursor
        
        try:
            res = requests.post(url, headers=headers, json=payload)
            res.raise_for_status()
            data = res.json()
            results.extend(data.get("results", []))
            next_cursor = data.get("next_cursor")
            has_more = data.get("has_more", False)
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion API ì—ëŸ¬: {e}")
            break
        
        time.sleep(0.3)
    
    return results


def get_notion_data_map(pages: list) -> dict:
    """Notion í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ {key: {page_id, values}} ë§µìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    page_map = {}
    
    for page in pages:
        props = page.get("properties", {})
        row = {}
        key_val = ""
        
        for prop_name, prop_val in props.items():
            content = ""
            prop_type = prop_val.get("type")

            if prop_type == "title":
                content = "".join([block.get("plain_text", "") for block in prop_val.get("title", [])])
            elif prop_type == "rich_text":
                content = "".join([block.get("plain_text", "") for block in prop_val.get("rich_text", [])])
            elif prop_type == "date":
                date_info = prop_val.get("date")
                if date_info:
                    content = date_info.get("start")
            
            row[prop_name] = content
            if prop_name == 'ëŒ€í™”id':
                key_val = content

        if key_val:
            page_map[key_val] = {"page_id": page["id"], "values": row}
    
    return page_map


def build_properties_payload(row_data: dict) -> dict:
    """DataFrame í–‰ì„ Notion API í˜ì´ë¡œë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    properties = {}
    prop_map = {
        "ëŒ€í™”id": row_data.get("conversation_id"),
        "ì‚¬ìš©ì": row_data.get("user_email"),
        "ìŠ¤í˜ì´ìŠ¤ëª…": row_data.get("space_name"),
        "ëŒ€í™”ìˆœì„œ": row_data.get("conversation_flow"),
        "ëŒ€í™”ë‚ ì§œ": row_data.get("conversation_date")
    }

    for key, value in prop_map.items():
        if value is None:
            continue

        if key == "ëŒ€í™”id":
            properties[key] = {"title": [{"text": {"content": str(value)[:2000]}}]}
        
        elif key == "ëŒ€í™”ë‚ ì§œ":
            if hasattr(value, 'strftime'):
                date_str = value.strftime('%Y-%m-%d')
                properties[key] = {"date": {"start": date_str}}
            elif isinstance(value, str):
                properties[key] = {"date": {"start": value}}
        
        else:  # ì‚¬ìš©ì, ìŠ¤í˜ì´ìŠ¤ëª…, ëŒ€í™”ìˆœì„œ
            content = ""
            if isinstance(value, list):
                if not value:
                    continue
                content = '\n'.join(map(str, [item for item in value if item is not None]))
            else:
                content = str(value or "")
            
            if not content:
                continue

            if len(content) > 2000:
                chunks = [content[i:i + 2000] for i in range(0, len(content), 2000)]
                properties[key] = {"rich_text": [{"text": {"content": chunk}} for chunk in chunks]}
            else:
                properties[key] = {"rich_text": [{"text": {"content": content}}]}
                
    return properties


def has_data_changed(source_row: dict, notion_row: dict) -> bool:
    """ì†ŒìŠ¤ ë°ì´í„°ì™€ Notion ë°ì´í„° ë³€ê²½ ì—¬ë¶€ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""
    prop_map = {
        "ëŒ€í™”id": str(source_row.get("conversation_id") or ""),
        "ì‚¬ìš©ì": str(source_row.get("user_email") or ""),
        "ìŠ¤í˜ì´ìŠ¤ëª…": str(source_row.get("space_name") or ""),
        "ëŒ€í™”ë‚ ì§œ": source_row.get("conversation_date").strftime('%Y-%m-%d') if pd.notna(source_row.get("conversation_date")) else "",
    }

    source_flow_as_string = '\n'.join(map(str, [item for item in source_row.get("conversation_flow", []) if item is not None]))
    if source_flow_as_string != notion_row.get("ëŒ€í™”ìˆœì„œ", ""):
        return True

    for key, source_value in prop_map.items():
        notion_value = notion_row.get(key, "")
        if source_value != notion_value:
            return True
            
    return False


# ============================================================
# DAG Task í•¨ìˆ˜ë“¤
# ============================================================
def fetch_data_from_databricks(**context):
    """
    Step 1: Databricksì—ì„œ ë°ì´í„° ì¡°íšŒ
    """
    print("Step 1: Databricksì—ì„œ ë°ì´í„° ì¡°íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # Databricks ì„¤ì • í™•ì¸
    db_hostname = get_var("DATABRICKS_SERVER_HOSTNAME")
    db_http_path = get_var("DATABRICKS_HTTP_PATH")
    db_token = get_var("DATABRICKS_TOKEN")
    
    # Databricks ì„¤ì •ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤ì œ ì—°ê²° ì‹œë„
    if db_hostname and db_http_path and db_token:
        try:
            from databricks import sql
            
            print("âœ“ Databricks ì„¤ì •ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            connection = sql.connect(
                server_hostname=db_hostname,
                http_path=db_http_path,
                access_token=db_token
            )
            cursor = connection.cursor()
            cursor.execute("""
                SELECT conversation_id, user_email, space_name, content, event_time_kst
                FROM datahub.injoy_ops_schema.injoy_monitoring_data
                ORDER BY conversation_id, event_time_kst
            """)
            
            # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [desc[0] for desc in cursor.description]
            data = cursor.fetchall()
            source_df = pd.DataFrame(data, columns=columns)
            
            cursor.close()
            connection.close()
            
            print(f"âœ… Databricksì—ì„œ {len(source_df)}ê°œì˜ ë©”ì‹œì§€ ë°ì´í„°ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.")
            
        except ImportError:
            print("âš ï¸  databricks-sql-connectorê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install databricks-sql-connector ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            source_df = pd.DataFrame({
                'conversation_id': [],
                'user_email': [],
                'space_name': [],
                'content': [],
                'event_time_kst': []
            })
        except Exception as e:
            print(f"âŒ Databricks ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("   ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            source_df = pd.DataFrame({
                'conversation_id': [],
                'user_email': [],
                'space_name': [],
                'content': [],
                'event_time_kst': []
            })
    else:
        # Databricks ì„¤ì •ì´ ì—†ëŠ” ê²½ìš°
        print("âš ï¸  Databricks ì„¤ì •ì´ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”:")
        print("   - DATABRICKS_SERVER_HOSTNAME")
        print("   - DATABRICKS_HTTP_PATH")
        print("   - DATABRICKS_TOKEN")
        print("   ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        
        source_df = pd.DataFrame({
            'conversation_id': [],
            'user_email': [],
            'space_name': [],
            'content': [],
            'event_time_kst': []
        })
    
    # XComì— ë°ì´í„° ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ)
    context['task_instance'].xcom_push(
        key='source_data', 
        value=source_df.to_json(orient='records', date_format='iso')
    )


def aggregate_data(**context):
    """
    Step 2: Pandasë¡œ ë°ì´í„° ì§‘ê³„
    """
    print("\nStep 2: Pandasë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ëŒ€í™”ë³„ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.")
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    source_data_json = context['task_instance'].xcom_pull(key='source_data', task_ids='fetch_data')
    source_df = pd.read_json(source_data_json, orient='records')
    
    if source_df.empty:
        print("âš ï¸  ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì§‘ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        context['task_instance'].xcom_push(key='aggregated_data', value='[]')
        return
    
    # ë°ì´í„° ì§‘ê³„
    aggregated_df = source_df.groupby('conversation_id').agg(
        user_email=('user_email', 'first'),
        space_name=('space_name', 'first'),
        conversation_flow=('content', list),
        conversation_date=('event_time_kst', 'first')
    ).reset_index()

    # ë°ì´í„° íƒ€ì… ë³€í™˜
    aggregated_df['conversation_date'] = pd.to_datetime(aggregated_df['conversation_date']).dt.date
    aggregated_df['conversation_id'] = aggregated_df['conversation_id'].astype(str)

    print(f"âœ… Pandas ì§‘ê³„ ì™„ë£Œ. ì´ {len(aggregated_df)}ê°œì˜ ëŒ€í™”ë¡œ ê·¸ë£¹í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # XComì— ì§‘ê³„ ë°ì´í„° ì €ì¥
    context['task_instance'].xcom_push(key='aggregated_data', value=aggregated_df.to_json(orient='records', date_format='iso'))


def sync_with_notion(**context):
    """
    Step 3: Notion DBì™€ ë°ì´í„° ë™ê¸°í™”
    """
    print("\nStep 3: Notion DBì™€ ë°ì´í„° ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸°
    NOTION_DB_ID = get_var("NOTION_DB_ID_INJOY_MONITORINGDATA", "234ea67a568180029ffbfeaa7e73a011")
    headers = get_notion_headers()
    
    # XComì—ì„œ ì§‘ê³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    aggregated_data_json = context['task_instance'].xcom_pull(key='aggregated_data', task_ids='aggregate_data')
    aggregated_df = pd.read_json(aggregated_data_json, orient='records')
    
    if aggregated_df.empty:
        print("âš ï¸  ì§‘ê³„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # conversation_dateë¥¼ date ê°ì²´ë¡œ ë³€í™˜
    aggregated_df['conversation_date'] = pd.to_datetime(aggregated_df['conversation_date']).dt.date
    
    # Notion DB ë°ì´í„° ì¡°íšŒ
    notion_pages = get_all_notion_pages(NOTION_DB_ID, headers)
    notion_map = get_notion_data_map(notion_pages)
    print(f"â˜ï¸ Notion DBì—ì„œ {len(notion_map)}ê°œì˜ ë°ì´í„°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    # ì†ŒìŠ¤ ë°ì´í„° ë§µ ìƒì„±
    source_map = {row['conversation_id']: row for row in aggregated_df.to_dict('records')}
    print(f"â˜ï¸ ì†ŒìŠ¤ì—ì„œ {len(source_map)}ê°œì˜ ë°ì´í„°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    # ë™ê¸°í™” ê³„íš ìˆ˜ë¦½
    source_keys = set(source_map.keys())
    notion_keys = set(notion_map.keys())
    to_insert = source_keys - notion_keys
    to_update_keys = source_keys & notion_keys
    to_delete = notion_keys - source_keys

    print(f"\nğŸ”„ ë™ê¸°í™” ê³„íš: ì¶”ê°€ {len(to_insert)}ê±´, ì—…ë°ì´íŠ¸ í™•ì¸ {len(to_update_keys)}ê±´, ì‚­ì œ {len(to_delete)}ê±´")

    # INSERT ì‘ì—…
    print("\n[INSERT ì‘ì—… ì‹œì‘]")
    for key in to_insert:
        print(f"  -> INSERT: {key}")
        row_data = source_map[key]
        properties = build_properties_payload(row_data)
        payload = {"parent": {"database_id": NOTION_DB_ID}, "properties": properties}
        
        try:
            res = requests.post("https://api.notion.com/v1/pages", headers=headers, json=payload)
            res.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"      âŒ ì¶”ê°€ ì‹¤íŒ¨: {e}\n      - Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        time.sleep(0.3)

    # UPDATE ì‘ì—…
    print("\n[UPDATE ì‘ì—… ì‹œì‘]")
    updates_to_perform = []
    for key in to_update_keys:
        source_row = source_map[key]
        notion_row_data = notion_map[key]['values']
        if has_data_changed(source_row, notion_row_data):
            updates_to_perform.append(key)
            
    print(f"  -> ì´ {len(updates_to_perform)}ê±´ì˜ ë°ì´í„° ë³€ê²½ ê°ì§€. ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    for key in updates_to_perform:
        print(f"  -> UPDATE: {key}")
        page_id = notion_map[key]['page_id']
        properties = build_properties_payload(source_map[key])
        payload = {"properties": properties}
        
        try:
            res = requests.patch(f"https://api.notion.com/v1/pages/{page_id}", headers=headers, json=payload)
            res.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"      âŒ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\n      - Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        time.sleep(0.3)

    # DELETE ì‘ì—…
    print("\n[DELETE ì‘ì—… ì‹œì‘]")
    for key in to_delete:
        print(f"  -> DELETE: {key}")
        page_id = notion_map[key]['page_id']
        
        try:
            res = requests.patch(f"https://api.notion.com/v1/pages/{page_id}", headers=headers, json={"archived": True})
            res.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"      âŒ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        time.sleep(0.3)

    print("\nâœ¨ ëª¨ë“  ë™ê¸°í™” ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! âœ¨")



# ============================================================
# DAG ì •ì˜
# ============================================================

injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

with DAG(
    dag_id='injoy_monitoringdata_consumer_1',
    default_args=default_args,
    description='Databricks ë°ì´í„°ë¥¼ Notion DBì™€ ë™ê¸°í™”',
    schedule=[injoy_monitoringdata_producer],
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['notion', 'databricks', 'sync'],
) as dag:

    # Task 1: Databricksì—ì„œ ë°ì´í„° ì¡°íšŒ
    task_fetch_data = PythonOperator(
        task_id='fetch_data',
        python_callable=fetch_data_from_databricks,
        dag=dag,
    )

    # Task 2: ë°ì´í„° ì§‘ê³„
    task_aggregate = PythonOperator(
        task_id='aggregate_data',
        python_callable=aggregate_data,
        dag=dag,
    )

    # Task 3: Notion DB ë™ê¸°í™”
    task_sync = PythonOperator(
        task_id='sync_with_notion',
        python_callable=sync_with_notion,
        dag=dag,
    )

    # Task ì˜ì¡´ì„± ì„¤ì •
    task_fetch_data >> task_aggregate >> task_sync