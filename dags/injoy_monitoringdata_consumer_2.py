from airflow import DAG, Dataset
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import time
import os
from databricks import sql

# notion : In-Joy ì§ˆì˜ ëª¨ë‹ˆí„°ë§ ë°ì´í„° DB ì— ë™ê¸°í™” ì²˜ë¦¬
# Airflow Variable import (ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬)
try:
    from airflow.sdk import Variable
except ImportError:
    from airflow.models import Variable


# ============================================================
# ê¸°ë³¸ ì„¤ì •
# ============================================================
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def get_var(key: str, default: str = None, required: bool = False) -> str:
    """í™˜ê²½ ë³€ìˆ˜ â†’ Airflow Variable ìˆœì„œë¡œ ì¡°íšŒ"""
    env_value = os.environ.get(key)
    if env_value:
        print(f"âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ {key} ë¡œë“œë¨")
        return env_value
    
    try:
        try:
            var_value = Variable.get(key, default=None)
        except TypeError:
            var_value = Variable.get(key, default_var=None)
        
        if var_value:
            print(f"âœ“ Airflow Variableì—ì„œ {key} ë¡œë“œë¨")
            return var_value
    except Exception as e:
        print(f"âš ï¸  Variable.get({key}) ì˜¤ë¥˜: {str(e)}")
    
    if default is not None:
        print(f"â„¹ï¸  ê¸°ë³¸ê°’ìœ¼ë¡œ {key} ì„¤ì •ë¨: {default}")
        return default
    
    if required:
        raise ValueError(f"í•„ìˆ˜ ì„¤ì • {key}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"â„¹ï¸  {key} ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)")
    return None


NOTION_DATABASE_ID = get_var('NOTION_DB_ID_INJOY_MONITORINGDATA_CONSUMER', "230ea67a568180c591fee27d4e90e001")
## ë³µì œ DBì— ì‚¬ìš© ì‹œ
# NOTION_DATABASE_ID = "30dea67a56818034bb5ae80442f5b0d6"

def get_notion_headers():
    """Notion API í—¤ë” ìƒì„±"""
    return {
        "Authorization": f"Bearer {get_var('NOTION_TOKEN', required=True)}",
        "Notion-Version": get_var("NOTION_API_VERSION", "2022-06-28"),
        "Content-Type": "application/json"
    }


def generate_row_key(row: dict) -> str:
    """ê³ ìœ  í‚¤ ìƒì„±: ì‚¬ìš©ì_ìŠ¤í˜ì´ìŠ¤id_ëŒ€í™”id_ë©”ì‹œì§€id"""
    
    def clean_key_value(val) -> str:
        # 1. Null, NaN, None ì²˜ë¦¬ -> ë¹ˆ ë¬¸ìì—´ë¡œ í†µì¼
        if pd.isna(val) or val is None:
            return ""
        
        s = str(val).strip()
        
        # 2. íŒŒì´ì¬ì´ 'nan' ì´ë¼ëŠ” ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´ë²„ë¦° ê²½ìš° ë°©ì–´
        if s.lower() == 'nan':
            return ""
            
        # 3. Pandas ì‹¤ìˆ˜í˜• ë³€í™˜ ë°©ì–´ (ì˜ˆ: 1234.0 -> 1234)
        if s.endswith('.0'):
            s = s[:-2]
            
        return s

    user = clean_key_value(row.get('ì‚¬ìš©ì'))
    space_id = clean_key_value(row.get('ìŠ¤í˜ì´ìŠ¤id'))
    convo_id = clean_key_value(row.get('ëŒ€í™”id'))
    msg_id = clean_key_value(row.get('ë©”ì‹œì§€id'))
    
    return f"{user}_{space_id}_{convo_id}_{msg_id}"


def build_properties_payload(row_data: dict) -> dict:
    """DataFrame í–‰ì„ Notion API propertiesë¡œ ë³€í™˜"""
    properties = {}

    for key, value in row_data.items():
        # ğŸ“Œ 1. ë¨¼ì € array/list íƒ€ì… í™•ì¸
        if isinstance(value, (list, tuple, pd.Series)):
            # ë¹ˆ ë°°ì—´ì´ë©´ continue
            if len(value) == 0:
                continue
        else:
            # ì¼ë°˜ ê°’ì€ pd.isna() ì‚¬ìš©
            if pd.isna(value):
                continue

        if key == "ì‚¬ìš©ì ì§ˆì˜":
            content = str(value or "")
            if len(content) > 2000:
                print(f"  -> âš ï¸ ê²½ê³ : ì œëª© í•„ë“œëŠ” 2000ìë¡œ ì œí•œë©ë‹ˆë‹¤.")
                content = content[:2000]
            properties[key] = {"title": [{"text": {"content": content}}]}

        elif key == "ì§ˆë¬¸ë‚ ì§œ":
            properties[key] = {"date": {"start": str(value or "")}}

        elif key == "ì‘ë‹µì†ë„(ì´ˆ)":
            try:
                numeric_value = float(value)
                properties[key] = {"number": numeric_value}
            except (ValueError, TypeError):
                continue
        
        elif key == "ì¿¼ë¦¬":
            content = str(value or "")
            if len(content) > 2000:
                print(f"  -> âœ¨ '{key}'ì˜ ê¸´ í…ìŠ¤íŠ¸({len(content)}ì)ë¥¼ ë¶„í• í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.")
                text_chunks = []
                for i in range(0, len(content), 2000):
                    chunk = content[i:i + 2000]
                    text_chunks.append({"type": "text", "text": {"content": chunk}})
                properties[key] = {"rich_text": text_chunks}
            else:
                properties[key] = {"rich_text": [{"text": {"content": content}}]}

        elif key in ["status", "error_type", "feedback_rating", "ìŠ¤í˜ì´ìŠ¤ëª…"]:
            # ì„ íƒ(Select) ì†ì„±ì€ ê°’ì´ ë¹„ì–´ìˆì„ ë•Œ ë¹ˆ ë¬¸ìì—´("")ì„ ë³´ë‚´ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ë”°ë¼ì„œ ê°’ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ select payloadë¥¼ ìƒì„±í•˜ë„ë¡ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
            val_str = str(value or "").strip()
            if val_str:
                properties[key] = {"select": {"name": val_str}}

        elif key in ['row_count', 'auto_regenerate_count']:
            try:
                int_value = int(value)
                properties[key] = {"number": int_value}
            except (ValueError, TypeError):
                continue

        # ğŸ“Œ 2. question ë°°ì—´ ì²˜ë¦¬ (ìƒˆë¡œ ì¶”ê°€)
        elif key in ["questions"]:
            # valueê°€ array/listì¸ì§€ í™•ì¸
            if isinstance(value, (list, tuple, pd.Series)):
                # ë°°ì—´ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                question_list = []
                
                for item in value:
                    if pd.notna(item):  # None/NaN ì œì™¸
                        question_list.append(str(item).strip())
                
 
                question_str = "; ".join(question_list) if question_list else ""
                print(f"&&&&&&Question_str: {question_str}")
                if question_str:
                    # 2000ì ì œí•œ í™•ì¸
                    if len(question_str) > 2000:
                        question_str = question_str[:1999]
                    
                    properties[key] = {"rich_text": [{"text": {"content": question_str}}]}
            else:
                # valueê°€ ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš° (ë‹¨ì¼ ë¬¸ìì—´)
                content = str(value or "")
                if content:
                    properties[key] = {"rich_text": [{"text": {"content": content}}]}
            
            continue  # ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ

        elif key in ["error", "ì—ëŸ¬"]:
            content = str(value or "")
            if len(content) > 2000:
                print(f"  -> âœ¨ '{key}'ì˜ ê¸´ í…ìŠ¤íŠ¸({len(content)}ì)ë¥¼ ì˜ë¼ì„œ ì €ì¥í•©ë‹ˆë‹¤.")
                content = content[:2000]
            
            properties[key] = {"rich_text": [{"text": {"content": content}}]}

        else:
            properties[key] = {"rich_text": [{"text": {"content": str(value or "")}}]}
            
    return properties


def get_all_notion_pages(database_id: str, headers: dict) -> list:
    """Notion DBì˜ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)"""
    url = f"https://api.notion.com/v1/databases/{database_id}/query"
    results = []
    has_more = True
    next_cursor = None
    
    print("â³ Notion DBì—ì„œ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    while has_more:
        payload = {"page_size": 100}
        if next_cursor:
            payload["start_cursor"] = next_cursor
        
        try:
            res = requests.post(url, headers=headers, json=payload)
            res.raise_for_status()
            data = res.json()
            results.extend(data.get("results", []))
            next_cursor = data.get("next_cursor")
            has_more = data.get("has_more", False)
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion API ì—ëŸ¬ ë°œìƒ: {e}")
            break
        time.sleep(0.3)
        
    print(f"âœ… Notion DBì—ì„œ ì´ {len(results)}ê°œì˜ í˜ì´ì§€ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.")
    return results


# ============================================================
# Task í•¨ìˆ˜ë“¤
# ============================================================
def extract_data(**context):
    """Databricksì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    print("=" * 50)
    print("Step 1: Databricksì—ì„œ ë°ì´í„° ì¡°íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # Databricks ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    DATABRICKS_SERVER_HOSTNAME = get_var('DATABRICKS_SERVER_HOSTNAME', required=True)
    DATABRICKS_HTTP_PATH = get_var('databricks_http_path', required=True)
    DATABRICKS_TOKEN = get_var('databricks_token', required=True)
    
    try:
        # Databricks SQL ì—°ê²°
        print("ğŸ”— Databricks SQLì— ì—°ê²° ì¤‘...")
        connection = sql.connect(
            server_hostname=DATABRICKS_SERVER_HOSTNAME,
            http_path=DATABRICKS_HTTP_PATH,
            access_token=DATABRICKS_TOKEN
        )
        
        sql_query = """
            SELECT
                content,
                user_email,
                space_name,
                space_id,
                conversation_id,
                message_id,
                query,
                message_response_duration_seconds,
                event_time_kst as event_time_kst,
                row_count,
                status,
                description,
                questions,
                auto_regenerate_count,
                error,
                error_type,
                feedback_rating
            FROM 
                datahub.injoy_ops_schema.injoy_monitoring_data
            ORDER BY conversation_id, event_time_kst
        """
        
        print("ğŸ“Š SQL ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
        cursor = connection.cursor()
        cursor.execute(sql_query)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()
        source_df = pd.DataFrame(rows, columns=columns)
        
        # í•˜ë“œì½”ë”©ìœ¼ë¡œ ìœ ì € ì œì™¸
        exclude_emails = ['heegle@joycity.com', 'kimjack415@joycity.com']
        source_df = source_df[~source_df['user_email'].isin(exclude_emails)]

        cursor.close()
        connection.close()
        
        print(f"âœ… Databricksì—ì„œ ì´ {len(source_df)}ê°œì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.")
        email_list = source_df['user_email'].tolist()
        
        
    except ImportError:
        print("âŒ databricks-sql-connector ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install databricks-sql-connector")
        raise
    except Exception as e:
        print(f"âŒ Databricks ì—°ê²° ë˜ëŠ” ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise
    
    print("=" * 50)
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='source_data', value=source_df.to_json(orient='records', date_format='iso'))


def transform_data(**context):
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³€í™˜"""
    print("\n" + "=" * 50)
    print("Step 2: ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    source_data_json = context['ti'].xcom_pull(key='source_data', task_ids='extract_data')
    source_df = pd.read_json(source_data_json, orient='records')
    
    # Notion DB ì»¬ëŸ¼ëª…ì— ë§ê²Œ ë³€ê²½
    df_renamed = source_df.rename(columns={
        "content": "ì‚¬ìš©ì ì§ˆì˜",
        "user_email": "ì‚¬ìš©ì",
        "space_name": "ìŠ¤í˜ì´ìŠ¤ëª…",
        "space_id": "ìŠ¤í˜ì´ìŠ¤id",
        "conversation_id": "ëŒ€í™”id",
        "message_id": "ë©”ì‹œì§€id",
        "query": "ì¿¼ë¦¬",
        "message_response_duration_seconds": "ì‘ë‹µì†ë„(ì´ˆ)",
        "event_time_kst": "ì§ˆë¬¸ë‚ ì§œ"
    })
    print("ğŸ”„ ì»¬ëŸ¼ëª…ì„ Notion DBì— ë§ê²Œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")

    # ì¤‘ë³µëœ í‚¤ê°€ ìˆë‹¤ë©´ ê°€ì¥ ë§ˆì§€ë§‰(ìµœì‹ ) ë°ì´í„° í•˜ë‚˜ë§Œ ë‚¨ê¸°ê¸°
    df_renamed = df_renamed.drop_duplicates(
        subset=["ì‚¬ìš©ì", "ìŠ¤í˜ì´ìŠ¤id", "ëŒ€í™”id", "ë©”ì‹œì§€id"], 
        keep="last"
    )
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    s = pd.to_datetime(df_renamed['ì§ˆë¬¸ë‚ ì§œ'], errors='coerce')
    
    # íƒ€ì„ì¡´ ì²˜ë¦¬: ì´ë¯¸ íƒ€ì„ì¡´ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì²˜ë¦¬
    if s.dt.tz is None:
        # íƒ€ì„ì¡´ì´ ì—†ëŠ” ê²½ìš°: tz_localize ì‚¬ìš©
        print("â„¹ï¸  íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. Asia/Seoulë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        s = s.dt.tz_localize('Asia/Seoul', ambiguous='infer', nonexistent='shift_forward')
    else:
        # ì´ë¯¸ íƒ€ì„ì¡´ì´ ìˆëŠ” ê²½ìš°: tz_convert ì‚¬ìš©
        print(f"â„¹ï¸  ê¸°ì¡´ íƒ€ì„ì¡´({s.dt.tz})ì„ Asia/Seoulë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        print(f"â„¹ï¸ ê¸°ì¡´ íƒ€ì„ì¡´ {s.dt.tz}")
        s = s.dt.tz_convert('Asia/Seoul')
        print(f"â„¹ï¸ ë³€ê²½ íƒ€ì„ì¡´ {s.dt.tz}")
    
    # ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df_renamed['ì§ˆë¬¸ë‚ ì§œ'] = s.apply(lambda x: x.isoformat(timespec='seconds') if pd.notna(x) else None)
    df_renamed.loc[s.isna(), 'ì§ˆë¬¸ë‚ ì§œ'] = None
    print("ğŸ”„ 'ì§ˆë¬¸ë‚ ì§œ' ì»¬ëŸ¼ì„ Notion í‘œì¤€ ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
    
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)
    
    # XComìœ¼ë¡œ ì „ë‹¬
    # [í•µì‹¬ ìˆ˜ì • 2] Keyë¡œ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° íƒ€ì… ì—„ê²© í†µì œ
    def safe_string_convert(val):
        if pd.isna(val):
            return ""
        s = str(val)
        return s[:-2] if s.endswith('.0') else s  # 1234.0 -> 1234 ë¡œ ì •ë¦¬

    key_columns = ["ì‚¬ìš©ì", "ìŠ¤í˜ì´ìŠ¤id", "ëŒ€í™”id", "ë©”ì‹œì§€id"]
    for col in key_columns:
        df_renamed[col] = df_renamed[col].apply(safe_string_convert)

    context['ti'].xcom_push(key='transformed_data', value=df_renamed.to_json(orient='records', date_format='iso'))


def load_to_notion(**context):
    """Notion DBì— ë°ì´í„° ì ì¬"""
    print("\n" + "=" * 50)
    print("Step 3: Notion DB ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    NOTION_DB_ID = get_var('NOTION_DB_ID_INJOY_MONITORINGDATA_CONSUMER', "230ea67a568180c591fee27d4e90e001")
    headers = get_notion_headers()
    
    # XComì—ì„œ ë³€í™˜ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    transformed_data_json = context['ti'].xcom_pull(key='transformed_data', task_ids='transform_data')
    df_renamed = pd.read_json(transformed_data_json, orient='records')
    
    # Notion ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
    print("\nStep 3-1: Notion DBì˜ ê¸°ì¡´ ë°ì´í„° ê³ ìœ  í‚¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
    notion_pages = get_all_notion_pages(NOTION_DB_ID, headers)
    
    existing_keys = set()
    for page in notion_pages:
        props = page.get("properties", {})
        
        user_rt = props.get('ì‚¬ìš©ì', {}).get('rich_text', [])
        space_id_rt = props.get('ìŠ¤í˜ì´ìŠ¤id', {}).get('rich_text', [])
        convo_id_rt = props.get('ëŒ€í™”id', {}).get('rich_text', [])
        msg_id_rt = props.get('ë©”ì‹œì§€id', {}).get('rich_text', [])
        status_rt = props.get('status', {}).get('rich_text', [])
        error_type_rt = props.get('error_type', {}).get('rich_text', [])
        feedback_rating_rt = props.get('feedback_rating', {}).get('rich_text', [])
        
        row_values = {
            'ì‚¬ìš©ì': user_rt[0].get('plain_text', '') if user_rt else '',
            'ìŠ¤í˜ì´ìŠ¤id': space_id_rt[0].get('plain_text', '') if space_id_rt else '',
            'ëŒ€í™”id': convo_id_rt[0].get('plain_text', '') if convo_id_rt else '',
            'ë©”ì‹œì§€id': msg_id_rt[0].get('plain_text', '') if msg_id_rt else '',
            'status': status_rt[0].get('select', '') if status_rt else '',
            'error_type': error_type_rt[0].get('select', '') if error_type_rt else '',
            'feedback_rating': feedback_rating_rt[0].get('select', '') if feedback_rating_rt else ''
        }

        key = generate_row_key(row_values)
        if key:
            existing_keys.add(key)
            
    print(f"âœ… Notion DBì— ì¡´ì¬í•˜ëŠ” ê³ ìœ  í‚¤ {len(existing_keys)}ê°œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
    
    # ì‹ ê·œ ë°ì´í„°ë§Œ ì¶”ê°€
    print(f"\nStep 3-2: ì‹ ê·œ ë°ì´í„° ì¶”ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    insert_count = 0
    
    for index, row in df_renamed.iterrows():
        row_dict = row.to_dict()
        key = generate_row_key(row_dict)
        
        if key not in existing_keys:
            insert_count += 1
            print(f"  -> [{insert_count}] ì‹ ê·œ ë°ì´í„° ë°œê²¬! Notionì— ì¶”ê°€í•©ë‹ˆë‹¤. (Key: {key})")
            
            properties_payload = build_properties_payload(row_dict)
            payload = {"parent": {"database_id": NOTION_DB_ID}, "properties": properties_payload}
            
            res = requests.post("https://api.notion.com/v1/pages", headers=headers, json=payload)
            
            if not res.ok:
                print(f"    âŒ ì¶”ê°€ ì‹¤íŒ¨! (Key: {key}) - ì—ëŸ¬: {res.text}")
            else:
                # [í•µì‹¬ ìˆ˜ì • 1] ì„±ê³µ ì‹œ existing_keysì— ì¶”ê°€í•˜ì—¬ ë™ì¼ ë°°ì¹˜ ë‚´ ì¤‘ë³µ ë°©ì§€
                existing_keys.add(key)
                print(f"    âœ… ì¶”ê°€ ì„±ê³µ! (Key: {key})")
            
            time.sleep(0.3)

    if insert_count == 0:
        print("âœ… ì¶”ê°€í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ… ì´ {insert_count}ê°œì˜ ì‹ ê·œ ë°ì´í„°ë¥¼ Notionì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

    print("\nâœ¨ ëª¨ë“  ë™ê¸°í™” ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! âœ¨")
    print("=" * 50)


# ============================================================
# DAG ì •ì˜
# ============================================================

injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

with DAG(
    dag_id='injoy_monitoringdata_consumer_2',
    default_args=default_args,
    description='Databricks ë°ì´í„°ë¥¼ Notion DBì— ë™ê¸°í™”í•˜ëŠ” DAG',
    schedule=[injoy_monitoringdata_producer],
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['notion', 'sync', 'monitoring'],
) as dag:

    # Task 1: ë°ì´í„° ì¶”ì¶œ
    extract_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
        dag=dag,
    )

    # Task 2: ë°ì´í„° ë³€í™˜
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        dag=dag,
    )

    # Task 3: Notion ì ì¬
    load_task = PythonOperator(
        task_id='load_to_notion',
        python_callable=load_to_notion,
        dag=dag,
    )

    # Task ì˜ì¡´ì„± ì„¤ì •
    extract_task >> transform_task >> load_task