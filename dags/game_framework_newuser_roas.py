import time
import pandas as pd
from google.cloud import bigquery
from google import genai
from google.genai import types
from google.cloud import storage
import vertexai
from google.genai import Client
from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore

# ì¸ì¦ê´€ë ¨
import google.auth
from google.auth.transport.requests import Request
import logging

# ê·¸ë˜í”„ ê´€ë ¨ íŒ¨í‚¤ì§€
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter, StrMethodFormatter, PercentFormatter, MultipleLocator
import matplotlib as mpl
import matplotlib.font_manager as fm
from matplotlib import cm
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont # 2ê°€ì§€ íŒŒì¼ í•©ì¹˜ê¸°
import matplotlib.dates as mdates
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio
import IPython.display as IPd
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List, Tuple
from matplotlib import rcParams
from matplotlib.patches import Rectangle

# ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€
import numpy as np
import re
import os 
import math
import time
import pandas as pd
from notion_client import Client
import requests
import json
from datetime import datetime, timezone, timedelta
from adjustText import adjust_text
from airflow.models import Variable
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
from pathlib import Path
from airflow.sdk import get_current_context
from game_framework_util import *

PROJECT_ID = "data-science-division-216308"
LOCATION = "us-central1"

## ì‹ ê·œ ìœ ì € íšŒìˆ˜ í˜„í™©
## 6_newuser_roas

def result6_monthlyROAS(joyplegameid:int, gameidx:str, bigquery_client, bucket, **context):
    
    query= f"""
    WITH revraw AS(
    select JoypleGameID, Month
    , concat("D_",date_diff(date_sub(current_date('Asia/Seoul'), interval 1 day), date_sub(cast(concat(Month, '-01') as date), interval 1 day), day)) as matured_daydiff
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as Month,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID = {joyplegameid}
        and RegdateAuthAccountDateKST >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
        and RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    group by JoypleGameID, RegdateAuthAccountDateKST
    ) group by JoypleGameID, month
    )


    , final AS(
    select  JoypleGameID,  Month, matured_daydiff, RU,
    Sales_D1/RU as D1_LTV,
    Sales_D3/RU as D3_LTV,
    Sales_D7/RU as D7_LTV,
    Sales_D14/RU as D14_LTV,
    Sales_D30/RU as D30_LTV,
    Sales_D60/RU as D60_LTV,
    Sales_D90/RU as D90_LTV,
    Sales_D120/RU as D120_LTV,
    Sales_D150/RU as D150_LTV,
    Sales_D180/RU as D180_LTV,
    Sales_D210/RU as D210_LTV,
    Sales_D240/RU as D240_LTV,
    Sales_D270/RU as D270_LTV,
    Sales_D300/RU as D300_LTV,
    Sales_D330/RU as D330_LTV,
    Sales_D360/RU as D360_LTV,
    Sales_D14_p/RU as D14_LTV_p,
    Sales_D30_p/RU as D30_LTV_p,
    Sales_D60_p/RU as D60_LTV_p,
    Sales_D90_p/RU as D90_LTV_p,
    Sales_D120_p/RU as D120_LTV_p,
    Sales_D150_p/RU as D150_LTV_p,
    Sales_D180_p/RU as D180_LTV_p,
    Sales_D210_p/RU as D210_LTV_p,
    Sales_D240_p/RU as D240_LTV_p,
    Sales_D270_p/RU as D270_LTV_p,
    Sales_D300_p/RU as D300_LTV_p,
    Sales_D330_p/RU as D330_LTV_p,
    Sales_D360_p/RU as D360_LTV_p,
    D1D3_avg,  D3D7_avg , Sales_D7/RU as kpi_d7
    from(
    select *,
    CASE WHEN Sales_D14 is not null then null  ELSE  Sales_D7*D7D14_avg END as Sales_D14_p,
    CASE WHEN Sales_D30 is not null then null
    WHEN Sales_D30 is null and Sales_D14 is not null THEN Sales_D14*D14D30_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg END as Sales_D30_p,
    CASE WHEN Sales_D60 is not null then null
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg END as Sales_D60_p,
    CASE WHEN Sales_D90 is not null then null
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg END as Sales_D90_p,
    CASE WHEN Sales_D120 is not null then null
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg  END as Sales_D120_p,
    CASE WHEN Sales_D150 is not null then null
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg   END as Sales_D150_p,
    CASE WHEN Sales_D180 is not null then null
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg    END as Sales_D180_p,
    CASE WHEN Sales_D210 is not null then null
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg     END as Sales_D210_p,
    CASE WHEN Sales_D240 is not null then null
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg      END as Sales_D240_p,
    CASE WHEN Sales_D270 is not null then null
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg       END as Sales_D270_p,
    CASE WHEN Sales_D300 is not null then null
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg        END as Sales_D300_p,
    CASE WHEN Sales_D330 is not null then null
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg         END as Sales_D330_p,
    CASE WHEN Sales_D360 is not null then null
    WHEN Sales_D330 is not null THEN Sales_D330*D330D360_avg
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg *D330D360_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg*D330D360_avg          END as Sales_D360_p
    from(

    select *
    ,  LAST_VALUE(d1d3_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D1D3_avg
    ,  LAST_VALUE(d3d7_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D3D7_avg
    ,  LAST_VALUE(d7d14_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D7D14_avg
    ,  LAST_VALUE(d14d30_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D14D30_avg
    ,  LAST_VALUE(d30d60_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D30D60_avg
    ,  LAST_VALUE(d60d90_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D60D90_avg
    ,  LAST_VALUE(d90d120_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d90d120_avg
    ,  LAST_VALUE(d120d150_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d120d150_avg
    ,  LAST_VALUE(d150d180_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d150d180_avg
    ,  LAST_VALUE(d180d210_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d180d210_avg
    ,  LAST_VALUE(d210d240_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d210d240_avg
    ,  LAST_VALUE(d240d270_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d240d270_avg
    ,  LAST_VALUE(d270d300_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d270d300_avg
    ,  LAST_VALUE(d300d330_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d300d330_avg
    ,  LAST_VALUE(d330d360_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d330d360_avg

    from(
    select *,
    CASE WHEN Sales_D3 is null THEN null ELSE AVG(Sales_D3/Sales_D1) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING ) END AS d1d3_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©
    CASE WHEN Sales_D7 is null THEN null ELSE AVG(Sales_D7/Sales_D3) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING) END AS d3d7_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©

    CASE WHEN Sales_D14 is null THEN null ELSE AVG(Sales_D14/Sales_D7) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d7d14_avg3,
    CASE WHEN Sales_D30 is null THEN null ELSE AVG(Sales_D30/Sales_D14) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d14d30_avg3,
    CASE WHEN Sales_D60 is null THEN null ELSE AVG(Sales_D60/Sales_D30) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d30d60_avg3,
    CASE WHEN Sales_D90 is null THEN null ELSE AVG(Sales_D90/Sales_D60) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d60d90_avg3,
    CASE WHEN Sales_D120 is null THEN null ELSE AVG(Sales_D120/Sales_D90) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d90d120_avg3,
    CASE WHEN Sales_D150 is null THEN null ELSE AVG(Sales_D150/Sales_D120) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d120d150_avg3,
    CASE WHEN Sales_D180 is null THEN null ELSE AVG(Sales_D180/Sales_D150) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d150d180_avg3,
    CASE WHEN Sales_D210 is null THEN null ELSE AVG(Sales_D210/Sales_D180) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d180d210_avg3,
    CASE WHEN Sales_D240 is null THEN null ELSE AVG(Sales_D240/Sales_D210) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d210d240_avg3,
    CASE WHEN Sales_D270 is null THEN null ELSE AVG(Sales_D270/Sales_D240) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d240d270_avg3,
    CASE WHEN Sales_D300 is null THEN null ELSE AVG(Sales_D300/Sales_D270) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d270d300_avg3,
    CASE WHEN Sales_D330 is null THEN null ELSE AVG(Sales_D330/Sales_D300) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d300d330_avg3,
    CASE WHEN Sales_D360 is null THEN null ELSE AVG(Sales_D360/Sales_D330) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d330d360_avg3
    from revraw
    )
    )
    )
    )

    ,final2 AS(
    select a.*, b.cost, b.cost_exclude_credit
    from final as a
    left join (
    select joyplegameid,  format_date('%Y-%m', cmpgndate) as month
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    and cmpgndate <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    group by joyplegameid,  format_date('%Y-%m', cmpgndate)

    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.month = b.month
    )

    select month as `ê°€ì…ì›”`,
    matured_daydiff as `ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„`,
    cost_exclude_credit as `ë§ˆì¼€íŒ… ë¹„ìš©`,
    ru*d1_ltv/cost_exclude_credit as `ROAS D1`,
    ru*d3_ltv/cost_exclude_credit as `ROAS D3`,
    ru*d7_ltv/cost_exclude_credit as `ROAS D7`,
    ru*d14_ltv/cost_exclude_credit as `ROAS D14`,
    ru*d30_ltv/cost_exclude_credit as `ROAS D30`,
    ru*d60_ltv/cost_exclude_credit as `ROAS D60`,
    ru*d90_ltv/cost_exclude_credit as `ROAS D90`,
    ru*d120_ltv/cost_exclude_credit as `ROAS D120`,
    ru*d150_ltv/cost_exclude_credit as `ROAS D150`,
    ru*d180_ltv/cost_exclude_credit as `ROAS D180`,
    ru*d210_ltv/cost_exclude_credit as `ROAS D210`,
    ru*d240_ltv/cost_exclude_credit as `ROAS D240`,
    ru*d270_ltv/cost_exclude_credit as `ROAS D270`,
    ru*d300_ltv/cost_exclude_credit as `ROAS D300`,
    ru*d330_ltv/cost_exclude_credit as `ROAS D330`,
    ru*d360_ltv/cost_exclude_credit as `ROAS D360`,
    ru*d14_ltv_p/cost_exclude_credit as `ROAS D14 ì˜ˆì¸¡ì¹˜`,
    ru*d30_ltv_p/cost_exclude_credit as `ROAS D30 ì˜ˆì¸¡ì¹˜`,
    ru*d60_ltv_p/cost_exclude_credit as `ROAS D60 ì˜ˆì¸¡ì¹˜`,
    ru*d90_ltv_p/cost_exclude_credit as `ROAS D90 ì˜ˆì¸¡ì¹˜`,
    ru*d120_ltv_p/cost_exclude_credit as `ROAS D120 ì˜ˆì¸¡ì¹˜`,
    ru*d150_ltv_p/cost_exclude_credit as `ROAS D150 ì˜ˆì¸¡ì¹˜`,
    ru*d180_ltv_p/cost_exclude_credit as `ROAS D180 ì˜ˆì¸¡ì¹˜`,
    ru*d210_ltv_p/cost_exclude_credit as `ROAS D210 ì˜ˆì¸¡ì¹˜`,
    ru*d240_ltv_p/cost_exclude_credit as `ROAS D240 ì˜ˆì¸¡ì¹˜`,
    ru*d270_ltv_p/cost_exclude_credit as `ROAS D270 ì˜ˆì¸¡ì¹˜`,
    ru*d300_ltv_p/cost_exclude_credit as `ROAS D300 ì˜ˆì¸¡ì¹˜`,
    ru*d330_ltv_p/cost_exclude_credit as `ROAS D330 ì˜ˆì¸¡ì¹˜`,
    ru*d360_ltv_p/cost_exclude_credit as `ROAS D360 ì˜ˆì¸¡ì¹˜`
    from final2
    order by `ê°€ì…ì›”`
    """

    query_result6_monthlyROAS = query_run_method('6_newuser_roas', bigquery_client, query)
    query_result6_monthlyROAS['ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„'] = (
    query_result6_monthlyROAS['ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„'].astype(str).str.replace('_', '', regex=False)
    )

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result6_monthlyROAS, bucket, gcs_path)

    return saved_path



def result6_pLTV(joyplegameid:int, gameidx:str, bigquery_client, bucket, **context):

    ## pLTV D360
    query = f"""

    with perfo_raw AS(
    select a.*
    , b.countrycode, b.os
    , b.gcat, b.mediacategory, b.class, b.media, b.adsetname, b.adname, b.optim, b.oscam, b.geocam, b.targetgroup
    from(
    select *,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_low end as combined_rev_low,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_upp end as combined_rev_upp,
    FROM `data-science-division-216308.VU.Performance_pLTV`
    where authaccountregdatekst >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    and authaccountregdatekst <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    and JoypleGameID = {joyplegameid}
    ) as a
    left join (select  *
    from `dataplatform-reporting.DataService.V_0316_0000_AuthAccountInfo_V`
    where JoypleGameID = {joyplegameid}
    ) as b
    on a.authaccountname = b.authaccountname
    and a.joyplegameid = b.joyplegameid
    )

    select format_date('%Y-%m',AuthAccountRegDateKST) as `ê°€ì…ì›”`
        ,count(distinct if(daysfromregisterdate = 0, authaccountname, null)) as RU
        ,sum(if(daysfromregisterdate <= 360, combined_rev, null)) as `ë§¤ì¶œ D360 ì˜ˆì¸¡ì¹˜`
        , max(authaccountregdatekst) as `ìµœëŒ€ ê°€ì…ì¼ì`
    from perfo_raw
    group by joyplegameid, format_date('%Y-%m',AuthAccountRegDateKST)

    """
    query_result6_pLTV = query_run_method('6_newuser_roas', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result6_pLTV, bucket, gcs_path)

    return saved_path



##### ë³µê·€ ìœ ì € ë°ì´í„°
def result6_return(joyplegameid:int, gameidx:str, bigquery_client, bucket, **context):

    query = f"""
    with raw AS(
    select *
    , sum(d90diff) over(partition by joyplegameid, authaccountname order by logdatekst) as cum_d90diff
    from(
    select *
    , date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day ) as daydiff_beforeaccess   -- authaccountlastaccessbeforedatekst : Access ê¸°ì¤€ìœ¼ë¡œ ë¡œê¹…
    , case when  date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day )  >= 90 then 1 else 0  end as d90diff
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    WHERE joyplegameid = {joyplegameid}
    and logdatekst >= '2023-01-01'
    and DaysFromRegisterDate >= 0 -- ê°€ì…ì¼ì´ ì´í›„ì— ì°íŒ caseì œì™¸
    )
    )

    , raw2 AS(
    select *, date_diff(logdatekst, returndate, day) as daydiff_re -- ë³µê·€ì¼ cohort
    -- , if(returndate = AuthAccountRegDateKST, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    , if(cum_d90diff = 0, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    from(
    select *
    , first_value(logdatekst) over(partition by joyplegameid, authaccountname, cum_d90diff order by logdatekst) as returndate
    from raw
    )
    )

    , ru_raw AS(
    -- ì‹ ê·œ ìœ ì € ê¸°ì¤€
    select joyplegameid,  format_date('%Y-%m',authaccountregdatekst) as regmonth
    , count(distinct authaccountname) as ru
    , sum(if(DaysFromRegisterDate<=360, pricekrw, null)) as d360rev
    from raw2
    where  AuthAccountRegDateKST  >= '2023-01-01'
    and AuthAccountRegDateKST <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,    format_date('%Y-%m',authaccountregdatekst)
    )

    , return_raw AS(
    -- ë³µê·€ìœ ì €
    select joyplegameid,  format_date('%Y-%m', returndate) as regmonth
    , count(distinct if(daydiff_re = 0 , authaccountname, null)) as ru
    , sum(if(daydiff_re<=360, pricekrw, null)) as d360rev_all
    from raw2
    where  returndate  >= '2023-01-01'
    and returndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,   format_date('%Y-%m', returndate)
    )


    ,final AS(
    select
    ifnull(ifnull(a.joyplegameid , b.joyplegameid) , c.joyplegameid)  as joyplegameid
    ,ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) as regmonth
    , a.ru ,b.ru as ru_all,
    d360rev  AS rev_D360,
    d360rev_all  AS rev_D360_all
    ,  cost
    ,  cost_exclude_credit
    , d360rev_all - d360rev as rev_D360_return ,
    case WHEN DATE_DIFF(
            DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY),
            LAST_DAY(DATE(CONCAT(ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) , '-01'))),
            DAY
            ) >= 360 THEN 'mature'
        ELSE 'notmature'
    END AS status
    from ru_raw  as a
    full join return_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    full join (
            select joyplegameid,  format_date('%Y-%m',cmpgndate) as regmonth, sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
            from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
            where joyplegameid = 133
            and cmpgndate >='2023-01-01'
            and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
            group by  joyplegameid,  format_date('%Y-%m',cmpgndate)
            ) as c
            on a.joyplegameid = c.joyplegameid
    and a.regmonth = c.regmonth
    )

    #notmature êµ¬ê°„ ìµœê·¼ 6ê°œì›” í‰ê· 
    #POTC 2024/4 ~ 6ì›” ë¡œê·¸ì¸ ì´ìŠˆë¡œ ë³µê·€ìœ ì € ê¸°ì—¬ë„ ë‚®ì•„ ì œì™¸
    , return_user_proas AS(
    select joyplegameid, avg(rev_D360_return/cost_exclude_credit) as d360_return_roas
    , approx_quantiles(rev_D360_return/cost_exclude_credit, 2)[OFFSET(1)] as d360_return_roas_med
    from(
    select *, row_number() over (partition by joyplegameid order by regmonth desc) as rownum
    from final
    where status = 'mature'
    and (
        -- (joyplegameid = 131 and regmonth not in ('2024-04','2024-05','2024-06')) or joyplegameid in (133,30001,30003)
        joyplegameid = {joyplegameid}
        )
    )
    where rownum <= 6 -- ìµœê·¼ 6ê°œì›”
    group by joyplegameid
    )

    select a.regmonth as `ê°€ì…ì›”`
        , a.RU
        , a.RU_all
        , a.cost_exclude_credit as `ë§ˆì¼€íŒ… ë¹„ìš©`
        , rev_D360_return as `ë³µê·€ìœ ì € ë§¤ì¶œ D360`
        , status as `ë°ì´í„° ì™„ì„± ì—¬ë¶€`
    , rev_D360_return/cost_exclude_credit as `ë³µê·€ìœ ì € ROAS D360`
    , case when status = 'mature' then rev_D360_return/cost_exclude_credit
        else b.d360_return_roas_med end as `ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜`
    from final  as a
    left join return_user_proas as b
    on a.joyplegameid = b.joyplegameid
    """

    query_result6_return =query_run_method('6_newuser_roas', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result6_return, bucket, gcs_path)

    return saved_path



### ìˆ˜ìˆ˜ë£Œ ì ìš© BEP ê³„ì‚°
def result6_BEP(joyplegameid:int, gameidx:str, bigquery_client, bucket, **context):

    query = f"""
    with raw AS(
    select a.*, b.value
    , case when b.value is not null then b.value
    when b.value is null and a.PGName = 'Google' then 0.3
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 131 then 0.33
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 133 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30001 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30003 then 0.31
    when b.value is null and a.PGName = 'Xsolla' and a.joyplegameid = 131 then 0.15
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 133 then 0.10
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30001 then 0.09
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30003 then 0.08
    when b.value is null and  a.PGName = 'Danal' then 0.03
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 131 then 0.3
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 133 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30001 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30003 then 0.24
    when b.value is null and  a.PGName = 'Facebook Gaming' then 0.3
    when b.value is null and a.PGName = 'Steam' then 0.3
    else 0.3
    end as commission_rate
    from(
    select JoypleGameID, format_date('%Y-%m', authaccountregdatekst) as regmonth , t2.PGName, sum(t2.PGPriceKRW) as sales
    from  dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = {joyplegameid}
    and authaccountregdatekst >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    group by JoypleGameID, format_date('%Y-%m', authaccountregdatekst) , t2.PGName
    ) as a
    left join (
    select joyplegameid, regmonth, PGName,  value
    , case when PGName = 'One_Store' then 'One Store'
    when PGName = 'Facebook_Gaming' then 'Facebook Gaming'
    else PGName end as pgname2
    from `data-science-division-216308.Common.pg_commission_rate_copy2`
    unpivot (
    value for PGName in (Google,Apple, Xsolla	,Danal,	One_Store	,Facebook_Gaming,	Steam)
    )
    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.pgname = b.pgname2
    )

    -- BEP ê³„ì‚°

    select distinct regmonth as `ê°€ì…ì›”`, bep_commission as `ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP`
    from (
        select * , sum(sales) over(partition by joyplegameid , regmonth) as cumsales
                , sales /  sum(sales) over(partition by joyplegameid , regmonth) as sales_p
                , sum(commission) over(partition by joyplegameid , regmonth) as cumcommission
                , sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth) as total_commssion_rate
                , case when joyplegameid in (131,133) then 1/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
                        when joyplegameid in (30001,30003) then 1.08/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
                    end as bep_commission
        from(
            select  *, sales*commission_rate as commission
            from raw
            )
        )
    order by `ê°€ì…ì›”`
    """

    query_result6_BEP =query_run_method('6_newuser_roas', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result6_BEP, bucket, gcs_path)

    return saved_path


### ROAS KPI
def result6_roaskpi(gameidx:str, bigquery_client, bucket, **context):

    query = f"""
    select kpi_d1, kpi_d3, kpi_d7, kpi_d14, kpi_d30, kpi_d60, kpi_d90, kpi_d120, kpi_d150, kpi_d180, kpi_d210, kpi_d240, kpi_d270, kpi_d300, kpi_d330, kpi_d360
    from
    (select * ,row_number() OVER (partition by project ORDER BY updateDate desc ) AS row_
    from `data-science-division-216308.MetaData.roas_kpi`
    where project='{gameidx}'
    and operationStatus = 'ìš´ì˜ ì¤‘')
    where row_=1

    """

    query_result6_roaskpi = query_run_method('6_newuser_roas', bigquery_client, query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result6_roaskpi, bucket, gcs_path)

    return saved_path


def roas_kpi(gameidx:str, path_result6_roaskpi:str, bucket, **context):

    query_result6_roaskpi = load_df_from_gcs(bucket=bucket, path=path_result6_roaskpi)

    query_result6_roaskpi = query_result6_roaskpi * 100
    data = query_result6_roaskpi.rename(columns={
            'kpi_d1' : 'ROAS D1',
            'kpi_d3' : 'ROAS D3',
            'kpi_d7' : 'ROAS D7',
            'kpi_d14' : 'ROAS D14',
            'kpi_d30' : 'ROAS D30',
            'kpi_d60' : 'ROAS D60',
            'kpi_d90' : 'ROAS D90',
            'kpi_d120' : 'ROAS D120',
            'kpi_d150' : 'ROAS D150',
            'kpi_d180' : 'ROAS D180',
            'kpi_d210' : 'ROAS D210',
            'kpi_d240' : 'ROAS D240',
            'kpi_d270' : 'ROAS D270',
            'kpi_d300' : 'ROAS D300',
            'kpi_d330' : 'ROAS D330',
            'kpi_d360' : 'ROAS D360'
            })
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    roas_kpi = pd.DataFrame(data)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(roas_kpi, bucket, gcs_path)

    return saved_path


###
def roas_dataframe_preprocessing(gameidx:str, path_result6_monthlyROAS:str, path_result6_pLTV:str, path_result6_return:str, path_result6_BEP:str, bucket, **context):

    query_result6_monthlyROAS = load_df_from_gcs(bucket, path_result6_monthlyROAS)
    query_result6_pLTV = load_df_from_gcs(bucket, path_result6_pLTV)
    query_result6_return = load_df_from_gcs(bucket, path_result6_return)
    query_result6_BEP = load_df_from_gcs(bucket, path_result6_BEP)

    query6_monthlyROAS = pd.merge(query_result6_monthlyROAS, query_result6_pLTV[['ê°€ì…ì›”', 'ë§¤ì¶œ D360 ì˜ˆì¸¡ì¹˜']], on = ['ê°€ì…ì›”'], how = "left")
    query6_monthlyROAS = pd.merge(query6_monthlyROAS
                                , query_result6_return[['ê°€ì…ì›”', 'ë³µê·€ìœ ì € ROAS D360', 'ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜', 'ë°ì´í„° ì™„ì„± ì—¬ë¶€']]
                                , on = ['ê°€ì…ì›”'], how = 'left')
    query6_monthlyROAS = pd.merge(query6_monthlyROAS, query_result6_BEP, on = ['ê°€ì…ì›”'], how = 'left')

    target_columns = ["ROAS D14","ROAS D30","ROAS D60","ROAS D90","ROAS D120",
            "ROAS D150","ROAS D180","ROAS D210","ROAS D240","ROAS D270",
            "ROAS D300","ROAS D330","ROAS D360"]

    ## ì‹¤ì¸¡ì¹˜ ì»¬ëŸ¼ ë¹ˆì¹¸ì— ì˜ˆì¸¡ì¹˜ ì»¬ëŸ¼ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    for col in target_columns:
        pred = f"{col} ì˜ˆì¸¡ì¹˜"
        if col in query6_monthlyROAS.columns and pred in query6_monthlyROAS.columns:
            query6_monthlyROAS[col] = query6_monthlyROAS[col].fillna(query6_monthlyROAS[pred])

    ## ì˜ˆì¸¡ì¹˜ ì»¬ëŸ¼ ì œì™¸(ë³µê·€ìœ ì € ì˜ˆì¸¡ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ë‘ê¸°)
    cols_to_drop = [
        c for c in query6_monthlyROAS.columns
        if ("ì˜ˆì¸¡ì¹˜" in c) and ("ë³µê·€ìœ ì €" not in c)
    ]
    query6_monthlyROAS = query6_monthlyROAS.drop(columns=cols_to_drop)

    ## ë³µê·€ìœ ì € í¬í•¨ D360 ROAS ê³„ì‚° -> mature ì•ˆëœ ê²½ìš° ì˜ˆì¸¡ì¹˜ë¡œ ê³„ì‚°
    mature_mask = query6_monthlyROAS['ë°ì´í„° ì™„ì„± ì—¬ë¶€'].eq('mature')

    query6_monthlyROAS['ë³µê·€ìœ ì € í¬í•¨ ROAS D360'] = np.where(
        mature_mask,
        query6_monthlyROAS['ROAS D360'] + query6_monthlyROAS['ë³µê·€ìœ ì € ROAS D360'],
        query6_monthlyROAS['ROAS D360'] + query6_monthlyROAS['ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜']
    )

    query6_monthlyROAS['ê¸°ë³¸ BEP'] = 1.429

    ## 2) ì»¬ëŸ¼ì„ 'ROAS D360' ë‹¤ìŒìœ¼ë¡œ ì´ë™
    cols = query6_monthlyROAS.columns.tolist()
    cols.remove('ë³µê·€ìœ ì € í¬í•¨ ROAS D360')
    cols.remove('ê¸°ë³¸ BEP')
    insert_at = cols.index('ROAS D360') + 1
    cols.insert(insert_at, 'ë³µê·€ìœ ì € í¬í•¨ ROAS D360')
    insert_at = cols.index('ë°ì´í„° ì™„ì„± ì—¬ë¶€') + 1
    cols.insert(insert_at, 'ê¸°ë³¸ BEP')
    cols.remove('ë°ì´í„° ì™„ì„± ì—¬ë¶€')
    insert_at = cols.index('ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP') + 1
    cols.insert(insert_at, 'ë°ì´í„° ì™„ì„± ì—¬ë¶€')
    query6_monthlyROAS = query6_monthlyROAS[cols]

    roas_days = [1, 3, 7, 14, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]

    # ì„±ì¥ì„¸ ì»¬ëŸ¼ ìƒì„±
    for i in range(1, len(roas_days)):
        prev_day = roas_days[i - 1]
        curr_day = roas_days[i]

        prev_col = f"ROAS D{prev_day}"
        curr_col = f"ROAS D{curr_day}"
        new_col = f"LTV ì„±ì¥ì„¸ D{curr_day}"

        query6_monthlyROAS[new_col] = query6_monthlyROAS[curr_col] / query6_monthlyROAS[prev_col]

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query6_monthlyROAS, bucket, gcs_path)

    return saved_path


########## ROAS í”„ë¡¬í”„íŠ¸
def result6_ROAS_gemini(gameidx:str, service_sub:str, path_monthlyBEP_ROAS:str, path_roas_kpi:str, bucket, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION, **context):

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    # KST íƒ€ì„ì¡´ ì •ì˜ (UTC+9)
    kst = timezone(timedelta(hours=9))

    # ì–´ì œ ë‚ ì§œ (KST ê¸°ì¤€)
    yesterday_kst = datetime.now(kst) - timedelta(days=1)

    # ì–´ì œ ë‚ ì§œì˜ ì—°ë„
    year = yesterday_kst.year

    #print("ì–´ì œ ë‚ ì§œ(KST):", yesterday_kst.date())
    #print("ì–´ì œ ì—°ë„:", year)

    query6_monthlyROAS = load_df_from_gcs(bucket=bucket, path=path_monthlyBEP_ROAS)
    roas_kpi = load_df_from_gcs(bucket=bucket, path=path_roas_kpi)

    response6_monthlyROAS = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents = f"""
    < ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ROAS>
    {query6_monthlyROAS.to_csv(index=False)}
    ë‹¤ìŒì€ ê°€ì…ì›”ë³„ ROAS ì•¼. "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ ROASëŠ” ì˜ˆì¸¡ì¹˜ì´ë‹ˆ, "ì§€í‘œí™•ì •  ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ ROASë¥¼ ì–¸ê¸‰í•  ë•Œì—ëŠ” ì˜ˆì¸¡ì¹˜ë¼ê³  ë§í•´ì¤˜.
    ì–´ëŠ ê°€ì…ì›”ì´ "ë³µê·€ìœ ì € í¬í•¨ ROAS D360"ì´ KPIë¥¼ ë‹¬ì„±í–ˆëŠ”ì§€, ë˜ëŠ” ë‹¬ì„±í•˜ì§€ ëª»í–ˆëŠ”ì§€ë¥¼ ì„œë‘ì— Bold ì²´ë¡œ í•œì¤„ë¡œ ì–¸ê¸‰í•´ì¤˜.
    {year} ì—°ë„ë§Œ ì ì–´ì¤˜.

    KPI ëŠ” ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP ë¡œ íŒë‹¨í•˜ë©´ë¼.

    ê·¸ë¦¬ê³  "ë³µê·€ìœ ì € í¬í•¨ ROAS D360" ì´ KPI ë‹¬ì„±í•˜ì§€ ëª»í•œ ì˜¬í•´ ì›”ë“¤ì€
    ì•„ë˜ ROAS KPI ì™€ ë¹„êµí•´ì„œ ì–´ë–¤ ì½”í˜¸íŠ¸ë¶€í„° ë¯¸ë‹¬í•˜ì—¬ ë‹¬ì„±í•˜ì§€ ëª»í–ˆë‹¤ê³  ê°„ë‹¨íˆ ì•Œë ¤ì¤˜.
    ë‹¬ì„±í•˜ì§€ ëª»í•œ ì›”ë“¤ë§Œ ì–¸ê¸‰í•´ì¤˜.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    ROASì™€ KPI ìˆ˜ì¹˜ëŠ” ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ %ë¡œ í‘œì‹œí•´ì¤˜.

    <ROAS KPI>
    {roas_kpi}

    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION
            #,tools=[RAG]
            ,temperature=0.1
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    # GCSì— ì—…ë¡œë“œ
    print("ğŸ“¤ GCSì— ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸ ì—…ë¡œë“œ ì¤‘...")
    gcs_response_path = f"{gameidx}/response6_monthlyROAS.text"
    blob = bucket.blob(gcs_response_path)
    blob.upload_from_string(
        response6_monthlyROAS.text,
        content_type='text/markdown; charset=utf-8'
    )
    
    
    return response6_monthlyROAS.text



########## LTV ì„±ì¥ì„¸ ë¶€ë¶„ í”„ë¡¬í”„íŠ¸
def monthlyLTVgrowth_gemini(service_sub:str, path_monthlyBEP_ROAS:str, bucket, genai_client, MODEL_NAME, SYSTEM_INSTRUCTION, **context):

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    query6_monthlyROAS = load_df_from_gcs(bucket=bucket, path=path_monthlyBEP_ROAS)

    response6_monthlyLTVgrowth = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    < ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ROAS>
    {query6_monthlyROAS.to_csv(index=False)}
    ë‹¤ìŒì€ ê°€ì…ì›”ë³„ ROASì™€ LTV ì„±ì¥ì„¸ì•¼. "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ LTV ì„±ì¥ì„¸ëŠ” ì˜ˆì¸¡ì¹˜ì´ë‹ˆ, "ì§€í‘œí™•ì •  ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ LTV ì„±ì¥ì„¸ë¥¼ ì–¸ê¸‰í•  ë•Œì—ëŠ” ì˜ˆì¸¡ì¹˜ë¼ê³  ë§í•´ì¤˜.
    ê°€ì…ì›”ì— ë”°ë¼ì„œ ì–´ëŠ êµ¬ê°„ì˜ ì„±ì¥ì„¸ê°€ ì¦ê°€í•˜ê±°ë‚˜, í•˜ë½í•˜ëŠ”ì§€ íŠ¸ë Œë“œë§Œ ì–¸ê¸‰í•´ì¤˜. e.g. D7 LTV ì„±ì¥ì„¸ê°€ 2025ë…„ 1ì›”ë¶€í„° í•˜ë½í–ˆìŠµë‹ˆë‹¤.
    êµ¬ê°„ì€ D3ë¶€í„° D30ê¹Œì§€ ì´ˆë°˜, D60ë¶€í„° D180ê¹Œì§€ ì¤‘ë°˜, D180ë¶€í„° D360ê¹Œì§€ëŠ” í›„ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê° êµ¬ê°„ì— ëŒ€í•œ íŠ¸ë Œë“œë¡œ ì–¸ê¸‰í•´ì¤˜.
    ê°€ì…ì›”ë³„ë¡œ í•˜ì§€ë§ê³  ê°€ì…ì›”ì— ë”°ë¥¸ ê° êµ¬ê°„ì˜ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•´ì„œ ì–¸ê¸‰í•˜ë˜, íŠ¹ì • ê°€ì…ì›”ì—ì„œ í¬ê²Œ ìƒìŠ¹í•˜ê±°ë‚˜ í¬ê²Œ í•˜ë½í–ˆë‹¤ë©´ ê·¸ ê°€ì…ì›”ì— ëŒ€í•´ì„œëŠ” ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì–¸ê¸‰í•´ì¤˜.
    LTV ì„±ì¥ì„¸ëŠ” ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ %ë¡œ í‘œì‹œí•´ì¤˜.
    10ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.1
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )
    return response6_monthlyLTVgrowth.text


### ROAS í˜„í™© ë° KPI í‘œ ì´ë¯¸ì§€ ìƒì„±
def roas_table_draw(gameidx:str, path_roas_dataframe_preprocessing:str, path_result6_monthlyROAS:str, bucket, gcs_bucket:str, **context):

    query6_monthlyROAS = load_df_from_gcs(bucket=bucket, path=path_roas_dataframe_preprocessing)
    query_result6_monthlyROAS = load_df_from_gcs(bucket=bucket, path=path_result6_monthlyROAS)

    df_numeric = query6_monthlyROAS.drop(columns=['ë°ì´í„° ì™„ì„± ì—¬ë¶€']).copy()
    df_numeric = df_numeric.reset_index(drop=True)

    nest_asyncio.apply()

    def infer_cohort_dn_map(df):
        cohort_map = {}
        for idx, row in df.iterrows():
            regmonth = idx[1] if isinstance(idx, tuple) else row.get('ê°€ì…ì›”', None)
            for col in df.columns:
                if re.search(r"ì˜ˆì¸¡ì¹˜$", col):
                    if pd.notna(row[col]):
                        dn = int(re.findall(r'\d+', col)[0])
                        # ì˜ˆì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê°€ì¥ ì‘ì€ dn ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
                        if regmonth not in cohort_map or dn < cohort_map[regmonth]:
                            cohort_map[regmonth] = dn
        return cohort_map
    
    cohort_dn_map = infer_cohort_dn_map(query_result6_monthlyROAS)

    ## ì»¬ëŸ¼ ì´ë¦„ì—ì„œ dn ê°’ ì¶”ì¶œ
    def extract_dn(col):
        match = re.match(r'(ROAS|LTV ì„±ì¥ì„¸) D(\d+)', col)
        return int(match.group(2)) if match else None

    # dn_valuesëŠ” <br> ì—†ëŠ” clean ì»¬ëŸ¼ëª… ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    dn_values = {col: extract_dn(col) for col in query6_monthlyROAS.columns if col.startswith("ROAS D") or col.startswith("LTV ì„±ì¥ì„¸ D")}

    # ê°œí–‰í•  ì»¬ëŸ¼ ì§€ì • ë° ê°œí–‰ ì…ë ¥í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
    custom_colnames = {
        "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„": "ì§€í‘œí™•ì •<br>ìµœëŒ€ê¸°ê°„",
        "ë³µê·€ìœ ì € í¬í•¨ ROAS D360": "ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360",
        "ë³µê·€ìœ ì € ROAS D360": "ë³µê·€ìœ ì €<br>ROAS D360",
        "ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜": "ë³µê·€ìœ ì €<br>ROAS D360<br>ì˜ˆì¸¡ì¹˜",
        "ê¸°ë³¸ BEP": "ê¸°ë³¸<br>BEP",
        "ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP": "ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP",
        "LTV ì„±ì¥ì„¸ D3": "LTV<br>ì„±ì¥ì„¸<br>D1 D3",
        "LTV ì„±ì¥ì„¸ D7": "LTV<br>ì„±ì¥ì„¸<br>D3 D7",
        "LTV ì„±ì¥ì„¸ D14": "LTV<br>ì„±ì¥ì„¸<br>D7 D14",
        "LTV ì„±ì¥ì„¸ D30": "LTV<br>ì„±ì¥ì„¸<br>D14 D30",
        "LTV ì„±ì¥ì„¸ D60": "LTV<br>ì„±ì¥ì„¸<br>D30 D60",
        "LTV ì„±ì¥ì„¸ D90": "LTV<br>ì„±ì¥ì„¸<br>D60 D90",
        "LTV ì„±ì¥ì„¸ D120": "LTV<br>ì„±ì¥ì„¸<br>D90 D120",
        "LTV ì„±ì¥ì„¸ D150": "LTV<br>ì„±ì¥ì„¸<br>D120 D150",
        "LTV ì„±ì¥ì„¸ D180": "LTV<br>ì„±ì¥ì„¸<br>D150 D180",
        "LTV ì„±ì¥ì„¸ D210": "LTV<br>ì„±ì¥ì„¸<br>D180 D210",
        "LTV ì„±ì¥ì„¸ D240": "LTV<br>ì„±ì¥ì„¸<br>D210 D240",
        "LTV ì„±ì¥ì„¸ D270": "LTV<br>ì„±ì¥ì„¸<br>D240 D270",
        "LTV ì„±ì¥ì„¸ D300": "LTV<br>ì„±ì¥ì„¸<br>D270 D300",
        "LTV ì„±ì¥ì„¸ D330": "LTV<br>ì„±ì¥ì„¸<br>D300 D330",
        "LTV ì„±ì¥ì„¸ D360": "LTV<br>ì„±ì¥ì„¸<br>D330 D360"
    }
    df_numeric = df_numeric.rename(columns=custom_colnames)

    ## ìŠ¤íƒ€ì¼ í•¨ìˆ˜ ì •ì˜ - mautred ë˜ì§€ ì•Šì€ êµ¬ê°„ íšŒìƒ‰ì²˜ë¦¬
    def highlight_based_on_dn(row):
        regmonth = row['ê°€ì…ì›”']
        cohort_dn = cohort_dn_map.get(regmonth, np.inf)

        styles = []
        for col in row.index:
            clean_col = col.replace("<br>", " ").strip()

            # ë‘ ìˆ«ìê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ dnìœ¼ë¡œ
            match = re.findall(r'D(\d+)', clean_col)
            dn_val = int(match[-1]) if match else None

            if (
                (clean_col.startswith('ROAS D') or clean_col.startswith('LTV ì„±ì¥ì„¸'))
                and dn_val is not None
                and dn_val >= cohort_dn
                and pd.notna(row[col])
            ):
                styles.append('background-color: lightgray')
            else:
                styles.append('')
        return styles

    #### roas ë‹¬ì„± êµ¬ê°„ ë¹¨ê°„ìƒ‰ ìŒì˜
    def highlight_roas_vs_bep(row):
        styles = []
        for col in row.index:
            style = ""
            try:
                # ê°’ ë³€í™˜
                if isinstance(row[col], str) and row[col].endswith('%'):
                    roas_val = float(row[col].replace('%', '')) / 100
                elif isinstance(row[col], (int, float)):
                    roas_val = row[col]
                else:
                    roas_val = None

                # ê¸°ì¤€ bep_base ë¹„êµ
                if col.startswith("ROAS D") and pd.notnull(row.get("ê¸°ë³¸<br>BEP")):
                    bep_val = row["ê¸°ë³¸<br>BEP"]
                    if pd.notnull(roas_val) and roas_val > bep_val:
                        style = "background-color: #fbe4e6"

                # d360 plus ë¹„êµ vs bep_commission
                elif col == "ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360" and pd.notnull(row.get("ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP")):
                    bep_comm = row["ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP"]
                    if pd.notnull(roas_val) and roas_val > bep_comm:
                        style = "background-color: #fbe4e6"

            except Exception as e:
                print(f"[DEBUG] {col} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                style = ""

            styles.append(style)
        return styles
    
    #### ROAS ìˆ˜ì¹˜ì˜ ë°” ì„œì‹ì„ ì»¬ëŸ¼ë³„ì´ ì•„ë‹Œ ì „ì²´ ìˆ˜ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì„œì‹ì ìš©ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ê°’ ì„¤ì •

    roas_cols = [c for c in df_numeric.columns if c.startswith("ROAS D")] + ["ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360"]

    # ì „ì²´ ìµœì†Œ/ìµœëŒ€ êµ¬í•˜ê¸°
    roas_global_min = df_numeric[roas_cols].min().min()
    roas_global_max = df_numeric[roas_cols].max().max()

    #### ì„±ì¥ì„¸ ìˆ˜ì¹˜ì˜ ë°” ì„œì‹ì„ ì»¬ëŸ¼ë³„ì´ ì•„ë‹Œ ì „ì²´ ìˆ˜ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì„œì‹ì ìš©ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ê°’ ì„¤ì •

    growth_cols = [c for c in df_numeric.columns if c.startswith("LTV<br>ì„±ì¥ì„¸<br>D")]

    # ì „ì²´ ìµœì†Œ/ìµœëŒ€ êµ¬í•˜ê¸°
    growth_global_min = df_numeric[growth_cols].min().min()
    growth_global_max = df_numeric[growth_cols].max().max()

    styled = (
        df_numeric.style
        .hide(axis="index")
        .format({
            "ë§ˆì¼€íŒ… ë¹„ìš©": "{:,.0f}",
            **{
                col: "{:.1%}"
                for col in df_numeric.columns
                if col.startswith("ROAS D")
                or col.startswith("LTV<br>ì„±ì¥ì„¸<br>D")
                or col.startswith("ë³µê·€ìœ ì €")
                or col.endswith("BEP")
            }
        })
        .bar(subset=["ë§ˆì¼€íŒ… ë¹„ìš©"], color="#f4cccc")
        .bar(subset=roas_cols, color="#c9daf8", vmin=roas_global_min, vmax=roas_global_max)
        .bar(subset=["ë³µê·€ìœ ì €<br>ROAS D360", "ë³µê·€ìœ ì €<br>ROAS D360<br>ì˜ˆì¸¡ì¹˜"], color="#ffe599")
        .bar(subset=growth_cols, color="#b5f7a3", vmin=growth_global_min, vmax=growth_global_max)
        .set_table_styles(
            [
                {"selector": "th", "props": [("background-color", "#f0f0f0"), ("font-weight", "bold"), ("border", "1px solid black")]},
                {"selector": "td", "props": [("border", "1px solid black")]}
            ]
        )
        # ê°•ì¡° í•¨ìˆ˜ ì ìš©
        .apply(highlight_based_on_dn, axis=1)
        .apply(highlight_roas_vs_bep, axis=1)
    )

    # HTML í…œí”Œë¦¿ ì •ì˜
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            table { border-collapse: collapse; font-size: 13px; }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: center;
            }
            th { background-color: #f0f0f0; }

            /* íŠ¹ì • ì»¬ëŸ¼ë³„ ìŠ¤íƒ€ì¼ - ì»¬ëŸ¼ìˆœì„œëŒ€ë¡œ*/
            th:nth-child(1), td:nth-child(1) { min-width: 50px; max-width: 60px; } /* ê°€ì…ì›” */
            th:nth-child(2), td:nth-child(2) { min-width: 60px; max-width: 65px; white-space: normal; } /* ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„ */
            th:nth-child(3), td:nth-child(3) { min-width: 70px; max-width: 95px; } /* ë§ˆì¼€íŒ… ë¹„ìš© */
            th:nth-child(20), td:nth-child(20) { min-width: 80px; white-space: normal; } /* ë³µê·€ìœ ì € í¬í•¨ ROAS D360 */
            th:nth-child(21), td:nth-child(21) { min-width: 75px; white-space: normal; } /* ë³µê·€ìœ ì € ROAS D360 */
            th:nth-child(22), td:nth-child(22) { min-width: 85px; white-space: normal; } /* ë³µê·€ìœ ì € í¬í•¨ ROAS D360 ì˜ˆì¸¡ì¹˜ */
            th:nth-child(24), td:nth-child(24) { min-width: 80px; white-space: normal; } /* ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP */
            th:nth-child(28), td:nth-child(28) { min-width: 55px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D14 D30 */
            th:nth-child(29), td:nth-child(29) { min-width: 55px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D30 D60 */
            th:nth-child(30), td:nth-child(30) { min-width: 55px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D60 D90 */
            th:nth-child(31), td:nth-child(31) { min-width: 60px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D90 D120 */
            th:nth-child(32), td:nth-child(32) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D120 D150 */
            th:nth-child(33), td:nth-child(33) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D150 D180 */
            th:nth-child(34), td:nth-child(34) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D180 D210 */
            th:nth-child(35), td:nth-child(35) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D210 D240 */
            th:nth-child(36), td:nth-child(36) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D240 D270 */
            th:nth-child(37), td:nth-child(37) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D270 D300 */
            th:nth-child(38), td:nth-child(38) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D300 D330 */
            th:nth-child(39), td:nth-child(39) { min-width: 70px; white-space: normal; } /* LTV ì„±ì¥ì„¸ D330 D360 */

        </style>
    </head>
    <body>
        <h2>{{ game_name }} GBTW ì‹ ê·œìœ ì € íšŒìˆ˜ í˜„í™©</h2>
        {{ table | safe }}
    </body>
    </html>
    """

    # html ì €ì¥
    table_html = styled.to_html()
    rendered_html = Template(html_template).render(table=table_html)
    html_path = "table6_monthlyROAS.html"
    with open(html_path, "w", encoding="utf-8") as f:
            f.write(rendered_html)

    # ì´ë¯¸ì§€ ìº¡ì²˜ ë¹„ë™ê¸° í•¨ìˆ˜
    async def capture_html_to_image(html_path, output_image_path):
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì—´ê¸°
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={"width": 1800, "height": 800})

            # HTML íŒŒì¼ ë¡œë“œ
            await page.goto("file://" + os.path.abspath(html_path))

            # ì´ë¯¸ì§€ ìº¡ì²˜
            await page.screenshot(path=output_image_path, full_page=True)

            # ë¸Œë¼ìš°ì € ë‹«ê¸°
            await browser.close()

    def capture_image_task(html_path, gcs_bucket, gcs_path, project_id=None):
        """ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ê³  GCSì— ì—…ë¡œë“œ"""
        import tempfile

        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì‘ì—…
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_image_path = os.path.join(temp_dir, 'screenshot.png')
            
            # 1. ì„ì‹œ íŒŒì¼ì— ì´ë¯¸ì§€ ìº¡ì²˜
            print(f"Capturing HTML to image: {html_path}")
            asyncio.run(capture_html_to_image(html_path, temp_image_path))
            print(f"Image saved temporarily to: {temp_image_path}")
            
            # 2. íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not os.path.exists(temp_image_path):
                raise FileNotFoundError(f"Screenshot file not created: {temp_image_path}")
            
            file_size = os.path.getsize(temp_image_path)
            print(f"File size: {file_size} bytes")
            
            # 3. GCSì— ì—…ë¡œë“œ
            print(f"Uploading to GCS: gs://{gcs_bucket}/{gcs_path}")
            client = storage.Client(project=project_id)
            bucket = client.bucket(gcs_bucket)
            blob = bucket.blob(gcs_path)
            blob.upload_from_filename(temp_image_path, content_type='image/png')
            
            gcs_uri = f"gs://{gcs_bucket}/{gcs_path}"
            print(f"Image successfully uploaded to: {gcs_uri}")
            
            return True

    html_path = "table6_monthlyROAS.html"  # HTML íŒŒì¼ ê²½ë¡œ
    output_image_path = "graph6_monthlyROAS.png"  # ì €ì¥ë  ì´ë¯¸ì§€ ê²½ë¡œ
    gcs_path = f'{gameidx}/graph6_monthlyROAS.png'  # GCSì— ì €ì¥ë  ê²½ë¡œ

    #í•¨ìˆ˜ í˜¸ì¶œ 
    capture_image_task(html_path=html_path, gcs_bucket=gcs_bucket, gcs_path=gcs_path, project_id=PROJECT_ID)

    return gcs_path


############# KPI í…Œì´ë¸” ì´ë¯¸ì§€ ìƒì„±
def kpi_table_draw(gameidx:str, path_roas_kpi:str, bucket, gcs_bucket:str, **context):

    roas_kpi = load_df_from_gcs(bucket=bucket, path=path_roas_kpi)

    nest_asyncio.apply()

    df_numeric = roas_kpi.copy()
    df_numeric = df_numeric.reset_index(drop=True)

    # 1) ROAS % â†’ ë¹„ìœ¨ ë³€í™˜
    def to_ratio_series(s: pd.Series) -> pd.Series:
        s_str = s.astype(str)
        s_num = pd.to_numeric(s_str.str.replace('%', '', regex=False), errors='coerce')
        return s_num / 100.0

    for c in df_numeric.columns:
        if c.startswith("ROAS "):
            df_numeric[c] = to_ratio_series(df_numeric[c])

    # 2) suffixes ì¶”ì¶œ
    suffixes = []
    for c in df_numeric.columns:
        m = re.search(r'\b(D\d+)\b$', str(c))
        if m and m.group(1) not in suffixes:
            suffixes.append(m.group(1))
    suffixes_tuple = tuple(suffixes)

    # 3) Styler ê¸°ë³¸ í¬ë§·
    styled = (
        df_numeric.style
        .hide(axis="index")
        .format({col: "{:.1%}" for col in df_numeric.columns if col.startswith("ROAS ")})
        .set_table_attributes('style="table-layout:fixed; width:600px;"')
    )

    # 4) HTML í…œí”Œë¦¿
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            h2 { margin: 0 0 10px 0; font-size: 18px; }
            table { border-collapse: collapse; font-size: 12px; border: 1px solid black; }
            th, td {
                border: 1px solid black;
                padding: 6px 8px;
                text-align: center;
                white-space: nowrap;
            }
            th { background-color: #f0f0f0; font-weight: bold; }
        </style>
    </head>
    <body>
        <h2>GBTW ROAS KPI (ì‹ ê·œìœ ì € ê¸°ì¤€)</h2>
        {{ table | safe }}
    </body>
    </html>
    """

    # 5) Styler â†’ HTML
    soup = BeautifulSoup(styled.to_html(), "html.parser")
    table = soup.find("table")

    # 6) colgroup & width ì ìš©
    ncols = len(df_numeric.columns)
    for cg in table.find_all("colgroup"):
        cg.decompose()

    colgroup = soup.new_tag("colgroup")
    width_map = {col: (80 if col.startswith("ROAS ") else 110) for col in df_numeric.columns}
    for col_name in df_numeric.columns:
        col = soup.new_tag("col", style=f"width: {width_map[col_name]}px !important;")
        colgroup.append(col)
    table.insert(0, colgroup)

    # 7) í—¤ë” ì¤„ë°”ê¿ˆ (ROAS â†’ ROAS<br>â€¦)
    for th in table.find_all("th"):
        text = th.get_text(strip=True)
        if text.startswith("ROAS "):
            th.string = ""
            th.append(BeautifulSoup(text.replace("ROAS ", "ROAS<br>"), "html.parser"))

    # 8) ìµœì¢… HTML ì €ì¥
    rendered_html = Template(html_template).render(table=str(table))
    html_path = "table6_ROAS_KPI.html"
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(rendered_html)

    import tempfile
    async def capture_html_to_image_async(html_path, gcs_bucket, gcs_path):
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_image_path = os.path.join(temp_dir, "graph6_ROAS_KPI.png")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page(viewport={"width": 600, "height": 160})
                await page.goto("file://" + os.path.abspath(html_path))
                await page.screenshot(path=temp_image_path, full_page=True)
                await browser.close()
            
            print(f"âœ“ Screenshot captured")
            
            # GCS ì—…ë¡œë“œ
            client = storage.Client()
            bucket = client.bucket(gcs_bucket)
            blob = bucket.blob(gcs_path)
            blob.upload_from_filename(temp_image_path, content_type='image/png')
            
            gcs_uri = f"gs://{gcs_bucket}/{gcs_path}"
            print(f"âœ“ Uploaded to: {gcs_uri}")
            
            return gcs_uri

    def capture_and_upload_task(html_path, gcs_bucket, gcs_path, **context):
        """Airflowìš© ë™ê¸° ë˜í¼ í•¨ìˆ˜"""
        result = asyncio.run(
            capture_html_to_image_async(html_path, gcs_bucket, gcs_path)
        )
        
        # XComì— GCS ê²½ë¡œ ì €ì¥
        return result
    
    html_path = "table6_ROAS_KPI.html"
    gcs_path = f'{gameidx}/graph6_ROAS_KPI.png' 

    result = capture_and_upload_task(html_path, gcs_bucket, gcs_path, **context)

    return gcs_path



def roas_kpi_table_merge(gameidx:str, path_roas_dataframe_preprocessing:str, path_result6_monthlyROAS:str, path_roas_kpi:str, bucket, gcs_bucket:str, **context):

    p1 = roas_table_draw(gameidx, path_roas_dataframe_preprocessing, path_result6_monthlyROAS, bucket, gcs_bucket, **context)
    p2 = kpi_table_draw(gameidx, path_roas_kpi, bucket, gcs_bucket, **context)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    img1 = Image.open(BytesIO(im1)).convert("RGBA")
    img2 = Image.open(BytesIO(im2)).convert("RGBA") 

    # ë‘ ì´ë¯¸ì§€ì˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    w1, h1 = img1.size
    w2, h2 = img2.size

    # ìµœì¢… ìº”ë²„ìŠ¤ í¬ê¸° (ë„ˆë¹„ëŠ” ë‘ ì´ë¯¸ì§€ ì¤‘ í° ê°’, ë†’ì´ëŠ” í•©ê³„)
    final_width = max(w1, w2)
    final_height = h1 + h2

    # í°ìƒ‰ ë°°ê²½ì˜ ìƒˆ ìº”ë²„ìŠ¤ ìƒì„±
    combined = Image.new("RGB", (final_width, final_height), (255, 255, 255))

    # ìœ„ì— roas_pc, ì•„ë˜ì— roaskpi_pc ë¶™ì´ê¸° (ì™¼ìª½ ì •ë ¬)
    combined.paste(img1, (0, 0))
    combined.paste(img2, (0, h1))

    # ì €ì¥
    combined.save("graph6_monthlyROAS_and_KPI.png", dpi=(180,180))

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    combined.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph6_monthlyROAS_and_KPI.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')
    print(f"âœ“âœ“âœ“âœ“âœ“ GCSì— ìµœì¢… ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ: {gcs_path}")

    return gcs_path


def retrieve_new_user_upload_notion(gameidx:str, service_sub:str, path_monthlyBEP_ROAS:str, path_roas_kpi:str, 
                                    MODEL_NAME:str, SYSTEM_INSTRUCTION:list, NOTION_TOKEN:str, NOTION_VERSION:str, notion, bucket, headers_json, **context):

    current_context = get_current_context()
    
    PAGE_INFO=current_context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page_wraper',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "6. ì‹ ê·œìœ ì € íšŒìˆ˜ í˜„í™©" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = headers_json

    try:
        gcs_path = f'{gameidx}/graph6_monthlyROAS_and_KPI.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph6_monthlyROAS_and_KPI.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    print("â– â– â– â– â– â– â– â– â– â– â–  monthlyBEP_ROAS ë°ì´í„°í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° ì‹œì‘ â– â– â– â– â– â– â– â– â– â– â– ")
    query6_monthlyROAS = load_df_from_gcs(bucket=bucket, path=path_monthlyBEP_ROAS)

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query6_monthlyROAS,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì‹ ê·œìœ ì € íšŒìˆ˜í˜„í™©",
        max_first_batch_rows=90,
        batch_size=100,
    )

    print("â– â– â– â– â– â– â– â– â– â– â–  monthlyBEP_ROAS ë°ì´í„°í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ â– â– â– â– â– â– â– â– â– â– â– ")
    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    text = result6_ROAS_gemini(gameidx=gameidx, service_sub=service_sub, 
                               path_monthlyBEP_ROAS=path_monthlyBEP_ROAS,
                               path_roas_kpi=path_roas_kpi,
                               bucket=bucket,
                               genai_client=genai_client,
                               MODEL_NAME=MODEL_NAME,
                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                               **context)
    
    blocks = md_to_notion_blocks(text, 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    text = monthlyLTVgrowth_gemini(service_sub=service_sub, 
                               path_monthlyBEP_ROAS=path_monthlyBEP_ROAS,
                               bucket=bucket,
                               genai_client=genai_client,
                               MODEL_NAME=MODEL_NAME,
                               SYSTEM_INSTRUCTION=SYSTEM_INSTRUCTION,
                               **context)
    blocks = md_to_notion_blocks(text, 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    print("â– â– â– â– â– â– â– â– â– â– â–  NOTION ì—…ë¡œë“œ ì™„ë£Œ â– â– â– â– â– â– â– â– â– â– â– ")
    return True
