import requests
import pandas as pd
from datetime import datetime, timezone, timedelta
import json
import pytz
from databricks import sql
import os
from airflow.models import Variable
from airflow import DAG
from airflow.operators.python import PythonOperator


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}

with DAG(
    dag_id='injoy_monitoringdata_consumer_2',
    default_args=default_args,
    description='Databricks ë°ì´í„°ë¥¼ Notion DBì— ë™ê¸°í™”í•˜ëŠ” DAG',
    schedule= '10 23 * * *', # ë§¤ì¼ ìƒˆë²½ 2ì‹œ 10ë¶„ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['notion', 'sync', 'monitoring'],
) as dag:


    def get_var(key: str, default: str = None, required: bool = False) -> str:
        """í™˜ê²½ ë³€ìˆ˜ â†’ Airflow Variable ìˆœì„œë¡œ ì¡°íšŒ"""
        env_value = os.environ.get(key)
        if env_value:
            print(f"âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ {key} ë¡œë“œë¨")
            return env_value
        
        try:
            try:
                var_value = Variable.get(key, default=None)
            except TypeError:
                var_value = Variable.get(key, default_var=None)
            
            if var_value:
                print(f"âœ“ Airflow Variableì—ì„œ {key} ë¡œë“œë¨")
                return var_value
        except Exception as e:
            print(f"âš ï¸  Variable.get({key}) ì˜¤ë¥˜: {str(e)}")
        
        if default is not None:
            print(f"â„¹ï¸  ê¸°ë³¸ê°’ìœ¼ë¡œ {key} ì„¤ì •ë¨: {default}")
            return default
        
        if required:
            raise ValueError(f"í•„ìˆ˜ ì„¤ì • {key}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"â„¹ï¸  {key} ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)")
        return None

    def get_notion_headers():
        """Notion API í—¤ë” ìƒì„±"""
        return {
            "Authorization": f"Bearer {get_var('NOTION_TOKEN', required=True)}",
            "Notion-Version": get_var("NOTION_API_VERSION", "2022-06-28"),
            "Content-Type": "application/json"
        }

    NOTION_HEADERS = get_notion_headers()
    DATABRICKS_INSTANCE = get_var("databricks_instance", required=True)
    DATABRICKS_TOKEN = get_var("databricks_token", required=True)
    url = f"{DATABRICKS_INSTANCE}/api/2.0/token/list"

    DATABRICKS_HEADERS = {
        "Authorization": f"Bearer {DATABRICKS_TOKEN}",
        "Content-Type": "application/json"
    }

    def tokenize_databricks(url, headers):
        
        resp = requests.get(url, headers=headers)

        if resp.status_code != 200:
            print(f"âŒ í† í° ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
            print(resp.text)
        else:
            tokens = resp.json().get("token_infos", [])
            
            def convert_ms(ms):
                if ms == -1:
                    return None
                return datetime.fromtimestamp(ms / 1000)

            for t in tokens:
                creation = convert_ms(t.get("creation_time"))
                expiry = convert_ms(t.get("expiry_time"))
                if creation:
                    print(f"   ìƒì„±ì‹œê°„: {creation.strftime('%Y-%m-%d %H:%M:%S')}")
                else:
                    print("   ìƒì„±ì‹œê°„: ì•Œ ìˆ˜ ì—†ìŒ")
                if expiry:
                    print(f"   ë§Œë£Œì‹œê°„: {expiry.strftime('%Y-%m-%d %H:%M:%S')}")
                else:
                    print("   ë§Œë£Œì‹œê°„: ë¬´ê¸°í•œ ë˜ëŠ” ì—†ìŒ")

    def search_databricks_users(**context):

        try:
            user_url = f"{DATABRICKS_INSTANCE}/api/2.0/preview/scim/v2/Users"
            user_resp = requests.get(user_url, headers=DATABRICKS_HEADERS)
            user_resp.raise_for_status()
            user_list = user_resp.json().get("Resources", [])
            user_data = []

            for user in user_list:
                user_data.append({
                    "user_id": user.get("id"),
                    "user_name": user.get("userName"),
                    "display_name": user.get("displayName"),
                    "email": next((e.get("value") for e in user.get("emails", []) if e.get("primary")), None),
                    "groups": ", ".join([g.get("display") for g in user.get("groups", []) if g.get("display")])
                })
            df_users = pd.DataFrame(user_data)
            print("âœ… ì‚¬ìš©ì ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            df_users = pd.DataFrame()
        
        return df_users


    def search_databricks_groups(**context):
        try:
            group_url = f"{DATABRICKS_INSTANCE}/api/2.0/preview/scim/v2/Groups"
            group_resp = requests.get(group_url, headers=DATABRICKS_HEADERS)
            group_resp.raise_for_status()
            groups = group_resp.json().get("Resources", [])
            group_data = []

            for g in groups:
                group_data.append({
                    "group_name": g.get("displayName"),
                    "group_id": g.get("id"),
                    "entitlements": ", ".join([e.get("value") for e in g.get("entitlements", []) if e.get("value")])
                })

            df_groups = pd.DataFrame(group_data)
            print("âœ… ê·¸ë£¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ê·¸ë£¹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            df_groups = pd.DataFrame()

        return df_groups


    # 3ï¸âƒ£ Unity Catalog ì˜¤ë¸Œì íŠ¸ ì •ë³´ ìˆ˜ì§‘ (datahubë§Œ ìˆ˜ì§‘, datahub//information_schema ì œì™¸)
    def search_unity_catalog_object(**context):

        catalog_objects = []

        try:
            catalog_name = "datahub"  # âœ… datahubë§Œ ìˆ˜ì§‘
            print(f"ğŸ“Œ Catalog ëŒ€ìƒ: {catalog_name}")
            
            # catalog ì¶”ê°€
            catalog_objects.append({"type": "catalog", "full_name": catalog_name})

            # schema ìˆ˜ì§‘ (ë‹¨, information_schema ì œì™¸)
            schemas_url = f"{DATABRICKS_INSTANCE}/api/2.1/unity-catalog/schemas?catalog_name={catalog_name}"
            sch_resp = requests.get(schemas_url, headers=DATABRICKS_HEADERS)
            sch_resp.raise_for_status()
            schemas = sch_resp.json().get("schemas", [])

            for sch in schemas:
                if sch['name'] == 'information_schema':
                    continue  # âŒ ì œì™¸
                
                sch_full = f"{catalog_name}.{sch['name']}"
                catalog_objects.append({"type": "schema", "full_name": sch_full})

                # table ìˆ˜ì§‘
                tables_url = f"{DATABRICKS_INSTANCE}/api/2.1/unity-catalog/tables?catalog_name={catalog_name}&schema_name={sch['name']}"
                tbl_resp = requests.get(tables_url, headers=DATABRICKS_HEADERS)
                tbl_resp.raise_for_status()
                tables = tbl_resp.json().get("tables", [])

                for tbl in tables:
                    tbl_full = f"{catalog_name}.{sch['name']}.{tbl['name']}"
                    catalog_objects.append({"type": "table", "full_name": tbl_full})

            df_catalog_objects = pd.DataFrame(catalog_objects)
            print("âœ… Unity Catalog ì˜¤ë¸Œì íŠ¸ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ Unity Catalog ì˜¤ë¸Œì íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            df_catalog_objects = pd.DataFrame()

        return df_catalog_objects


    def search_object_permissions(**context):

        # search_unity_catalog_object í•¨ìˆ˜ í˜¸ì¶œ
        df_catalog_objects = search_unity_catalog_object(**context)
        permissions_data = []
        
        for _, row in df_catalog_objects.iterrows():
            try:
                url = f"{DATABRICKS_INSTANCE}/api/2.1/unity-catalog/permissions/{row['type']}/{row['full_name']}"
                perm_resp = requests.get(url, headers=DATABRICKS_HEADERS)
                perm_resp.raise_for_status()
                acl_list = perm_resp.json().get("privilege_assignments", [])

                for acl in acl_list:
                    permissions_data.append({
                        "object_type": row["type"],
                        "object_name": row["full_name"],
                        "principal": acl.get("principal"),
                        "privileges": ", ".join(acl.get("privileges", []))
                    })
            except Exception as e:
                print(f"âš ï¸ {row['full_name']} ê¶Œí•œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        df_permissions = pd.DataFrame(permissions_data)
        return df_permissions

        print("âœ… ì˜¤ë¸Œì íŠ¸ ê¶Œí•œ ìˆ˜ì§‘ ì™„ë£Œ")
        # df_permissions.head()

    def search_warehouses_dashboards_permissions(**context):
        workspace_objects = []

        # 5-1ï¸âƒ£ Warehouses ìˆ˜ì§‘
        try:
            warehouse_url = f"{DATABRICKS_INSTANCE}/api/2.0/sql/warehouses"
            resp = requests.get(warehouse_url, headers=DATABRICKS_HEADERS)
            resp.raise_for_status()

            for wh in resp.json().get("warehouses", []):
                workspace_objects.append({
                    "type": "warehouses",
                    "object_id": wh["id"],
                    "object_name": wh["name"]
                })
            print("âœ… Warehouses ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ Warehouses ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # 5-2ï¸âƒ£ Dashboards ìˆ˜ì§‘
        try:
            dashboard_url = f"{DATABRICKS_INSTANCE}/api/2.0/lakeview/dashboards"
            resp = requests.get(dashboard_url, headers=DATABRICKS_HEADERS)
            resp.raise_for_status()

            for dash in resp.json().get("dashboards", []):
                workspace_objects.append({
                    "type": "dashboards",
                    "object_id": dash["dashboard_id"],
                    "object_name": dash["display_name"]
                })
            print("âœ… Dashboards ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ Dashboards ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # 5-3ï¸âƒ£ ê° ì˜¤ë¸Œì íŠ¸ë³„ permission ìˆ˜ì§‘
        workspace_perms = []

        for obj in workspace_objects:
            try:
                url = f"{DATABRICKS_INSTANCE}/api/2.0/permissions/{obj['type']}/{obj['object_id']}"
                resp = requests.get(url, headers=DATABRICKS_HEADERS)
                resp.raise_for_status()

                acl_list = resp.json().get("access_control_list", [])

                for entry in acl_list:
                    workspace_perms.append({
                        "object_type": obj["type"],
                        "object_name": obj["object_name"],
                        "principal": entry.get("user_name") or entry.get("group_name"),
                        "privileges": ", ".join([
                            perm.get("permission_level") for perm in entry.get("all_permissions", [])
                        ])
                    })

            except Exception as e:
                print(f"âš ï¸ {obj['type']} - {obj['object_name']} ê¶Œí•œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # DataFrameìœ¼ë¡œ ì •ë¦¬
        df_workspace_perms = pd.DataFrame(workspace_perms)
        return df_workspace_perms
        
        print("âœ… Warehouses & Dashboards ê¶Œí•œ ìˆ˜ì§‘ ì™„ë£Œ")
        

    def complex_all_permissions(**context):
        """ëª¨ë“  ê¶Œí•œ ì •ë³´ í†µí•©"""

        # ğŸ” 6ï¸âƒ£ ê¶Œí•œ ì •ë³´ í†µí•©

        df_workspace_perms = search_warehouses_dashboards_permissions(**context)
        df_permissions = search_object_permissions(**context)
        df_users = search_databricks_users(**context)
        df_groups = search_databricks_groups(**context)

        df_all_permissions = pd.concat([df_permissions, df_workspace_perms], ignore_index=True)

        # 6-1ï¸âƒ£ ì§ì ‘ ê¶Œí•œ (ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ë¶€ì—¬ëœ ê¶Œí•œ)
        direct_perms = df_all_permissions[df_all_permissions["principal"].isin(df_users["user_name"])].copy()
        direct_perms = direct_perms[direct_perms["object_name"].notnull()]  # userì—ê²Œ ì§ì ‘ ë¶€ì—¬ëœ ê²½ìš° object_name ì—†ìœ¼ë©´ ì œê±°

        df_direct = pd.merge(
            direct_perms,
            df_users,
            left_on="principal",
            right_on="user_name",
            how="left"
        )

        # ì§ì ‘ ê¶Œí•œì´ë¯€ë¡œ ê·¸ë£¹ ê¸°ë°˜ inherited ì •ë³´ëŠ” ë¹„ì›Œë‘ê¸°
        df_direct["inherited_group_name"] = ""
        df_direct["inherited_group_id"] = ""
        df_direct["inherited_entitlements"] = ""

        # ì¹¼ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        df_direct = df_direct[[
            "user_id", "display_name", "email", "groups",  # ì‚¬ìš©ì ê¸°ë³¸ ì •ë³´
            "inherited_group_name", "inherited_group_id", "inherited_entitlements",  # ìƒì† ì •ë³´
            "object_name", "object_type", "privileges"  # ê¶Œí•œ ì •ë³´
        ]]

        print("âœ… df_direct (ì§ì ‘ ê¶Œí•œ) ìƒì„± ì™„ë£Œ")

        # âœ… 6-2ï¸âƒ£ ê·¸ë£¹ ê¸°ë°˜ ê¶Œí•œ (ê·¸ë£¹ì— ë¶€ì—¬ëœ ê¶Œí•œ â†’ ì‚¬ìš©ìì—ê²Œ ìƒì†)
        group_perms = df_all_permissions[df_all_permissions["principal"].isin(df_groups["group_name"])].copy()

        # ê·¸ë£¹ëª… â†’ ê·¸ë£¹ ì •ë³´ ì—°ê²°
        group_perms = pd.merge(
            group_perms,
            df_groups,
            left_on="principal",
            right_on="group_name",
            how="left"
        )

        # ì‚¬ìš©ì ì •ë³´ì™€ ê·¸ë£¹ ì—°ê²°: ê·¸ë£¹ëª… â†’ ì‚¬ìš©ìë“¤ ì¶”ì¶œ
        user_group_pairs = []
        for _, row in df_users.iterrows():
            user_groups = str(row["groups"]).split(", ")
            for g in user_groups:
                user_group_pairs.append({
                    "user_id": row["user_id"],
                    "display_name": row["display_name"],
                    "email": row["email"],
                    "groups": row["groups"],
                    "user_group_name": g
                })
        df_user_groups = pd.DataFrame(user_group_pairs)

        # ê·¸ë£¹ ê¶Œí•œ â†’ ì‚¬ìš©ì ìƒì† ë§¤í•‘
        df_group_inherited = pd.merge(
            group_perms,
            df_user_groups,
            left_on="principal",
            right_on="user_group_name",
            how="inner"
        )

        # ì¹¼ëŸ¼ëª… ì •ë¦¬
        df_group_inherited["inherited_group_name"] = df_group_inherited["principal"]
        df_group_inherited["inherited_group_id"] = df_group_inherited["group_id"]
        df_group_inherited["inherited_entitlements"] = df_group_inherited["entitlements"]

        # ì¹¼ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        df_group_inherited = df_group_inherited[[
            "user_id", "display_name", "email", "groups",
            "inherited_group_name", "inherited_group_id", "inherited_entitlements",
            "object_name", "object_type", "privileges"
        ]]

        # âœ… 6-3ï¸âƒ£ df_direct + df_group_inherited í†µí•©
        df_final = pd.concat([df_direct, df_group_inherited], ignore_index=True)

        # âœ… ì¤‘ë³µ ì œê±° (ê°™ì€ ìœ ì €ê°€ ê°™ì€ objectì— ê°™ì€ ê¶Œí•œì„ ì—¬ëŸ¬ ê²½ë¡œë¡œ ë°›ì„ ìˆ˜ ìˆìŒ)
        df_final.drop_duplicates(subset=[
            "user_id", "object_name", "privileges"
        ], inplace=True)

        # í˜„ì¬ ì‹œê° (UTC ë˜ëŠ” local time, í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        kst = pytz.timezone("Asia/Seoul")
        now_kst = datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")

        # loaded_at ì»¬ëŸ¼ ì¶”ê°€
        df_final["loaded_at"] = now_kst

        df_final = df_final.rename(columns={
        "display_name": "user_name",
        "groups": "all_groups"  # í•´ë‹¹ ìœ ì €ê°€ ì†í•œ ëª¨ë“  ê·¸ë£¹
        })

        return df_final



    def detect_and_store_changes(databricks_config=None):
        """ë³€ê²½ ì‚¬í•­ ê°ì§€ ë° ì €ì¥"""
        
        # ëª¨ë“  ê¶Œí•œ ì •ë³´ í†µí•©
        df_final = complex_all_permissions()

        # Databricks ì—°ê²° ì •ë³´
        config = databricks_config or {
            'server_hostname': get_var('DATABRICKS_SERVER_HOSTNAME'),
            'http_path': get_var('databricks_http_path'),
            'access_token': get_var('databricks_token')
        }
        
        target_table = "datahub.injoy_ops_schema.user_permission_snapshot"
        changes_detected = False

        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹œë„
        try:
            with sql.connect(**config) as conn:
                df_existing = pd.read_sql(f"SELECT * FROM {target_table}", conn)
            
            print(f"âœ… ê¸°ì¡´ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ")
            
            # 'loaded_at' ì œì™¸í•˜ê³  ë¹„êµ
            compare_cols = [col for col in df_final.columns if col != 'loaded_at']
            df_new = df_final[compare_cols].sort_values(by=compare_cols).reset_index(drop=True)
            df_old = df_existing[compare_cols].sort_values(by=compare_cols).reset_index(drop=True)
            
            changes_detected = not df_new.equals(df_old)
            print(f"â„¹ï¸ ë³€ê²½ ì‚¬í•­: {'ìˆìŒ' if changes_detected else 'ì—†ìŒ'}")
            
        except Exception as e:
            if "NOT_FOUND" in str(e).upper() or "NOT EXIST" in str(e).upper():
                print(f"â„¹ï¸ ê¸°ì¡´ í…Œì´ë¸” ì—†ìŒ. ì²« ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼")
                changes_detected = True
            else:
                raise e

        # ë³€ê²½ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì €ì¥
        if changes_detected:
            print("ğŸš€ í…Œì´ë¸” ì €ì¥ ì¤‘...")
            with sql.connect(**config) as conn:
                cursor = conn.cursor()
                
                # í…Œì´ë¸” ì¬ìƒì„±
                cursor.execute(f"DROP TABLE IF EXISTS {target_table}")
                
                # ì»¬ëŸ¼ íƒ€ì… ë§¤í•‘
                type_map = {'object': 'STRING', 'int64': 'BIGINT', 'float64': 'DOUBLE', 'bool': 'BOOLEAN'}
                cols = ', '.join([f"`{col}` {type_map.get(str(df_final[col].dtype), 'STRING')}" 
                                for col in df_final.columns])
                cursor.execute(f"CREATE TABLE {target_table} ({cols})")
                
                # SQL ê°’ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                def escape_sql_value(v):
                    if pd.isna(v):
                        return 'NULL'
                    s = str(v).replace("\\", "\\\\").replace("'", "''")
                    return f"'{s}'"
                
                for i in range(0, len(df_final), 1000):
                    batch = df_final.iloc[i:i+1000]
                    values = []
                    for _, row in batch.iterrows():
                        row_values = [escape_sql_value(v) for v in row]  # âœ… ìˆ˜ì •
                        values.append(f"({', '.join(row_values)})")
                    cursor.execute(f"INSERT INTO {target_table} VALUES {', '.join(values)}")
                
                cursor.close()
            
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {target_table}")
            return {"has_changes": True}
        
        print("â­ï¸ ë³€ê²½ ì—†ìŒ. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
        return {"has_changes": False}
    




    task_tokenize_databricks = PythonOperator(
        task_id='tokenize_databricks',
        python_callable=tokenize_databricks,
        op_kwargs={
            'url': url,
            'headers': {'Authorization': f"Bearer {DATABRICKS_TOKEN}"}
        },
        dag=dag,
    )

    task_detect_and_store_changes = PythonOperator(
        task_id='detect_and_store_changes',
        python_callable=detect_and_store_changes,
        op_kwargs={'databricks_config': {
            'server_hostname': get_var('DATABRICKS_SERVER_HOSTNAME'),
            'http_path': get_var('databricks_http_path'),
            'access_token': get_var('databricks_token')
        }},
        dag=dag,
    )

    task_tokenize_databricks >> task_detect_and_store_changes