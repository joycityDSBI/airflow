"""
ìˆ˜ì •ëœ ë¶€ë¶„:
1. Variable.get() ì—ëŸ¬ ì²˜ë¦¬ ê°œì„  (KeyError, VARIABLE_NOT_FOUND)
2. Airflow 3.0+ í˜¸í™˜ì„± ì²˜ë¦¬
3. ì´ˆê¸° ì‹¤í–‰ ì‹œ Variable ìë™ ìƒì„±
"""

import hashlib
import logging
import os
import smtplib
import time
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from typing import Any, Dict
import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.base import BaseSensorOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from google.cloud import bigquery

# notion_utils ëª¨ë“ˆ ì„í¬íŠ¸
from notion_utils import update_notion_databases

# ===== ì„¤ì • =====
def get_config(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variableì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°"""
    env_value = os.environ.get(key)
    if env_value:
        return env_value
    
    try:
        # Airflow 3.0+ í˜¸í™˜ì„±
        try:
            from airflow.sdk import Variable as SDKVariable
            return SDKVariable.get(key, default)
        except ImportError:
            # Airflow 2.x
            return Variable.get(key, default_var=default)
    except Exception:
        return default


# ì´ë©”ì¼ ì„¤ì •
SMTP_HOST = get_config("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(get_config("SMTP_PORT", "587"))
SMTP_USER = get_config("SMTP_USER")
SMTP_PASSWORD = get_config("SMTP_PASSWORD")
EMAIL_TO = get_config("EMAIL_TO")
EMAIL_FROM = get_config("EMAIL_FROM", SMTP_USER)

# BigQuery ì„¤ì •
TARGET_PROJECT = "aibi-service"
TARGET_DATASET = "Service_Set"
METADATA_HASH_VAR = "bq_metadata_hash"

COLUMN_QUERY = f"""
SELECT
  table_catalog AS project_id,
  table_schema  AS dataset_id,
  table_name,
  column_name,
  data_type,
  is_nullable,
  CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_table_id
FROM `{TARGET_PROJECT}.{TARGET_DATASET}.INFORMATION_SCHEMA.COLUMNS`
WHERE table_name IN (
  SELECT table_name
  FROM `{TARGET_PROJECT}.{TARGET_DATASET}.INFORMATION_SCHEMA.TABLES`
  WHERE table_type IN ('VIEW', 'MATERIALIZED VIEW')
)
ORDER BY full_table_id, column_name
"""


# ===== Custom Sensor (ìˆ˜ì •ë¨) =====
class BigQueryMetadataChangeSensor(BaseSensorOperator):
    """BigQuery ë©”íƒ€ë°ì´í„° ë³€ê²½ ê°ì§€ Sensor"""
    
    template_fields = ('project_id', 'query', 'hash_variable')
    
    def __init__(
        self,
        *,
        project_id: str,
        query: str,
        hash_variable: str,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.project_id = project_id
        self.query = query
        self.hash_variable = hash_variable
    
    def _get_variable_safely(self, key: str, default: str = None) -> str:
        """Variableì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        try:
            # Airflow 3.0+ í˜¸í™˜ì„±
            try:
                from airflow.sdk import Variable as SDKVariable
                return SDKVariable.get(key, default)
            except ImportError:
                # Airflow 2.x
                return Variable.get(key, default_var=default)
        except Exception as e:
            logging.warning(f"âš ï¸ Variable '{key}' ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__}")
            return default
    
    def _set_variable_safely(self, key: str, value: str) -> bool:
        """Variableì„ ì•ˆì „í•˜ê²Œ ì €ì¥"""
        try:
            Variable.set(key, value)
            logging.info(f"ğŸ’¾ Variable ì €ì¥ ì„±ê³µ: {key}")
            return True
        except Exception as e:
            logging.error(f"ğŸ”¥ Variable ì €ì¥ ì‹¤íŒ¨: {type(e).__name__} - {e}")
            return False
    
    def poke(self, context: Dict[str, Any]) -> bool:
        """ë©”íƒ€ë°ì´í„° ë³€ê²½ ê°ì§€"""
        logging.info("ğŸ” BigQuery ë©”íƒ€ë°ì´í„° ë³€ê²½ ê°ì§€ ì‹œì‘")
        
        try:
            bq_client = bigquery.Client(project=self.project_id)
            query_job = bq_client.query(self.query)
            results = query_job.result()
            
            # í•´ì‹œ ê³„ì‚°
            rows = []
            for row in results:
                row_dict = dict(row)
                rows.append(str(sorted(row_dict.items())))
            
            data_str = "".join(sorted(rows))
            current_hash = hashlib.sha256(data_str.encode()).hexdigest()
            
            logging.info(f"ğŸ“Š í˜„ì¬ í•´ì‹œ: {current_hash[:16]}...")
            
            # ì´ì „ í•´ì‹œ ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•˜ê²Œ)
            previous_hash = self._get_variable_safely(self.hash_variable)
            
            if previous_hash:
                logging.info(f"ğŸ“‹ ì´ì „ í•´ì‹œ: {previous_hash[:16]}...")
            else:
                logging.info("ğŸ“‹ ì´ì „ í•´ì‹œ ì—†ìŒ (ì´ˆê¸° ì‹¤í–‰) â†’ Variable ì´ˆê¸°í™”")
                # ì´ˆê¸° ì‹¤í–‰ ì‹œ current_hashë¥¼ Variableì— ì €ì¥
                self._set_variable_safely(self.hash_variable, current_hash)
            
            # ë³€ê²½ ê°ì§€
            if current_hash != previous_hash:
                logging.info("âœ… ë³€ê²½ ê°ì§€ë¨ â†’ ETL ì‹¤í–‰")
                context['ti'].xcom_push(key='current_hash', value=current_hash)
                return True
            else:
                logging.info("â¸ï¸ ë³€ê²½ ì—†ìŒ â†’ ëŒ€ê¸°")
                return False
                
        except Exception as e:
            logging.error(f"ğŸ”¥ Sensor ì—ëŸ¬: {type(e).__name__} - {e}", exc_info=True)
            # SensorëŠ” Falseë¥¼ ë°˜í™˜í•˜ì—¬ ê³„ì† ëŒ€ê¸°
            return False


# ===== Task í•¨ìˆ˜ë“¤ =====
def extract_bq_metadata(**context):
    """BigQuery ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    start_ts = time.time()
    logging.info("ğŸš€ BigQuery ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ì¤‘")
    
    try:
        bq_client = bigquery.Client(project=TARGET_PROJECT)
        df = bq_client.query(COLUMN_QUERY).result().to_dataframe(
            create_bqstorage_client=False
        )
        
        logging.info(
            f"âœ… ì¡°íšŒ ì™„ë£Œ: rows={len(df)}, "
            f"tables={df['table_name'].nunique()}, "
            f"columns={df['column_name'].nunique()}"
        )
        
        # XComì— ì €ì¥
        context['ti'].xcom_push(key='metadata_df', value=df.to_dict('records'))
        context['ti'].xcom_push(key='row_count', value=len(df))
        context['ti'].xcom_push(key='table_count', value=int(df['table_name'].nunique()))
        context['ti'].xcom_push(key='column_count', value=int(df['column_name'].nunique()))
        
        took = time.time() - start_ts
        logging.info(f"â±ï¸ ì¶”ì¶œ ì†Œìš”: {took:.1f}s")
        
    except Exception as e:
        logging.exception(f"ğŸ”¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise


def sync_to_notion(**context):
    """Notion ë™ê¸°í™”"""
    start_ts = time.time()
    logging.info("=" * 70)
    logging.info("ğŸ§  Notion ë™ê¸°í™” ì‹œì‘")
    logging.info("=" * 70)
    
    try:
        ti = context['ti']
        
        # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        logging.info("ğŸ“Œ Step 1: XComì—ì„œ ë©”íƒ€ë°ì´í„° ì½ê¸°")
        metadata_records = ti.xcom_pull(
            task_ids='extract_metadata',
            key='metadata_df'
        )
        
        if not metadata_records:
            raise ValueError("XComì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logging.info(f"  âœ… ë°ì´í„° ë¡œë“œ: {len(metadata_records)} records")
        
        # DataFrame ë³€í™˜
        logging.info("ğŸ“Œ Step 2: DataFrame ë³€í™˜")
        df = pd.DataFrame(metadata_records)
        logging.info(f"  âœ… ë³€í™˜ ì™„ë£Œ: {len(df)} rows Ã— {len(df.columns)} cols")
        logging.info(f"  ì»¬ëŸ¼: {df.columns.tolist()}")
        
        # Notion ì—…ë°ì´íŠ¸ (ì—¬ê¸°ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ)
        logging.info("ğŸ“Œ Step 3: update_notion_databases í˜¸ì¶œ")
        result = update_notion_databases(df)  # ğŸ“Œ ì´ì œ ì˜ˆì™¸ë¥¼ throwí•  ê²ƒ!
        logging.info(f"  âœ… Notion ì—…ë°ì´íŠ¸ ì™„ë£Œ: {result}")
        
        took = time.time() - start_ts
        ti.xcom_push(key='success', value=True)
        ti.xcom_push(key='duration', value=round(took, 2))
        
        logging.info("=" * 70)
        logging.info(f"âœ… Notion ë™ê¸°í™” ì™„ë£Œ (â±ï¸ {took:.1f}s)")
        logging.info("=" * 70)
        
    except Exception as e:
        logging.error("=" * 70)
        logging.exception(f"ğŸ”¥ Notion ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error("=" * 70)
        
        context['ti'].xcom_push(key='success', value=False)
        context['ti'].xcom_push(key='error', value=str(e))
        
        # ğŸ“Œ ì—ëŸ¬ë¥¼ raiseí•˜ë©´ DAGì—ì„œ ê°ì§€ ê°€ëŠ¥!
        raise


def send_email_notification(**context):
    """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
    if not all([SMTP_USER, SMTP_PASSWORD, EMAIL_TO]):
        logging.warning("âš ï¸ ì´ë©”ì¼ ì„¤ì • ëˆ„ë½")
        return
    
    ti = context['ti']
    success = ti.xcom_pull(task_ids='sync_to_notion', key='success')
    rows = ti.xcom_pull(task_ids='extract_metadata', key='row_count')
    duration = ti.xcom_pull(task_ids='sync_to_notion', key='duration')
    hash_val = ti.xcom_pull(task_ids='detect_metadata_change', key='current_hash')
    
    status = "ì„±ê³µ" if success else "ì‹¤íŒ¨"
    status_color = "#4CAF50" if success else "#F44336"
    status_emoji = "âœ…" if success else "âŒ"
    
    subject = f"[Airflow] BigQuery â†’ Notion ë™ê¸°í™” {status} ({rows:,} rows)"
    
    body = f"""
    <!DOCTYPE html>
    <html>
    <head><meta charset="UTF-8"></head>
    <body style="font-family: Arial, sans-serif;">
        <div style="max-width: 600px; margin: 0 auto; padding: 20px;">
            <div style="background-color: {status_color}; color: white; padding: 20px; text-align: center;">
                <h2>{status_emoji} BigQuery â†’ Notion ë™ê¸°í™” {status}</h2>
            </div>
            <div style="background-color: #f9f9f9; padding: 20px; border: 1px solid #ddd;">
                <p><strong>ì‹¤í–‰ ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>ì²˜ë¦¬ëœ í–‰ ìˆ˜:</strong> {rows:,} rows</p>
                <p><strong>ì†Œìš” ì‹œê°„:</strong> {duration} ì´ˆ</p>
                <p><strong>ë©”íƒ€ë°ì´í„° í•´ì‹œ:</strong> {hash_val[:16] if hash_val else 'N/A'}...</p>
                <p><strong>ëŒ€ìƒ í”„ë¡œì íŠ¸:</strong> {TARGET_PROJECT}.{TARGET_DATASET}</p>
            </div>
        </div>
    </body>
    </html>
    """
    
    try:
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_FROM or SMTP_USER
        msg['To'] = EMAIL_TO
        msg.attach(MIMEText(body, 'html'))
        
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        
        logging.info("âœ… ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
        
    except Exception as e:
        logging.error(f"ğŸ”¥ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")


# ===== DAG ì •ì˜ =====
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

with DAG(
    dag_id='bq_notion_metadata_sync',
    default_args=default_args,
    description='BigQuery ë©”íƒ€ë°ì´í„°ë¥¼ Notionì— ë™ê¸°í™” (5ë¶„ë§ˆë‹¤ ì²´í¬)',
    schedule='*/5 * * * *',  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['bigquery', 'notion', 'metadata', 'etl'],
) as dag:
    
    # Task 1: ë³€ê²½ ê°ì§€ Sensor
    detect_change = BigQueryMetadataChangeSensor(
        task_id='detect_metadata_change',
        project_id=TARGET_PROJECT,
        query=COLUMN_QUERY,
        hash_variable=METADATA_HASH_VAR,
        poke_interval=60,   # 1ë¶„ë§ˆë‹¤ ì²´í¬
        timeout=300,        # 5ë¶„ íƒ€ì„ì•„ì›ƒ (DAG ì£¼ê¸°ì™€ ì¼ì¹˜)
        mode='poke',        # poke ëª¨ë“œ
    )
    
    # Task 2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    extract_metadata = PythonOperator(
        task_id='extract_metadata',
        python_callable=extract_bq_metadata,
    )
    
    # Task 3: Notion ë™ê¸°í™”
    sync_notion = PythonOperator(
        task_id='sync_to_notion',
        python_callable=sync_to_notion,
    )
    
    # # Task 4: ì´ë©”ì¼ ì•Œë¦¼
    # send_email = PythonOperator(
    #     task_id='send_email_notification',
    #     python_callable=send_email_notification,
    #     trigger_rule=TriggerRule.ALL_DONE,  # ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‘ ì‹¤í–‰
    # )
    
    # ì˜ì¡´ì„±
    detect_change >> extract_metadata >> sync_notion ## >> send_email