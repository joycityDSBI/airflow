from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator

# í°íŠ¸ ìºì‹œ ì¬êµ¬ì¶•
import matplotlib.font_manager as fm

# ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
import re
import random
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from matplotlib.ticker import PercentFormatter
import dataframe_image as dfi
from PIL import Image

# Google Cloud ê´€ë ¨
from google.cloud import bigquery
from google.cloud import aiplatform
import pandas_gbq

# Gemini AI ê´€ë ¨
# import vertexai
# from vertexai.generative_models import GenerativeModel
# import google.generativeai as genai
# from google.generativeai import GenerativeModel as GeminiModel # ì´ë¦„ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³„ì¹­ ì‚¬ìš©

#Gemini 3.0 ê´€ë ¨
# !pip install --upgrade google-genai
from google.genai import Client as GeminiClient
from google.genai.types import GenerateContentConfig
from google.genai import types

# Notion API
from notion_client import Client as NotionClient

# ì›¹ ê´€ë ¨ (HTML, CSS ë Œë”ë§ ë“±)
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio

# IPython ë””ìŠ¤í”Œë ˆì´
from IPython.display import display

def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)

# gemini ì„¤ì •
os.environ['GOOGLE_CLOUD_PROJECT'] = 'data-science-division-216308'
os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'  #global

# í•œê¸€ í°íŠ¸ ì§€ì •: ë¨¼ì € ì„¤ì¹˜ëœ ê²ƒì„ ìš°ì„ ìœ¼ë¡œ, ì—†ìœ¼ë©´ ë‹¤ìŒ í›„ë³´ë¡œ í´ë°±
mpl.rcParams["font.family"] = ["Noto Sans CJK KR", "NanumGothic", "DejaVu Sans"]
mpl.rcParams["axes.unicode_minus"] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€

names = sorted({f.name for f in fm.fontManager.ttflist})
[k for k in names if "Noto" in k or "Nanum" in k][:50]
names

NOTION_TOKEN = get_var("NOTION_TOKEN_MS")  # Airflow Variableì— ì €ì¥ëœ Notion í†µí•© í† í°
NOTION_VERSION = "2022-06-28"

### beta released
NOTION_PAGE_ID = "24cea67a56818059a90aee3f616bc263" # ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•  Notion í˜ì´ì§€ì˜ ID
NOTION_DATABASE_ID = "279ea67a5681807fb943e9894bad5c57"
author_person_id = 'a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae'  #ì‘ì„±ì

ref_person_ids= [ 'ebd0514a-939d-4c80-bb34-f1413478d9d9',  #ì˜¤ì¹˜ì„±
                  '7b68811e-e587-45a1-8ad2-940c87dadf9a',  #ì´í•œë‚˜ë¦¬
                 '5e777130-5039-4f71-9ac7-64645f674737' , #ë°•ì¤€ìŠ¹
                 '262e5f51-9d68-4444-9713-5f1506b3eead' , #ì´ë³‘ì„ 
                 '23c62fe1-a573-4b3f-b12c-c7df7dbe8c9b' , #ì§„ì •ì™„
                 'ae87a94b-cf69-41fd-ae37-b0385b4e4bdf' , #ë°•ë¯¼ì¬
                  '299d872b-594c-8174-9e5a-00028da23485', # ê¹€ë„ì˜
                  '645651d3-c051-40ae-b551-b0c4ef4b49f1', #ê³„ë™ê· 
                  '096802f3-3ae8-4e2d-bc06-911d6dc4052c', #ì‹ ì •ì—½
                 '8658b12e-cf6b-4247-abc2-c346381951ad'  #ì „ìëŒ
]

## ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ë…¸ì…˜ì— ê·¸ëŒ€ë¡œ ì ìš©ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜
# ğŸ‘‰ Markdown ë‚´ **êµµê²Œ** ì²˜ë¦¬ ë³€í™˜
def parse_rich_text(md_text):
    """
    '**êµµê²Œ**' â†’ Notion rich_text [{"text": {...}, "annotations": {"bold": True}}]
    """
    parts = re.split(r"(\*\*.*?\*\*)", md_text)  # **...** ê¸°ì¤€ split
    rich_text = []
    for part in parts:
        if part.startswith("**") and part.endswith("**"):
            rich_text.append({
                "type": "text",
                "text": {"content": part[2:-2]},
                "annotations": {"bold": True}
            })
        else:
            if part:
                rich_text.append({
                    "type": "text",
                    "text": {"content": part}
                })
    return rich_text

# ğŸ‘‰ Markdownì„ Notion Blocksë¡œ ë³€í™˜
def md_to_notion_blocks(md_text, blank_blocks=3):
    blocks = []
    lines = md_text.splitlines()
    stack = [blocks]  # í˜„ì¬ ê³„ì¸µ ì¶”ì 

    def detect_indent_unit(lines):
        indents = []
        for line in lines:
            if line.lstrip().startswith(("* ", "- ", "+ ")):  # ë¦¬ìŠ¤íŠ¸ ë¬¸ë²• ê°ì§€
                indent = len(line) - len(line.lstrip())
                if indent > 0:
                    indents.append(indent)
        return min(indents) if indents else 4  # fallback = 4ì¹¸
    indent_unit = detect_indent_unit(lines)

    i = 0
    while i < len(lines):
        line = lines[i].rstrip()
        if not line:
            i += 1
            continue

        # Heading ì²˜ë¦¬
        if line.startswith("# "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": parse_rich_text(line[2:])}
            })
        elif line.startswith("## "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": parse_rich_text(line[3:])}
            })
        elif line.startswith("### "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": parse_rich_text(line[4:])}
            })

        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        elif line.lstrip().startswith(("* ", "- ", "+ ")):
            indent = len(line) - len(line.lstrip())  # ë“¤ì—¬ì“°ê¸° ë ˆë²¨
            content = line.strip()[2:].strip()

            # ë‹¤ìŒ ì¤„ì´ ë“¤ì—¬ì“°ê¸°ê°€ ë” ê¹Šì€ì§€ í™•ì¸í•˜ì—¬ ìì‹ ë¸”ë¡ì´ ìˆëŠ”ì§€ íŒë‹¨
            has_children = False
            if i + 1 < len(lines):
                next_line = lines[i+1]
                next_indent = len(next_line) - len(next_line.lstrip())
                if next_indent > indent:
                    has_children = True

            block_data = {
                "rich_text": parse_rich_text(content),
            }
            # ìì‹ ë¸”ë¡ì´ ìˆì„ ê²½ìš°ì—ë§Œ 'children' í‚¤ë¥¼ ì¶”ê°€
            if has_children:
                block_data["children"] = []

            block = {
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": block_data
            }

            # indent ê¸°ë°˜ ê³„ì¸µ ì²˜ë¦¬
            level = indent // indent_unit + 1
            while len(stack) > level:
                stack.pop()
            stack[-1].append(block)

            # ìì‹ ë¸”ë¡ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ìŠ¤íƒì— ìì‹ ëª©ë¡ì„ ì¶”ê°€
            if has_children:
                stack.append(block["bulleted_list_item"]["children"])
            else:
                # ìì‹ ë¸”ë¡ì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ë ˆë²¨ë¡œ ìŠ¤íƒ ì¬ì„¤ì •
                stack = stack[:level]

        else:
            stack = [blocks]
            # ì¼ë°˜ ë¬¸ë‹¨
            stack[-1].append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": parse_rich_text(line.strip())}
            })

        i += 1

    # âœ… ë§ˆì§€ë§‰ì— ë¹ˆ ë¸”ë¡ ì¶”ê°€ (ê°œìˆ˜ëŠ” íŒŒë¼ë¯¸í„° blank_blocksë¡œ ì œì–´)
    for _ in range(blank_blocks):
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": []}
        })

    return blocks

def mkt_monthly_report_total():
    from datetime import datetime, timezone, timedelta
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_1_total_roas"} ## ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¶™ì¼ ìˆ˜ ìˆìŒ.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

    client = bigquery.Client()
    query = """
    WITH revraw AS(
    select  JoypleGameID, Month
    ,sum(RU) as RU,#
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as Month,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
    and (JoypleGameID = 131 AND RegdateAuthAccountDateKST BETWEEN '2021-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 133 AND RegdateAuthAccountDateKST BETWEEN '2021-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30001 AND RegdateAuthAccountDateKST BETWEEN '2022-05-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30003 AND RegdateAuthAccountDateKST BETWEEN '2024-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    group by JoypleGameID, RegdateAuthAccountDateKST
    ) group by JoypleGameID, month
    )


    , final AS(
    select  JoypleGameID,  Month, RU,
    Sales_D1/RU as D1_LTV,
    Sales_D3/RU as D3_LTV,
    Sales_D7/RU as D7_LTV,
    Sales_D14/RU as D14_LTV,
    Sales_D30/RU as D30_LTV,
    Sales_D60/RU as D60_LTV,
    Sales_D90/RU as D90_LTV,
    Sales_D120/RU as D120_LTV,
    Sales_D150/RU as D150_LTV,
    Sales_D180/RU as D180_LTV,
    Sales_D210/RU as D210_LTV,
    Sales_D240/RU as D240_LTV,
    Sales_D270/RU as D270_LTV,
    Sales_D300/RU as D300_LTV,
    Sales_D330/RU as D330_LTV,
    Sales_D360/RU as D360_LTV,
    Sales_D14_p/RU as D14_LTV_p,
    Sales_D30_p/RU as D30_LTV_p,
    Sales_D60_p/RU as D60_LTV_p,
    Sales_D90_p/RU as D90_LTV_p,
    Sales_D120_p/RU as D120_LTV_p,
    Sales_D150_p/RU as D150_LTV_p,
    Sales_D180_p/RU as D180_LTV_p,
    Sales_D210_p/RU as D210_LTV_p,
    Sales_D240_p/RU as D240_LTV_p,
    Sales_D270_p/RU as D270_LTV_p,
    Sales_D300_p/RU as D300_LTV_p,
    Sales_D330_p/RU as D330_LTV_p,
    Sales_D360_p/RU as D360_LTV_p,
    D1D3_avg,  D3D7_avg , Sales_D7/RU as kpi_d7
    from(
    select *,
    CASE WHEN Sales_D14 is not null then null  ELSE  Sales_D7*D7D14_avg END as Sales_D14_p,
    CASE WHEN Sales_D30 is not null then null
    WHEN Sales_D30 is null and Sales_D14 is not null THEN Sales_D14*D14D30_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg END as Sales_D30_p,
    CASE WHEN Sales_D60 is not null then null
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg END as Sales_D60_p,
    CASE WHEN Sales_D90 is not null then null
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg END as Sales_D90_p,
    CASE WHEN Sales_D120 is not null then null
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg  END as Sales_D120_p,
    CASE WHEN Sales_D150 is not null then null
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg   END as Sales_D150_p,
    CASE WHEN Sales_D180 is not null then null
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg    END as Sales_D180_p,
    CASE WHEN Sales_D210 is not null then null
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg     END as Sales_D210_p,
    CASE WHEN Sales_D240 is not null then null
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg      END as Sales_D240_p,
    CASE WHEN Sales_D270 is not null then null
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg       END as Sales_D270_p,
    CASE WHEN Sales_D300 is not null then null
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg        END as Sales_D300_p,
    CASE WHEN Sales_D330 is not null then null
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg         END as Sales_D330_p,
    CASE WHEN Sales_D360 is not null then null
    WHEN Sales_D330 is not null THEN Sales_D330*D330D360_avg
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg *D330D360_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg*D330D360_avg          END as Sales_D360_p
    from(

    select *
    ,  LAST_VALUE(d1d3_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D1D3_avg
    ,  LAST_VALUE(d3d7_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D3D7_avg
    ,  LAST_VALUE(d7d14_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D7D14_avg
    ,  LAST_VALUE(d14d30_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D14D30_avg
    ,  LAST_VALUE(d30d60_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D30D60_avg
    ,  LAST_VALUE(d60d90_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D60D90_avg
    ,  LAST_VALUE(d90d120_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d90d120_avg
    ,  LAST_VALUE(d120d150_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d120d150_avg
    ,  LAST_VALUE(d150d180_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d150d180_avg
    ,  LAST_VALUE(d180d210_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d180d210_avg
    ,  LAST_VALUE(d210d240_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d210d240_avg
    ,  LAST_VALUE(d240d270_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d240d270_avg
    ,  LAST_VALUE(d270d300_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d270d300_avg
    ,  LAST_VALUE(d300d330_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d300d330_avg
    ,  LAST_VALUE(d330d360_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d330d360_avg

    from(
    select *,
    CASE WHEN Sales_D3 is null THEN null ELSE AVG(Sales_D3/Sales_D1) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING ) END AS d1d3_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©
    CASE WHEN Sales_D7 is null THEN null ELSE AVG(Sales_D7/Sales_D3) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING) END AS d3d7_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©
    CASE WHEN Sales_D14 is null THEN null ELSE AVG(Sales_D14/Sales_D7) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d7d14_avg3,
    CASE WHEN Sales_D30 is null THEN null ELSE AVG(Sales_D30/Sales_D14) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d14d30_avg3,
    CASE WHEN Sales_D60 is null THEN null ELSE AVG(Sales_D60/Sales_D30) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d30d60_avg3,
    CASE WHEN Sales_D90 is null THEN null ELSE AVG(Sales_D90/Sales_D60) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d60d90_avg3,
    CASE WHEN Sales_D120 is null THEN null ELSE AVG(Sales_D120/Sales_D90) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d90d120_avg3,
    CASE WHEN Sales_D150 is null THEN null ELSE AVG(Sales_D150/Sales_D120) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d120d150_avg3,
    CASE WHEN Sales_D180 is null THEN null ELSE AVG(Sales_D180/Sales_D150) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d150d180_avg3,
    CASE WHEN Sales_D210 is null THEN null ELSE AVG(Sales_D210/Sales_D180) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d180d210_avg3,
    CASE WHEN Sales_D240 is null THEN null ELSE AVG(Sales_D240/Sales_D210) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d210d240_avg3,
    CASE WHEN Sales_D270 is null THEN null ELSE AVG(Sales_D270/Sales_D240) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d240d270_avg3,
    CASE WHEN Sales_D300 is null THEN null ELSE AVG(Sales_D300/Sales_D270) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d270d300_avg3,
    CASE WHEN Sales_D330 is null THEN null ELSE AVG(Sales_D330/Sales_D300) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d300d330_avg3,
    CASE WHEN Sales_D360 is null THEN null ELSE AVG(Sales_D360/Sales_D330) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d330d360_avg3
    from revraw
    )
    )
    )
    )


    ,final2 AS(
    select a.*, b.cost, b.cost_exclude_credit
    from final as a
    left join (
    select joyplegameid,  format_date('%Y-%m', cmpgndate) as month
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
    and cmpgndate >='2021-01-01'
    and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,  format_date('%Y-%m', cmpgndate)

    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.month = b.month
    )



    select joyplegameid, month, cost, cost_exclude_credit, ru, cost/ru as cpru,
    ru*d1_ltv/cost_exclude_credit as d1_roas,
    ru*d3_ltv/cost_exclude_credit as d3_roas,
    ru*d7_ltv/cost_exclude_credit as d7_roas,
    ru*d14_ltv/cost_exclude_credit as d14_roas,
    ru*d30_ltv/cost_exclude_credit as d30_roas,
    ru*d60_ltv/cost_exclude_credit as d60_roas,
    ru*d90_ltv/cost_exclude_credit as d90_roas,
    ru*d120_ltv/cost_exclude_credit as d120_roas,
    ru*d150_ltv/cost_exclude_credit as d150_roas,
    ru*d180_ltv/cost_exclude_credit as d180_roas,
    ru*d210_ltv/cost_exclude_credit as d210_roas,
    ru*d240_ltv/cost_exclude_credit as d240_roas,
    ru*d270_ltv/cost_exclude_credit as d270_roas,
    ru*d300_ltv/cost_exclude_credit as d300_roas,
    ru*d330_ltv/cost_exclude_credit as d330_roas,
    ru*d360_ltv/cost_exclude_credit as d360_roas,
    ru*d14_ltv_p/cost_exclude_credit as d14_roas_p,
    ru*d30_ltv_p/cost_exclude_credit as d30_roas_p,
    ru*d60_ltv_p/cost_exclude_credit as d60_roas_p,
    ru*d90_ltv_p/cost_exclude_credit as d90_roas_p,
    ru*d120_ltv_p/cost_exclude_credit as d120_roas_p,
    ru*d150_ltv_p/cost_exclude_credit as d150_roas_p,
    ru*d180_ltv_p/cost_exclude_credit as d180_roas_p,
    ru*d210_ltv_p/cost_exclude_credit as d210_roas_p,
    ru*d240_ltv_p/cost_exclude_credit as d240_roas_p,
    ru*d270_ltv_p/cost_exclude_credit as d270_roas_p,
    ru*d300_ltv_p/cost_exclude_credit as d300_roas_p,
    ru*d330_ltv_p/cost_exclude_credit as d330_roas_p,
    ru*d360_ltv_p/cost_exclude_credit as d360_roas_p
    from final2
    """

    query_result_pltv_growth = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()


    ### 2> pLTV_D360
    client = bigquery.Client()
    query = """with perfo_raw AS(
    select a.*
    , b.countrycode, b.os
    , b.gcat, b.mediacategory, b.class, b.media, b.adsetname, b.adname, b.optim, b.oscam, b.geocam, b.targetgroup
    from(
    select *,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_low end as combined_rev_low,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_upp end as combined_rev_upp,
    FROM `data-science-division-216308.VU.Performance_pLTV`
    where authaccountregdatekst>='2024-01-01'
    and authaccountregdatekst <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    and joyplegameid in (131,133)
    ) as a
    left join (select  *
    from `dataplatform-reporting.DataService.V_0316_0000_AuthAccountInfo_V`
    ) as b
    on a.authaccountname = b.authaccountname
    and a.joyplegameid = b.joyplegameid
    )



    select joyplegameid,  format_datE('%Y-%m',AuthAccountRegDateKST) as regmonth
    ,count(distinct if(daysfromregisterdate = 0, authaccountname, null)) as ru
    ,sum(if(daysfromregisterdate <= 360, combined_rev, null)) as pred_d360
    , max(authaccountregdatekst) as maxdate
    from perfo_raw
    group by joyplegameid, format_datE('%Y-%m',AuthAccountRegDateKST)
    """
    query_result_pltv_model = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()




    ### 3> ë³µê·€ìœ ì €
    client = bigquery.Client()
    query = """
    with raw AS(
    select *
    , sum(d90diff) over(partition by joyplegameid, authaccountname order by logdatekst) as cum_d90diff
    from(
    select *
    , date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day ) as daydiff_beforeaccess   -- authaccountlastaccessbeforedatekst : Access ê¸°ì¤€ìœ¼ë¡œ ë¡œê¹…
    , case when  date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day )  >= 90 then 1 else 0  end as d90diff
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    WHERE joyplegameid in (131,133,30001,30003)
    and logdatekst >= '2023-01-01'
    and DaysFromRegisterDate >= 0 -- ê°€ì…ì¼ì´ ì´í›„ì— ì°íŒ caseì œì™¸
    )
    )

    , raw2 AS(
    select *, date_diff(logdatekst, returndate, day) as daydiff_re -- ë³µê·€ì¼ cohort
    -- , if(returndate = AuthAccountRegDateKST, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    , if(cum_d90diff = 0, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    from(
    select *
    , first_value(logdatekst) over(partition by joyplegameid, authaccountname, cum_d90diff order by logdatekst) as returndate
    from raw
    )
    )

    , ru_raw AS(
    -- ì‹ ê·œ ìœ ì € ê¸°ì¤€
    select joyplegameid,  format_date('%Y-%m',authaccountregdatekst) as regmonth
    , count(distinct authaccountname) as ru
    , sum(if(DaysFromRegisterDate<=360, pricekrw, null)) as d360rev
    from raw2
    where  AuthAccountRegDateKST  >= '2023-01-01'
    and AuthAccountRegDateKST <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)

    group by joyplegameid,    format_date('%Y-%m',authaccountregdatekst)
    )

    , return_raw AS(
    -- ë³µê·€ìœ ì €
    select joyplegameid,  format_date('%Y-%m', returndate) as regmonth
    , count(distinct if(daydiff_re = 0 , authaccountname, null)) as ru
    , sum(if(daydiff_re<=360, pricekrw, null)) as d360rev_all
    from raw2
    where  returndate  >= '2023-01-01'
    and returndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,   format_date('%Y-%m', returndate)
    )


    ,final AS(
    select
    ifnull(ifnull(a.joyplegameid , b.joyplegameid) , c.joyplegameid)  as joyplegameid
    ,ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) as regmonth
    , a.ru ,b.ru as ru_all,
    d360rev  AS rev_D360,
    d360rev_all  AS rev_D360_all
    ,  cost
    ,  cost_exclude_credit
    , d360rev_all - d360rev as rev_D360_return ,
    case WHEN DATE_DIFF(
            DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY),
            LAST_DAY(DATE(CONCAT(ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) , '-01'))),
            DAY
            ) >= 360 THEN 'mature'
        ELSE 'notmature'
    END AS status
    from ru_raw  as a
    full join return_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    full join (
    select joyplegameid,  format_date('%Y-%m',cmpgndate) as regmonth, sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
    and cmpgndate >='2023-01-01'
    and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by  joyplegameid,  format_date('%Y-%m',cmpgndate)
    ) as c
    on a.joyplegameid = c.joyplegameid
    and a.regmonth = c.regmonth
    )


    #notmature êµ¬ê°„ ìµœê·¼ 6ê°œì›” í‰ê· 
    #POTC 2024/4 ~ 6ì›” ë¡œê·¸ì¸ ì´ìŠˆë¡œ ë³µê·€ìœ ì € ê¸°ì—¬ë„ ë‚®ì•„ ì œì™¸
    , return_user_proas AS(
    select joyplegameid, avg(rev_D360_return/cost_exclude_credit) as d360_return_roas
    , approx_quantiles(rev_D360_return/cost_exclude_credit, 2)[OFFSET(1)] as d360_return_roas_med
    from(
    select *, row_number() over (partition by joyplegameid order by regmonth desc) as rownum
    from final
    where status = 'mature'
    and ((joyplegameid = 131 and regmonth not in ('2024-04','2024-05','2024-06'))
    or joyplegameid in (133,30001,30003))
    )
    where rownum <= 6 -- ìµœê·¼ 6ê°œì›”
    group by joyplegameid
    )

    select a.*
    , rev_D360_return/cost_exclude_credit as d360_plus_return_actual
    , case when status = 'mature' then rev_D360_return/cost_exclude_credit
    else b.d360_return_roas_med  end as d360_plus_return_expected

    from final  as a
    left join return_user_proas as b
    on a.joyplegameid = b.joyplegameid

    """
    query_result_return = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()



    ### 4> BEP
    client = bigquery.Client()
    query = """
    with raw AS(
    select a.*, b.value
    , case when b.value is not null then b.value
    when b.value is null and a.PGName = 'Google' then 0.3
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 131 then 0.33
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 133 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30001 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30003 then 0.31
    when b.value is null and a.PGName = 'Xsolla' and a.joyplegameid = 131 then 0.15
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 133 then 0.10
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30001 then 0.09
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30003 then 0.08
    when b.value is null and  a.PGName = 'Danal' then 0.03
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 131 then 0.3
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 133 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30001 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30003 then 0.24
    when b.value is null and  a.PGName = 'Facebook Gaming' then 0.3
    when b.value is null and a.PGName = 'Steam' then 0.3
    else 0.3
    end as commission_rate
    from(
    select JoypleGameID, format_date('%Y-%m', authaccountregdatekst) as regmonth , t2.PGName, sum(t2.PGPriceKRW) as sales
    from  dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid in (131,133,30001,30003)
    and authaccountregdatekst >='2024-01-01'
    group by JoypleGameID, format_date('%Y-%m', authaccountregdatekst) , t2.PGName
    ) as a
    left join (
    select joyplegameid, regmonth, PGName,  value
    , case when PGName = 'One_Store' then 'One Store'
    when PGName = 'Facebook_Gaming' then 'Facebook Gaming'
    else PGName end as pgname2
    from `data-science-division-216308.Common.pg_commission_rate_copy2`
    unpivot (
    value for PGName in (Google,Apple, Xsolla	,Danal,	One_Store	,Facebook_Gaming,	Steam)
    )
    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.pgname = b.pgname2
    )

    -- BEP ê³„ì‚°
    select * , sum(sales) over(partition by joyplegameid , regmonth) as cumsales
    , sales /  sum(sales) over(partition by joyplegameid , regmonth) as sales_p
    , sum(commission) over(partition by joyplegameid , regmonth) as cumcommission
    , sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth) as total_commssion_rate
    , case when joyplegameid in (131,133) then 1/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
    when joyplegameid in (30001,30003) then 1.08/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
    end as bep_commission
    from(
    select  *, sales*commission_rate as commission
    from raw
    )
    """

    query_result_pg = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

    # 5> kpi roas (NY_ì¶”ê°€)
    client = bigquery.Client()
    query = """select * from data-science-division-216308.MetaData.roas_kpi
    where userType = 'ì‹ ê·œìœ ì €'and operationStatus = 'ìš´ì˜ ì¤‘'"""

    query_result_kpi = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()
    query_result_pltv_growth= query_result_pltv_growth.rename(columns={'month':'regmonth'})

    # 1. dXXX_roas íŒ¨í„´ì˜ ì»¬ëŸ¼ ì°¾ê¸°
    roas_cols = [col for col in query_result_pltv_growth.columns if re.fullmatch(r"d\d+_roas", col)]

    # 2. ê° roas ì»¬ëŸ¼ì— ëŒ€í•´ ëŒ€ì‘í•˜ëŠ” _p ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ _growth ì—´ ìƒì„±
    for col in roas_cols:
        p_col = f"{col}_p"
        if p_col in query_result_pltv_growth.columns:
            growth_col = col.replace("_roas", "roas_growth")
            query_result_pltv_growth[growth_col] = query_result_pltv_growth[col].fillna(query_result_pltv_growth[p_col])

    # 1.  ìµœì¢… ì„ íƒ ì»¬ëŸ¼
    base_cols = ['joyplegameid','regmonth', 'cost', 'cost_exclude_credit', 'ru','cpru', 'd7_roas']
    growth_cols = [col for col in query_result_pltv_growth.columns if re.fullmatch(r"d\d+roas_growth", col)]
    selected_cols = base_cols + growth_cols

    # 4. í•„í„°ë§ëœ í…Œì´ë¸” ì¶”ì¶œ
    query_result_pltv_growth2 = query_result_pltv_growth[selected_cols]


    final = pd.merge(query_result_pltv_growth2, query_result_pltv_model[['joyplegameid','regmonth','pred_d360']]
                                    ,    on=['joyplegameid', 'regmonth'],how = 'left' )


    final = pd.merge(final, query_result_return[['joyplegameid','regmonth','d360_plus_return_actual','d360_plus_return_expected','status']]
            ,on = ['joyplegameid','regmonth'], how = 'left')


    query_result_pg= query_result_pg.rename(columns={'month':'regmonth'})
    query_result_pg= query_result_pg.rename(columns={'JoypleGameID':'joyplegameid'})
    pg_distinct = (
        query_result_pg.loc[:, ["regmonth", "joyplegameid", "bep_commission"]]
                    .drop_duplicates(ignore_index=True)
    )

    final = pd.merge(final, pg_distinct,on = ['joyplegameid','regmonth'], how = 'left')

    ##game_name
    mapping = {
        131:   "1.POTC",
        133:   "2.GBTW",
        30001: "3.WWMC",
        30003: "4.DRSG",
    }

    final["game_name"] = final["joyplegameid"].map(mapping)  # ë§¤í•‘ ì—†ìœ¼ë©´ NaN

    final['bep_base'] = final['game_name'].map({
        '1.POTC': 1.429,
        '2.GBTW': 1.429,
        '3.WWMC': 1.543,
        '4.DRSG': 1.543
    })


    ## ê¸°ê°„ í•„í„°

    # ê¸°ì¤€ì¼ê³¼ ë˜ê·¸
    asof = pd.Timestamp.today().normalize()   # <-- ì—¬ê¸° í•µì‹¬ (tz ì—†ìŒ)
    LAG_DAYS = 8
    obs_end_candidate = asof - pd.Timedelta(days=LAG_DAYS)  # naive

    # ìµœê·¼ 12ê°œì›”ë§Œ í•„í„°ë§
    start_month = (asof.to_period("M") - 12).to_timestamp()
    final['regmonth_ts'] = pd.to_datetime(final['regmonth'] + "-01")

    final2 = final.copy()


    ## ë‹¹ì›” ì˜ˆìƒ COST
    # ì›”ì´ˆ/ì›”ë§, ì›”ì¼ìˆ˜
    final2['month_start'] = pd.to_datetime(final2['regmonth'] + '-01')
    final2['days_in_month'] = final2['month_start'].dt.daysinmonth
    final2['month_end'] = final2['month_start'] + pd.to_timedelta(final2['days_in_month'] - 1, unit='D')

    # 1) ìš°ì„  ì›”ë§ê³¼ obs_end_candidate ì¤‘ ì‘ì€ ê°’ì„ obs_endë¡œ
    final2['obs_end'] = final2['month_end'].where(
        final2['month_end'] <= obs_end_candidate,  # ë‘˜ ë‹¤ Timestamp
        other=obs_end_candidate
    )

    # 2) obs_end_candidateê°€ ì›”ì´ˆë³´ë‹¤ ë¹ ë¥´ë©´(=ê·¸ ë‹¬ì€ ì•„ì§ ì§‘ê³„ ì‹œì‘ ì „), NaTë¡œ
    final2.loc[final2['month_start'] > obs_end_candidate, 'obs_end'] = pd.NaT

    # ê´€ì¸¡ì‹œì‘ì¼ì€ ì›”ì´ˆë¡œ ê°€ì •
    final2['obs_start'] = final2['month_start']

    # ê´€ì¸¡ì¼ìˆ˜(ìŒìˆ˜/NaT ë°©ì§€)
    final2['observed_days'] = (
        (final2['obs_end'] - final2['obs_start']).dt.days + 1
    ).clip(lower=0).fillna(0).astype(int)

    # ì›” ì¼í•  ì˜ˆìƒë¹„ìš©
    final2['cost_raw'] = final2['cost_exclude_credit']  # ì›ë³¸ ë³´ê´€(ì˜µì…˜)
    final2['cost_exclude_credit'] = np.where(
        final2['observed_days'] > 0,
        final2['cost_raw'] * final2['days_in_month'] / final2['observed_days'],
        np.nan
    )

    # ì›ë³¸ ë³´ê´€
    final2['regmonth_base'] = final2['regmonth']

    # ì „ì²´ì—ì„œ ë§ˆì§€ë§‰ ì›”(YYYY-MM)
    last_month_str = final2['month_start'].max().to_period('M').strftime('%Y-%m')

    def _label_last_global(r):
        if r['regmonth_base'] == last_month_str and pd.notna(r['obs_end']):
            return f"{r['regmonth_base']} ( ~ {r['obs_end'].strftime('%m/%d')})"
        else:
            return r['regmonth_base']

    final2['regmonth'] = final2.apply(_label_last_global, axis=1)

    final3 = final2.eval("""
                        d360roas_pltv = pred_d360/cost_raw
                        d360roas_growth_plus_return_real = d360roas_growth+ d360_plus_return_actual
                        d360roas_growth_plus_return_expected = d360roas_growth + d360_plus_return_expected
                        bep_diff = bep_commission - bep_base
                        """)
    '''
    # ë³´ì¡° ì»¬ëŸ¼ ì •ë¦¬
    drop_cols = ['month_start','month_end','obs_start','obs_end','observed_days','days_in_month','regmonth_ts','cost','cost_raw']
    final3 = final3.drop(columns=drop_cols)

    final3.head()
    '''

    df = final3.copy()
    df = final3[final3['regmonth_ts'] >= start_month].copy()

    # ë³´ì¡° ì»¬ëŸ¼ ì •ë¦¬
    drop_cols = ['month_start','month_end','obs_start','obs_end','observed_days','days_in_month','regmonth_ts','cost','cost_raw']
    df = df.drop(columns=drop_cols)

    # DRSGë§Œ d180roas_growthë‘ê¸°
    if "d180roas_growth" in df.columns:
        mask = df["game_name"].astype(str).str.contains(r"\bDRSG\b", case=False, na=False)
        df.loc[~mask, "d180roas_growth"] = np.nan


    ## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
    font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
    if Path(font_path).exists():
        fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
        mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
        mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
    else:
        print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")




    # í•„ìš” ì»¬ëŸ¼ ì¡´ì¬ ì²´í¬(ì—†ìœ¼ë©´ KeyError ë°©ì§€ìš©ìœ¼ë¡œ ê±¸ëŸ¬ëƒ„)
    line_cols_all = [
        "bep_commission",
        "d360roas_growth",
        "d360roas_pltv",
        "d360roas_growth_plus_return_real",
        "d360roas_growth_plus_return_expected",
        "d180roas_growth",
    ]
    line_cols = [c for c in line_cols_all if c in df.columns]

    # ì»¬ëŸ¼ë³„ ìƒ‰ìƒ(ì›í•˜ëŠ” ìƒ‰/HEXë¡œ ë°”ê¿”ë„ ë¨)
    line_colors = {
        "bep_commission": "grey",
        "d360roas_growth": "DarkOrange",
        "d180roas_growth": "DarkOrange",
        "d360roas_pltv": "green",
        "d360roas_growth_plus_return_real": "brown",
        "d360roas_growth_plus_return_expected": "brown"
    }


    # ì ì„ ìœ¼ë¡œ ê·¸ë¦´ ëŒ€ìƒ
    linestyles = {
        "bep_commission": "--",
        "d360roas_growth_plus_return_real": "--",
        "d180roas_growth": "--",
    }


    # ì •ë ¬ ë° xë¼ë²¨ ì¤€ë¹„
    # 1) ì •ë ¬
    df["regmonth_dt"] = pd.to_datetime(df["regmonth"], errors="coerce")
    df = df.sort_values(["game_name", "regmonth_dt", "regmonth"], kind="mergesort").reset_index(drop=True)

    # 2) ê³ ìœ  x ì¢Œí‘œ(ì •ìˆ˜)ì™€ ë¼ë²¨ ë§Œë“¤ê¸° -- ì²«ë‹¬ë§Œ ê²Œì„ëª… í‘œê¸°
    df["xpos"] = np.arange(len(df))
    first_mask = df.groupby("game_name").cumcount() == 0
    xticklabels = np.where(
        first_mask,
        df["game_name"].astype(str) + " | " + df["regmonth"].astype(str),  # ê²Œì„ë³„ ì²« ë‹¬
        df["regmonth"].astype(str)                                         # ë‚˜ë¨¸ì§€ ë‹¬
    )


    ## graph
    fig, ax1 = plt.subplots(figsize=(15, 8))
    handles, labels = [], []
    fig.suptitle(
        "Monthly ROAS & Cost",           # ë©”ì¸ ì œëª©
        fontsize=16, fontweight="bold", y=0.98
    )


    # ì„ : ê²Œì„ë³„ë¡œ segmentë¥¼ ë‚˜ëˆ  ê·¸ë¦¬ë¯€ë¡œ ê²Œì„ ì‚¬ì´ê°€ ìë™ìœ¼ë¡œ "ëŠê¹€"
    for j, col in enumerate([c for c in line_cols if c in df.columns]):
        first_for_legend = True
        for g, gdf in df.groupby("game_name", sort=False):
            ln, = ax1.plot(
                gdf["xpos"], gdf[col],
                linestyle=linestyles.get(col, "-"),
                linewidth=2.0,
                #marker="o", markersize=3.5,
                color=line_colors.get(col),
                label=(col if first_for_legend else None)
            )
            first_for_legend = False
        handles.append(ln); labels.append(col)

    # ë¹„ìœ¨ yì¶•(0~300% ê³ ì •, í¼ì„¼íŠ¸ í‘œê¸°)
    ax1.set_ylim(0, 3.0) #yì¶• 300%ê¹Œì§€
    ax1.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))
    ax1.grid(axis="y", alpha=0.25)
    ax1.set_ylabel("ROAS (%, lines)")

    # ì´ì¤‘ì¶• ë§‰ëŒ€(cost)
    ax2 = ax1.twinx()

    bar_color = "#adb5bd"        # ì—°íšŒìƒ‰
    edge_color = "#495057"       # ì§„íšŒìƒ‰ í…Œë‘ë¦¬
    bar = ax2.bar(df["xpos"], df["cost_exclude_credit"], alpha=0.35, label="cost"
                ,color=bar_color, edgecolor=edge_color)
    handles += [bar]; labels += ["cost"]
    ax2.set_ylabel("Cost")


    # xì¶• ëˆˆê¸ˆì— ì»¤ìŠ¤í…€ ë¼ë²¨ ì ìš©
    ax1.set_xticks(df["xpos"])
    ax1.set_xticklabels(xticklabels, rotation=45, ha="right")

    # (ì„ íƒ) ê²Œì„ ê²½ê³„ì— ì„¸ë¡œ êµ¬ë¶„ì„ 
    boundary_pos = df.index[df["game_name"].ne(df["game_name"].shift()) & (df.index != 0)]
    for bp in boundary_pos:
        ax1.axvline(bp-0.5, color="lightgray", lw=1, alpha=0.6)


    ax1.legend(handles, labels, loc="best")
    plt.tight_layout()
    plt.savefig('roas_graph.png', dpi=160)
    plt.show()

    # íŠ¹ì • íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ í™•ì¸
    import os
    file_path = os.path.abspath("roas_graph.png")
    print("ì €ì¥ëœ íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ:", file_path)

    mapping = {"POTC": "1.POTC", "GBTW": "2.GBTW", "WWM": "3.WWMC", "DRSG": "4.DRSG", "RESU" : "5.RESU"}
    query_result_kpi['game'] = query_result_kpi['project'].map(mapping)
    cols = ['game', 'kpi_d1', 'kpi_d3', 'kpi_d7', 'kpi_d14', 'kpi_d30', 'kpi_d60',
    'kpi_d90', 'kpi_d120', 'kpi_d150', 'kpi_d180', 'kpi_d210',
    'kpi_d240', 'kpi_d270', 'kpi_d300', 'kpi_d330', 'kpi_d360']

    kpi = query_result_kpi[cols]


    kpi_by_game = {
        "1.POTC": kpi[kpi["game"] == "1.POTC"],
        "2.GBTW": kpi[kpi["game"] == "2.GBTW"],
        "3.WWMC": kpi[kpi["game"] == "3.WWMC"],
        "4.DRSG": kpi[kpi["game"] == "4.DRSG"],
    }

    for game in kpi_by_game.keys():

        df_tmp = kpi_by_game[game].copy()

        # ìˆ«ì ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (game ì œì™¸)
        num_cols = [c for c in df_tmp.columns if c != 'game']

        # "%" í˜•íƒœë¡œ ë³€í™˜
        df_tmp[num_cols] = df_tmp[num_cols].applymap(
            lambda x: f"{x*100:.2f}%" if pd.notna(x) else ""
        )

        kpi_by_game[game] = df_tmp

    base_cols = ['game_name','regmonth','cost_exclude_credit','cpru','d7_roas','regmonth_ts']
    growth_cols = [col for col in final3.columns if re.fullmatch(r"d\d+roas_growth", col)]
    base_cols2 = ['d360roas_pltv','d360_plus_return_actual','d360roas_growth_plus_return_real'
                ,'d360_plus_return_expected','d360roas_growth_plus_return_expected'
                ,'bep_base','bep_commission','bep_diff']

    selected_cols = base_cols + growth_cols+base_cols2

    final4 = final3[selected_cols].sort_values(by = ['game_name', 'regmonth'])
    final4 = final4[final4['regmonth_ts'] >= '2024-01-01']

    df_numeric = final4.copy()
    df_numeric = df_numeric.reset_index(drop=True)
    df_numeric = df_numeric.drop(columns='regmonth_ts')


    nest_asyncio.apply()


    ########### growth ì˜ˆì¸¡ì¹˜ íšŒìƒ‰ ìŒì˜ ë°˜ì˜
    # ì˜ˆì¸¡ ê¸°ì¤€ dnì„ ê° row(regmonth)ë³„ë¡œ ì¶”ë¡ 
    def infer_cohort_dn_map(df):
        cohort_map = {}
        for idx, row in df.iterrows():
            regmonth = idx[1] if isinstance(idx, tuple) else row.get('regmonth', None)
            for col in df.columns:
                if re.fullmatch(r'd\d+_roas_p', col):
                    if pd.notna(row[col]):
                        dn = int(re.findall(r'\d+', col)[0])
                        # ì˜ˆì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê°€ì¥ ì‘ì€ dn ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
                        if regmonth not in cohort_map or dn < cohort_map[regmonth]:
                            cohort_map[regmonth] = dn
        return cohort_map


    # ì˜ˆì¸¡ cohort í™•ì¸
    cohort_dn_map = infer_cohort_dn_map(query_result_pltv_growth)
    cohort_dn_map

    # ì»¬ëŸ¼ ì´ë¦„ì—ì„œ dn ê°’ ì¶”ì¶œ
    def extract_dn(col):
        match = re.match(r'd(\d+)', col)  # d ë’¤ì˜ ìˆ«ìë§Œ ì¶”ì¶œ
        return int(match.group(1)) if match else None


    dn_growth_columns = [col for col in df_numeric.columns if re.fullmatch(r'd\d+roas_growth', col)]
    dn_values = {col: extract_dn(col) for col in dn_growth_columns}


    # regmonth ë¬¸ìì•ˆì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    def resolve_cohort_dn(regmonth, cohort_dn_map):
        r = str(regmonth or "")
        # í¬í•¨ ë§¤ì¹­ë˜ëŠ” ëª¨ë“  í‚¤ ì¤‘ ê°€ì¥ ê¸´ í‚¤(ë” êµ¬ì²´ì )ë¥¼ ìš°ì„ 
        candidates = [(k, v) for k, v in cohort_dn_map.items() if k and str(k) in r]
        if not candidates:
            return np.inf
        # í‚¤ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìµœì¥ ë§¤ì¹­ ìš°ì„ 
        k, v = max(candidates, key=lambda kv: len(str(kv[0])))
        return v

    ## í…ŒìŠ¤íŠ¸ ì½”ë“œ

    # --- [1] ì»¬ëŸ¼ëª… ë³€ê²½ ë§µ ì •ì˜ ---
    rename_dict = {
        "d7_roas": "growth_d7",
        "d360roas_pltv": "pltv_d360",
        "d360_plus_return_actual": "return_d360",
        "d360roas_growth_plus_return_real": "gr_plus_ret_act_d360",
        "d360_plus_return_expected": "return_exp_d360",
        "d360roas_growth_plus_return_expected": "gr_plus_ret_exp_d360"
    }
    for d in [14, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]:
        rename_dict[f"d{d}roas_growth"] = f"growth_d{d}"

    # --- [2] ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ëª… ì„ ì œ ë³€ê²½ ---
    df_numeric = df_numeric.rename(columns=rename_dict)

    # --- [3] ë³€ê²½ëœ ì´ë¦„ ê¸°ì¤€ í—¬í¼ ë³€ìˆ˜ ì¬ì„¤ì • ---
    # ì´ì œ growth_dë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ì—ì„œ ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    def extract_dn_new(col):
        match = re.search(r'd(\d+)', col)
        return int(match.group(1)) if match else None

    # ë°”ë€ ì»¬ëŸ¼ëª… ê¸°ë°˜ìœ¼ë¡œ dn_values ìƒì„±
    dn_growth_columns = [col for col in df_numeric.columns if col.startswith('growth_d')]
    dn_values = {col: extract_dn_new(col) for col in dn_growth_columns}

    # --- [4] ìŠ¤íƒ€ì¼ í•¨ìˆ˜ë“¤ ì—…ë°ì´íŠ¸ (ìƒˆ ì´ë¦„ ë°˜ì˜) ---
    def highlight_based_on_dn(row):
        regmonth = row['regmonth']
        cohort_dn = resolve_cohort_dn(regmonth, cohort_dn_map)
        return [
            (
                'background-color: lightgray'
                if (
                    col.startswith('growth_d') and # ì¡°ê±´ ë³€ê²½
                    isinstance(dn_values.get(col), (int, float)) and
                    isinstance(cohort_dn, (int, float)) and
                    dn_values[col] >= cohort_dn and
                    pd.notna(row[col])
                ) else ''
            )
            for col in row.index
        ]

    def highlight_roas_vs_bep(row):
        styles = []
        for col in row.index:
            style = ""
            try:
                val = row[col]
                # % ì²˜ë¦¬ ë¡œì§ì€ ë™ì¼
                roas_val = float(val.replace('%', '')) / 100 if isinstance(val, str) and val.endswith('%') else val

                # growth_d ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ ì²´í¬
                if col.startswith("growth_d") and pd.notnull(row.get("bep_base")):
                    if pd.notnull(roas_val) and roas_val > row["bep_base"]:
                        style = "background-color: #fbe4e6"

                # ë°”ë€ d360 í”ŒëŸ¬ìŠ¤ ì»¬ëŸ¼ëª… ì²´í¬
                elif col in ["gr_plus_ret_act_d360", "gr_plus_ret_exp_d360"] and pd.notnull(row.get("bep_commission")):
                    if pd.notnull(roas_val) and roas_val > row["bep_commission"]:
                        style = "background-color: #fbe4e6"
            except: pass
            styles.append(style)
        return styles

    def highlight_over_kpi(row, kpi_df):
        game = row['game_name']
        target = kpi_df[kpi_df['game'] == game]
        if target.empty: return [''] * len(row)
        target_row = target.iloc[0]
        styles = []

        for col in row.index:
            base_style = ""
            # d7_roas -> growth_d7
            if col == "growth_d7":
                kpi_val = target_row.get("kpi_d7")
                if pd.notna(row[col]) and pd.notna(kpi_val) and row[col] >= kpi_val:
                    base_style = "color: red; font-weight: bold;"

            # growth_d?? í˜•íƒœ ì²´í¬
            elif col.startswith("growth_d"):
                dn = extract_dn_new(col)
                kpi_col = f"kpi_d{dn}"
                if kpi_col in target_row:
                    kpi_val = target_row[kpi_col]
                    if pd.notna(row[col]) and pd.notna(kpi_val) and row[col] >= kpi_val:
                        base_style = "color: red; font-weight: bold;"

            styles.append(base_style)
        return styles

    # --- [5] ê²Œì„ë³„ í‘œ ë¶„ë¦¬ ë° Styler/RAW ìƒì„± ---
    game_groups = df_numeric.groupby('game_name')
    styled_tables = {}
    raw_df_by_game = {}

    # í¬ë§·íŒ… ëŒ€ìƒ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ìƒˆë¡œìš´ ì»¬ëŸ¼ëª… ê¸°ì¤€)
    format_percent_cols = [
        "growth_d7", "pltv_d360", "return_d360", "bep_base", "bep_commission", "bep_diff",
        "gr_plus_ret_act_d360", "gr_plus_ret_exp_d360", "return_exp_d360",
    ]
    format_comma_cols = ["cost_exclude_credit", "cpru"]

    for game, game_df in game_groups:
        # A. ì´ë¯¸ì§€ìš© ìŠ¤íƒ€ì¼ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
        growth_cols = [col for col in game_df.columns if col.startswith("growth_d")]

        styled_game = game_df.style\
            .format({
                "cost_exclude_credit": "{:,.0f}",
                "cpru": "{:,.0f}",
                # ëª¨ë“  growth ê´€ë ¨ ë° ì§€í‘œ ì»¬ëŸ¼ì— % ì ìš©
                **{col: "{:.2%}" for col in game_df.columns if col.startswith("growth") or any(x in col for x in ["pltv", "return", "ret", "bep"])}
            })\
            .bar(subset=['cost_exclude_credit', 'bep_diff'], color='#f4cccc')\
            .bar(subset=['cpru'], color='#b6d7a8')\
            .bar(subset=growth_cols, color='#c9daf8')\
            .bar(subset=['return_d360', 'return_exp_d360'], color='#ffe599')\
            .set_table_styles([
                {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('font-weight', 'bold')]}
            ])\
            .apply(highlight_based_on_dn, axis=1)\
            .apply(highlight_roas_vs_bep, axis=1)\
            .apply(highlight_over_kpi, axis=1, kpi_df=kpi)

        styled_tables[game] = styled_game

        # B. ë…¸ì…˜ ì—…ë¡œë“œìš© RAW ë°ì´í„° í¬ë§·íŒ… (ì‹¤ì œ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
        notion_df = game_df.copy()

        # 1. ì½¤ë§ˆ ì ìš© (ë¹„ìš©, CPRU)
        for col in format_comma_cols:
            if col in notion_df.columns:
                notion_df[col] = notion_df[col].map(lambda x: f"{x:,.0f}" if pd.notnull(x) else "")

        # 2. í¼ì„¼íŠ¸ ì ìš© (ëª¨ë“  growth_ ê´€ë ¨ ë° KPI ê´€ë ¨)
        for col in notion_df.columns:
            # ì»¬ëŸ¼ëª…ì´ growthë¡œ ì‹œì‘í•˜ê±°ë‚˜, í¼ì„¼íŠ¸ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ê²½ìš°
            if col.startswith("growth") or col in format_percent_cols:
                notion_df[col] = notion_df[col].map(lambda x: f"{x:.2%}" if pd.notnull(x) else "")

        raw_df_by_game[game] = notion_df

    # styled_tables["1.POTC"]
    # print(f"Checking {game}: {styled_table.columns.tolist()[:30]}...")

    ## ë…¸ì…˜ í…Œì´ë¸” í˜•íƒœë¡œ ë„í‘œ í˜•ì„±
    def df_to_table_rows(df, max_rows=100):
        rows = []

        # header
        rows.append({
            "object": "block",
            "type": "table_row",
            "table_row": {
                "cells": [
                    [{"type": "text", "text": {"content": str(col)}}]
                    for col in df.columns
                ]
            }
        })

        # data
        for _, r in df.head(max_rows - 1).iterrows():
            rows.append({
                "object": "block",
                "type": "table_row",
                "table_row": {
                    "cells": [
                        [{"type": "text", "text": {"content": "" if pd.isna(v) else str(v)}}]
                        for v in r.tolist()
                    ]
                }
            })

        return rows

    # HTML í…œí”Œë¦¿ ì •ì˜
    # (NY_ì¶”ê°€) í…Œì´ë¸” í¬ê¸° ì¡°ì • & í…Œì´ë¸” ì¶”ê°€
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            table { border-collapse: collapse; font-size: 20px; }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: right;
            }
            th { background-color: #f0f0f0; }
        </style>
    </head>
    <body>

        <h2>{{ game_name }} ì›”ê°„ ROAS raw</h2>
        {{ table | safe }}

        <hr>

        <h2>{{ game_name }} KPI Table</h2>
        {{ kpi_table | safe }}
    </body>
    </html>
    """


    # ê° ê²Œì„ë³„ë¡œ HTML íŒŒì¼ ì €ì¥
    for game, styled_table in styled_tables.items():
        # í…Œì´ë¸”ì„ HTMLë¡œ ë³€í™˜
        table_html = styled_table.to_html()

        #(NY_ì¶”ê°€) ROAS kpi ë„í‘œë„ í•˜ë‹¨ì— ì¶”ê°€
        kpi_html = kpi_by_game[game].to_html(index=False, classes="kpi-table",
        escape=False   # % ê¸°í˜¸ ìœ ì§€
                                            )

        # HTML íŒŒì¼ë¡œ ì €ì¥
        rendered_html = Template(html_template).render(game_name=game, table=table_html, kpi_table=kpi_html)

        # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
        html_path = f"{game}_roas_table.html"

    # HTML íŒŒì¼ ì €ì¥
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(rendered_html)

        print(f"{game} í…Œì´ë¸” HTMLë¡œ ì €ì¥ ì™„ë£Œ: {html_path}")

        # dfi.export(
        #     styled_table,
        #     html_path,
        #     table_conversion='playwright', # í˜¹ì€ 'playwright'
        #     dpi=500
        # )

        # print(f"{game} í…Œì´ë¸” ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ (dfi ë°©ì‹): {html_path}")


    # ì´ë¯¸ì§€ ìº¡ì²˜ ë¹„ë™ê¸° í•¨ìˆ˜
    # ìˆ˜ì •ëœ capture_html_to_image í•¨ìˆ˜
    async def capture_html_to_image(html_path, output_image_path):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)

            # device_scale_factorë¥¼ 2 ë˜ëŠ” 3ìœ¼ë¡œ ì„¤ì • (ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ê³ í™”ì§ˆ)
            # ë„ˆë¹„(width)ë„ í…Œì´ë¸”ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë„‰ë„‰í•˜ê²Œ ì„¤ì •í•˜ì„¸ìš”.
            context = await browser.new_context(
                viewport={"width": 600, "height": 400},
                device_scale_factor= 1  # 4ë°° ì„ ëª…í•˜ê²Œ ìº¡ì²˜
            )
            page = await context.new_page()

            await page.goto("file://" + os.path.abspath(html_path), wait_until="networkidle")

            # ì´ë¯¸ì§€ ìº¡ì³
            await page.screenshot(path=output_image_path, full_page=True, animations="disabled")

            await browser.close()

    # # ê²Œì„ë³„ HTMLì„ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ê¸°
    async def save_images_for_all_games():
        for game in styled_tables.keys():
            html_path = f"{game}_roas_table.html"  # HTML íŒŒì¼ ê²½ë¡œ
            output_image_path = f"{game}_roas_table.png"  # ì €ì¥ë  ì´ë¯¸ì§€ ê²½ë¡œ

            await capture_html_to_image(html_path, output_image_path)
            print(f"{game} í…Œì´ë¸”ì„ ì´ë¯¸ì§€ë¡œ ì €ì¥ ì™„ë£Œ: {output_image_path}")

    # ë¹„ë™ê¸° ì‹¤í–‰ (Spyderë‚˜ GCP í™˜ê²½ì—ì„œ)
    asyncio.get_event_loop().run_until_complete(save_images_for_all_games())

    #### gemini ë¶„ì„ ë°ì´í„° í•„í„°
    # ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ ì¶”ì¶œ
    recent_data = final3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(6)
    #recent_data = final3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(7).sort_values(['game_name', 'regmonth']).groupby('game_name').head(6)

    exclude_cols = ['d360_plus_return_actual', 'd360_plus_return_expected','joyplegameid','pred_d360','cost','cost_raw','d360roas_pltv','ru','d360roas_growth_plus_return_real']
    recent_data = recent_data.drop(columns=exclude_cols)


    # í…Œì´ë¸” í˜•íƒœë¡œ ë¬¸ìì—´í™” (Markdown table)
    def df_to_md(df):
        header = "| " + " | ".join(df.columns) + " |"
        sep = "| " + " | ".join(["---"]*len(df.columns)) + " |"
        rows = "\n".join("| " + " | ".join(map(str, row)) + " |" for row in df.values)
        return "\n".join([header, sep, rows])

    recent_data_md = df_to_md(recent_data)

    PROJECTS = [
        {"project": "POTC", "database_id": "1eeea67a56818058a431dc9a754beeab"},
        {"project": "GBTW", "database_id": "1eeea67a568180f18048dcc7769a3621"},
        {"project": "WWMC", "database_id": "1eeea67a568180449ca2d06c15779d6e"},
        {"project": "DRSG", "database_id": "1eeea67a5681809b8da6ddb8e6e9d0d5"},
    ]


    # Notion API ë ˆì´íŠ¸ë¦¬ë°‹(í‰ê·  3rps ê¶Œì¥) â†’ ì•ˆì „ìŠ¬ë¦½
    NOTION_SLEEP_SEC = 0.4


    # ê¸°ì¤€ì¼(ìµœê·¼ 8ì¼ ì „) â€” ì›”ê°„ ë¦¬í¬íŠ¸ íƒìƒ‰ ì‹œ ê¸°ì¤€ì´ ë˜ëŠ” ë‚ ì§œ
    REF_DT = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=8)

    notion = NotionClient(auth=NOTION_TOKEN, timeout_ms=60_000, log_level=logging.WARNING, notion_version=NOTION_VERSION)
    print(NOTION_VERSION)

    def sleep():
        time.sleep(NOTION_SLEEP_SEC + random.uniform(0, 0.25))  # + ì§€í„°


    # ëª©ì : íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì—ì„œ "title" íƒ€ì… ì†ì„±ì˜ ì‹¤ì œ ì†ì„±ëª…ì„ ì¡°íšŒ
    # ì…ë ¥: database_id (str)
    # ì¶œë ¥: ì œëª© ì†ì„±ëª… (str) â€” ì˜ˆ: "Name", "ì œëª©" ë“±
    def get_title_prop_name(database_id: str) -> str:
        schema = notion.databases.retrieve(database_id=database_id)
        sleep()
        print(NOTION_VERSION)

        for name, prop in schema.get("properties", {}).items():
            if prop.get("type") == "title":
                return name
        raise RuntimeError("title íƒ€ì… ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ëª©ì : datetime â†’ 'YYë…„ Mì›”' í•œêµ­ì‹ ì—°/ì›” ë¬¸ìì—´ë¡œ ë³€í™˜
    # ì…ë ¥: dt (datetime)
    # ì¶œë ¥: ì˜ˆ) '25ë…„ 9ì›”'
    def format_kor_year_month(dt: datetime) -> str:
        # '25ë…„ 9ì›”'
        yy = dt.year % 100
        m  = dt.month
        return f"{yy}ë…„ {m}ì›”"

    # ëª©ì : datetime â†’ 'YYYY-MM' í˜•íƒœì˜ ì›” í‚¤ ìƒì„±(ë©”íƒ€ í‚¤/ë¦¬ìŠ¤íŠ¸ë¦­íŠ¸ ìš©)
    # ì…ë ¥: dt (datetime)
    # ì¶œë ¥: ì˜ˆ) '2025-09'
    def ym_key(dt: datetime) -> str:
        # 'YYYY-MM' (restricts/ë©”íƒ€ í‚¤)
        return f"{dt.year:04d}-{dt.month:02d}"


    # ëª©ì : í”„ë¡œì íŠ¸ ì›”ê°„ ë¦¬í¬íŠ¸ í˜ì´ì§€ 1ê±´ì„ Notion DBì—ì„œ ì œëª© íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
    #  - ì œëª© íŒ¨í„´: f"[{project}] {YY}ë…„ {M}ì›” ëˆ„ì  UA ë¦¬í¬íŠ¸"
    #  - '9ì›”'/'09ì›”' ëª¨ë‘ ë§¤ì¹­(OR)
    # ì…ë ¥: database_id (str), project (str), ref_dt (datetime|None)
    # ì¶œë ¥: {project, page_id, ym_title, ym, page_url} ë˜ëŠ” None
    def find_month_page_by_title(database_id: str, project: str, ref_dt: Optional[datetime] = None):
        if ref_dt is None:
            ref_dt = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=8)
        ym_kor = format_kor_year_month(ref_dt)       # '25ë…„ 9ì›”'
        ym_kor_zero = f"{ref_dt.year%100}ë…„ {ref_dt.month:02d}ì›”"  # '25ë…„ 09ì›”' (ì œëª©ì— 0íŒ¨ë”©ì¼ ë•Œ ëŒ€ë¹„)

        title_prop = get_title_prop_name(database_id)

        # contains AND (project / ê³ ì •ë¬¸êµ¬) + (ì›” í‘œê¸°ëŠ” OR)
        filt = {
            "and": [
                {"property": title_prop, "title": {"contains": f"[{project}]"}},
                {"property": title_prop, "title": {"contains": "ëˆ„ì  UA ë¦¬í¬íŠ¸"}},
                {"or": [
                    {"property": title_prop, "title": {"contains": ym_kor}},
                    {"property": title_prop, "title": {"contains": ym_kor_zero}},
                ]}
            ]
        }
        print(NOTION_VERSION)

        q = notion.databases.query(
            **{
                "database_id": database_id,
                "filter": filt,
                "sorts": [{"timestamp": "last_edited_time", "direction": "descending"}],
                "page_size": 1
            }
        )
        sleep()
        results = q.get("results", [])
        if not results:
            return None
        page = results[0]
        page_id = page["id"]
        return {
            "project": project,
            "page_id": page_id,
            "ym_title": ym_kor,
            "ym": ym_key(ref_dt),
            "page_url": f"https://www.notion.so/{page_id.replace('-','')}",


        }

    ################################ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # ==== 2) í˜ì´ì§€ â†’ ì„¹ì…˜(ì£¼ê°„) ì¶”ì¶œ(ì¬ê·€ + í˜ì´ì§€ë„¤ì´ì…˜) =========================
    def list_all_children(block_id: str):
        results, cursor = [], None
        while True:
            resp = notion.blocks.children.list(block_id=block_id, start_cursor=cursor)
            results.extend(resp["results"])
            sleep()
            if not resp.get("has_more"):
                break
            cursor = resp.get("next_cursor")
        return results

    def rts_to_text(rts):  # rich_text â†’ plain
        return "".join(rt.get("plain_text","") for rt in (rts or []))

    def block_to_text(block, include_children=True) -> str:
        t = block["type"]; b = block.get(t, {}); lines: List[str] = []
        def add(x):
            if x and x.strip():
                lines.append(x.strip())

        if t in ("paragraph","heading_1","heading_2","heading_3",
                "bulleted_list_item","numbered_list_item","to_do","quote","callout","code"):
            txt = rts_to_text(b.get("rich_text", []))
            if t=="bulleted_list_item" and txt: txt = "- " + txt
            if t=="numbered_list_item" and txt: txt = "1. " + txt
            if t=="to_do": txt = f"{'[x]' if b.get('checked') else '[ ]'} {txt}"
            add(txt)
        elif t=="bookmark":
            add(b.get("url",""))
        elif t=="table":
            # table_row children â†’ cell í…ìŠ¤íŠ¸ ' | 'ë¡œ ê²°í•©
            for row in list_all_children(block["id"]):
                if row["type"]=="table_row":
                    cells = row["table_row"]["cells"]
                    row_txt = " | ".join(rts_to_text(c) for c in cells)
                    add(row_txt)

            # has_childrenì´ Trueì¼ ë•Œë§Œ ìì‹ ë¸”ë¡ì„ ì²˜ë¦¬
        if include_children and block.get("has_children"):
            for ch in list_all_children(block["id"]):
                sub = block_to_text(ch, include_children=True)
                if sub:
                    lines.append(sub)

        return "\n".join(lines)


    def detect_heading_level(top_blocks):
        """top ë¸”ë¡ë“¤ì—ì„œ ì‚¬ìš©í•  í—¤ë”© ë ˆë²¨ì„ ìë™ ì„ íƒ"""
        for ht in ("heading_1", "heading_2", "heading_3"):
            if any(b["type"] == ht for b in top_blocks):
                return ht
        return None

    def extract_sections_by_heading_auto(page_id: str, include_preface=False):
        """
        - heading_1 ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ì„¹ì…˜í™”
        - ì—†ìœ¼ë©´ heading_2 â†’ ì—†ìœ¼ë©´ heading_3
        - ì–´ë–¤ í—¤ë”©ë„ ì—†ìœ¼ë©´ ì „ì²´ ë‚´ìš©ì„ 1ê°œ ì„¹ì…˜ìœ¼ë¡œ ë°˜í™˜
        - include_preface=Trueë©´ ì²« í—¤ë”© ì „ ë¬¸ë‹¨ì„ '(ì„œë¬¸)' ì„¹ì…˜ìœ¼ë¡œ í¬í•¨
        """
        top = list_all_children(page_id)
        heading_type = detect_heading_level(top)

        # í—¤ë”©ì´ ì „í˜€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¶ì–´ì„œ ë°˜í™˜
        if not heading_type:
            all_lines = []
            for blk in top:
                t = block_to_text(blk, include_children=True)
                if t: all_lines.append(t)
            content = "\n".join(all_lines).strip()
            return [{"title": "(ì „ì²´)", "content": content}] if content else []

        sections = []
        current = None
        preface_lines = []

        i = 0
        while i < len(top):
            blk = top[i]
            btype = blk["type"]

            if btype == heading_type:
                # ì´ì „ ì„¹ì…˜ ë§ˆê°
                if current:
                    sections.append(current)

                # ì œëª©(ìì‹ ì œì™¸)
                title = block_to_text(blk, include_children=False) or "Untitled"
                current = {"title": title, "content_lines": []}

                # ì´ í—¤ë”©ì˜ ìì‹(í† ê¸€/í‘œ/ë¦¬ìŠ¤íŠ¸ ë“±) í¬í•¨
                if blk.get("has_children"):
                    for ch in list_all_children(blk["id"]):
                        sub = block_to_text(ch, include_children=True)
                        if sub:
                            current["content_lines"].append(sub)

                i += 1
                continue

            # ì²« í—¤ë”© ë‚˜ì˜¤ê¸° ì „ ë¬¸ë‹¨ì€ í”„ë¦¬í˜ì´ìŠ¤ë¡œ ëª¨ì„ ìˆ˜ ìˆìŒ
            if current is None and include_preface:
                txt = block_to_text(blk, include_children=True)
                if txt:
                    preface_lines.append(txt)
            elif current is not None:
                # ë‹¤ìŒ ë™ì¼ ë ˆë²¨ í—¤ë”© ì „ê¹Œì§€ í˜•ì œ ë¸”ë¡ì„ ë³¸ë¬¸ì— í¬í•¨
                txt = block_to_text(blk, include_children=True)
                if txt:
                    current["content_lines"].append(txt)

            i += 1

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ë§ˆê°
        if current:
            sections.append(current)

        # í”„ë¦¬í˜ì´ìŠ¤ ì„¹ì…˜ ì‚½ì…
        if include_preface and preface_lines:
            preface = "\n".join(preface_lines).strip()
            if preface:
                sections.insert(0, {"title": "(ì„œë¬¸)", "content": preface})

        # content_lines ë³‘í•© + ë¹ˆ ì„¹ì…˜ ì œê±°
        out = []
        for s in sections:
            content = "\n".join(s.get("content_lines", [])).strip()
            if content:
                out.append({"title": s["title"], "content": content})

        return out

    ############################### ë…¸ì…˜ Weekly Report ì „ë¬¸ ì¶”ì¶œ

    weekly_report_db3 = []

    # ëª©ì : ê° í”„ë¡œì íŠ¸ë³„ ì›”ê°„ í˜ì´ì§€ë¥¼ ì°¾ì•„ ì„¹ì…˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ â†’ LLM ìš”ì•½ â†’ ìš”ì•½ë³¸ DBì— ì ì¬

    for p in PROJECTS:
        print(f"\n=== {p['project']} ===")
        sel = find_month_page_by_title(p["database_id"], p["project"], ref_dt=REF_DT)
        print(f"  [SELECT] {sel}")
        if not sel:
            print("  [WARN] ë‹¹ì›” ì œëª© íŒ¨í„´ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            weekly_report_db3.append({
                "project": p['project'],
                "summary": "(í•´ë‹¹ ì›” ì£¼ê°„ ë¦¬í¬íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
            })
            continue

        page_id = sel["page_id"]
        sections = extract_sections_by_heading_auto(page_id, include_preface=False)

        if not sections:
            print("  [HINT] ë™ì¼ ë ˆë²¨ í—¤ë”©ì´ ì—†ê³  ë³¸ë¬¸ì´ childrenì—ë§Œ ìˆì„ ìˆ˜ ìˆì–´ ìµœìƒìœ„ ë¸”ë¡ êµ¬ì¡°ë¥¼ ì¬ì ê²€í•˜ì„¸ìš”.")
            weekly_report_db3.append({
                "project": p['project'],
                "summary": "ì¶”ì¶œ ê°€ëŠ¥í•œ ë³¸ë¬¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            })
            continue

        # ì „ì²´ ì„¹ì…˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        full_report_text = "\n\n".join([s['content'] for s in sections])

        ## gemini 3.0 high ìœ„í•œ ì½”ë“œ ì¶”ê°€
        weekly_report_db3.append({
            "project": p['project'],
            "full_report_text": full_report_text
        })

    # ì‹œìŠ¤í…œ ì§€ì‹œ_í˜ë¥´ì†Œë‚˜ ë° ì œì•½ì¡°ê±´

    system_instruction = """
    ë„ˆëŠ” ì „ë¬¸ ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ê°€ì•¼.
    ì£¼ì–´ì§„ ROAS ë°ì´í„°ì™€ í¼í¬ë¨¼ìŠ¤íŒ€ì˜ ì›ë¬¸ ë¦¬í¬íŠ¸ë¥¼ **ì ˆëŒ€ ì˜¤ë¥˜ ì—†ì´ ë¶„ì„**í•˜ê³ , ìš”ì²­ëœ **ëª¨ë“  ì¶œë ¥ í˜•ì‹ ê·œì¹™**ì„ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì•¼í•´.

    [ë°ì´í„° ì •í•©ì„± ìµœìš°ì„  ê·œì¹™]
    1. ëª¨ë“  ìˆ˜ì¹˜ ë¹„êµ (BEP ë‹¬ì„±, ì¦ê°ë¥  ê³„ì‚°)ëŠ” ì˜¤ì§ ì œê³µëœ í…Œì´ë¸” ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•´
    2. í…Œì´ë¸”ì— ì—†ëŠ” ë°ì´í„°ë‚˜ ì¶”ë¡ ì€ ì—„ê¸ˆí•˜ë©°, ë¹„êµ ëŒ€ìƒì€ ë™ì¼í•œ ê²Œì„ ë‚´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì‹œì (ì›”)ì˜ ë™ì¼í•œ ì§€í‘œ(ì—´)ì´ì•¼

    [í‘œê¸°ë²• ê·œì¹™]
    - cost, install ru, CPI, cpruëŠ” ì²œë‹¨ìœ„ ì‰¼í‘œ(,)ë¥¼ ì‚¬ìš©
    - ROAS ê´€ë ¨ ì§€í‘œëŠ” ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ í‘œê¸°í•˜ê³  '%' ë‹¨ìœ„ë¥¼ ì‚¬ìš©
    - ì¦ê°ë¥ ì„ ì´ì•¼ê¸°í•  ë•ŒëŠ” +- ê¸°í˜¸ ëŒ€ì‹  ğŸ”º(ìƒìŠ¹) ë˜ëŠ” ğŸ”»(í•˜ë½) ê¸°í˜¸ë¥¼ ìˆ«ìì•ì— ì‚¬ìš©í•´ì¤˜
    - ë³€ìˆ˜ëª…ì— ëŒ€í•œ ì–¸ê¸‰ ì œì™¸

    [ì¶œë ¥í˜•ì‹ ê·œì¹™]
    - ë§ˆí¬ë‹¤ìš´ í¬ë§·: ë…¸ì…˜ ë§ˆí¬ë‹¤ìš´ í¬ë§·ì„ ì‚¬ìš©í•´
    - ë¦¬í¬íŠ¸ ì‘ì„± ì™„ë£Œí–ˆë‹¤ëŠ” ë‚´ìš©ì€ ë³„ë„ë¡œ ì–¸ê¸‰í•˜ì§€ë§ˆ

    """
    # 2. ë°ì´í„° í•™ìŠµ
    prompt_description_3_optimized = f"""
    ## ë°ì´í„° ì…ë ¥
    SLG ê²Œì„ 4ì¢…ì˜ ì›”ë³„ ROAS í˜„í™© ë°ì´í„° ë° ì£¼ìš” ì§€í‘œ ì„¤ëª…ì´ì•¼

    [ì£¼ìš” ë³€ìˆ˜ ì„¤ëª…]
    - cost_exclude_credit: í¬ë ˆë”§ ì œì™¸ ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ (roas ê³„ì‚° ê¸°ì¤€, ë‹¹ì›”ì€ ì¼í• ê³„ì‚° ì¶”ì • ì›” cost)
    - cpru: ë‹¨ê°€
    - d360roas_growth: ë³µê·€ìœ ì € ë¯¸í¬í•¨ ì‹ ê·œìœ ì € d360 ì˜ˆì¸¡ì¹˜
    - d360roas_growth_plus_return_expected: ë³µê·€ìœ ì € í¬í•¨ d360 ì˜ˆì¸¡ì¹˜
    - bep_commission: ìˆ˜ìˆ˜ë£Œ ê³ ë ¤í•œ BEP

    --- ROAS ë°ì´í„° í…Œì´ë¸” ---
    ì‹¤ì œ ì›”ë³„ ë§ˆì¼€íŒ… ë°ì´í„°ì•¼
    ì§€í‘œì— ëŒ€í•´ ì–¸ê¸‰í•  ë•ŒëŠ” í•´ë‹¹ í…Œì´ë¸” ìˆ˜ì¹˜ë§Œì„ ì‚¬ìš©í•´
    {recent_data_md}

    --- í¼í¬ë¨¼ìŠ¤íŒ€ ì£¼ê°„ ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ---
    í¼í¬ë¨¼ìŠ¤íŒ€ì—ì„œ ì‘ì„±í•œ ì£¼ê°„ ë¦¬í¬íŠ¸ ë‚´ìš©ì´ì•¼
    ë‹¹ì›”ì˜ ì£¼ìš” ì´ìŠˆì‚¬í•­ì„ ì•„ë˜ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì°¸ê³  í•´
    {weekly_report_db3}
    """

    prompt_parts_3_final = [
        prompt_description_3_optimized,
        """
    ### ë§ˆì¼€íŒ… ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± ìš”ì²­

    ì£¼ì–´ì§„ ROAS ë°ì´í„° ë° ì›ë¬¸ ë¦¬í¬íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì˜ 4ê°€ì§€ í•­ëª©ì— ëŒ€í•´ ê²Œì„ë³„ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜

    1. BEP ë‹¬ì„± ì—¬ë¶€ íŒë‹¨ ê°€ì¥ ìµœê·¼ì›”ì˜ ë³µê·€ìœ ì € í¬í•¨ D360 ROASì™€ ìˆ˜ìˆ˜ë£Œ ê³ ë ¤ BEP(bep_commission)ë¥¼ ë¹„êµí•˜ì—¬ ë‹¬ì„± ì—¬ë¶€ë¥¼ ëª…í™•íˆ íŒë‹¨í•´
    2. ì‹ ê·œ ìœ ì € ROAS BEP ì´ˆê³¼í•œ ìµœì†Œ Cohort ë¶„ì„: ê°€ì¥ ìµœê·¼ì›” ë³µê·€ìœ ì € ê³ ë ¤í•˜ì§€ ì•Šì€ ì‹ ê·œìœ ì € d360 ROAS(d360roas_growth)ê°€ BEPë¥¼ ì´ˆê³¼í•œ ê²½ìš°ì—ëŠ”d90roas_growthë¶€í„° d360roas_growthê¹Œì§€ì˜ cohort ì¤‘ BEP(bep_commission) ì´ìƒì¸ ìµœì†Œ cohort dnì„ ì–¸ê¸‰í•´ì¤˜. ë‹¨, d360roas_growthê°€ BEPë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ í•­ëª©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆ
    3. ì „ì›” ëŒ€ë¹„ ì¦ê°ë¥  ê³„ì‚°: ë¹„ìš©(cost_exclude_credit), ë‹¨ê°€(cpru), ë³µê·€ìœ ì € í¬í•¨ ROAS, ì‹ ê·œìœ ì € ROASì˜ ì „ì›” ëŒ€ë¹„ ì¦ê°ë¥ ì„ ê³„ì‚°í•´. ì „ì›” ìˆ˜ì¹˜ì™€ ë‹¹ì›” ìˆ˜ì¹˜ë„ í•¨ê»˜ í‘œê¸°í•˜ê³ , íŠ¹ì´ì ì„ ê°„ê²°í•˜ê²Œ ì–¸ê¸‰í•´
    4. ì£¼ìš” ì´ìŠˆ ì •ë¦¬: í¼í¬ë¨¼ìŠ¤íŒ€ì˜ ì£¼ê°„ ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ë‹¹ì›”ì˜ ì£¼ìš” ì´ìŠˆë‚˜ íˆìŠ¤í† ë¦¬ë¥¼ ì •ë¦¬í•´ì„œ ì¶”ê°€í•´ì¤˜

    ì‘ì„± ì‹œ ì•„ë˜ì˜ í˜•íƒœë¥¼ ì§€ì¼œì„œ ì‘ì„± ë¶€íƒí•´

    1. **BEP ë‹¬ì„± ì—¬ë¶€:** â€¦
    2. **ì‹ ê·œ ìœ ì € ROAS BEP ì´ˆê³¼ ì—¬ë¶€:** â€¦
    3. **ì „ì›” ëŒ€ë¹„ ì¦ê°ë¥ :**
        - ë¹„ìš©: â€¦
        - ë‹¨ê°€: â€¦
        - ë³µê·€ìœ ì € í¬í•¨ ROAS: â€¦
        - ì‹ ê·œìœ ì € ROAS: â€¦
        - íŠ¹ì´ì  : ...
    4. **ì£¼ìš” ì´ìŠˆ:** â€¦
    """]

    genai_client = GeminiClient(
        vertexai=True,
        location="global"      # genai í˜¸ì¶œìš©location ë³€ê²½
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response5 = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI ëª¨ë¸ëª…
        contents = prompt_parts_3_final
        ,config=config_3_optimized
    )
    print(response5.text)

    # Notion API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    notion = NotionClient(auth=NOTION_TOKEN, notion_version=NOTION_VERSION)

    title_prop: str = "ì´ë¦„"
    page_title = f"SLG ì›”ë³„ ë§ˆì¼€íŒ… í˜„í™© ë¦¬ë·°_{datetime.today().strftime('%y%m%d')}"
    project_list = ["GBTW","POTC","DRSG","WWM"]


    # DB ì†ì„± êµ¬ì„±
    props = {
        title_prop: {"title": [{"text": {"content": page_title}}]},
        "ë“±ë¡ ë‚ ì§œ": {"date": {"start": datetime.today().isoformat()}},

        # 'í”„ë¡œì íŠ¸' ì†ì„± (Rich Text ë˜ëŠ” Select)
        "í”„ë¡œì íŠ¸": {"multi_select": [{"name": project} for project in project_list  ] },
        "ë¦¬í¬íŠ¸ ì¢…ë¥˜": {"multi_select": [{"name": "ë§ˆì¼€íŒ…ë¶„ì„"}]}}
    #  if author_person_id:
    #      props["ì‘ì„±ì"] = {"people": [{"id": author_person_id}]}


    # if ref_person_id:
    #     props["ì°¸ì¡°ì"] = {"people": [{"id": ref_person_id}]}

    # ë‚˜ìœ¤ í…ŒìŠ¤íŠ¸ ë² ì´ìŠ¤
    NOTION_DATABASE_ID = "2ccea67a56818069b6abc52e5b5ca372"

    # í˜ì´ì§€ ìƒì„±
    new_page = notion.pages.create(
        parent={"database_id": NOTION_DATABASE_ID},
    #   properties=props
    )

    # ìƒì„±ëœ í˜ì´ì§€ ID ê°€ì ¸ì˜¤ê¸°
    PAGE_ID = new_page["id"]

    # ìƒì„±ëœ í˜ì´ì§€ URL ì¶œë ¥
    print("âœ… Notion í˜ì´ì§€ ìƒì„± ì™„ë£Œ:", new_page["url"])
    print("ğŸ†” ìƒì„±ëœ í˜ì´ì§€ ID:", PAGE_ID)


    ############## ì´ë¯¸ì§€ ì—…ë¡œë“œ##############

    import os, time, json, requests
    from pathlib import Path


    # ì—…ë¡œë“œìš© ê²½ë¡œ ë³€ìˆ˜
    IMG_PATH = Path("roas_graph.png").resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜(ê¶Œì¥)
    assert IMG_PATH.exists() and IMG_PATH.stat().st_size > 0

    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # 1) ì—…ë¡œë“œ ì˜¤ë¸Œì íŠ¸ ìƒì„± (upload_url ë°›ê¸°)
    resp = requests.post(
        "https://api.notion.com/v1/file_uploads",
        json={"filename": IMG_PATH.name, "content_type": "image/png"},
        headers=hdr_json
    )
    resp.raise_for_status()
    fu = resp.json()
    file_upload_id = fu["id"]
    upload_url = fu.get("upload_url")


    # 2) ì‹¤ì œ ì „ì†¡ (multipart/form-data) â‡’ status ê°€ uploaded ì—¬ì•¼ ì²¨ë¶€ ê°€ëŠ¥
    with open(IMG_PATH, "rb") as f:
        r2 = requests.post(
            # ê¶Œì¥: ëª…ì‹œì  send ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
            # ë˜ëŠ” upload_url ì‚¬ìš© ê°€ëŠ¥ (ë™ì¼ ë™ì‘): upload_url,
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
            files={"file": (IMG_PATH.name, f, "image/png")}
        )
    r2.raise_for_status()


    # (ì˜µì…˜) ìƒíƒœ í™•ì¸ ë° í´ë§: uploaded ë  ë•Œê¹Œì§€ ì ê¹ ëŒ€ê¸°
    for _ in range(10):
        r_chk = requests.get(
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}",
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
        )
        r_chk.raise_for_status()
        status = r_chk.json().get("status")
        if status == "uploaded":
            break
        time.sleep(0.4)
    assert status == "uploaded", f"ì—…ë¡œë“œ ìƒíƒœê°€ {status} ì…ë‹ˆë‹¤. (uploaded ì—¬ì•¼ ì²¨ë¶€ ê°€ëŠ¥)"

    ## ë³¸ë¬¸ ê¸°ì¡´
    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "1) ì „ì²´ ìœ ì € ROAS í˜„í™©" }}]
                },
            }
        ],
    )

    ########### (2) ê·¸ë˜í”„ ì²¨ë¶€
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    "caption": [
                        {"type": "text", "text": {"content": "ë‹¹ì›”ì˜ COSTëŠ” ì§‘ê³„ê¸°ê°„ë‚´ COST ê¸°ë°˜ ì¼í•  ê³„ì‚°ëœ ì¶”ì • ë‹¹ì›” ì†Œì§„ COSTì…ë‹ˆë‹¤."}}
                    ]
                },
            }
        ]
    )

    ########### (3) í‘œ ì²¨ë¶€
    # ì—…ë¡œë“œìš© ê²½ë¡œ
    IMG_PATHS = [
        Path("1.POTC_roas_table.png").resolve(),
        Path("2.GBTW_roas_table.png").resolve(),
        Path("3.WWMC_roas_table.png").resolve(),
        Path("4.DRSG_roas_table.png").resolve(),
    ]


    # ì—…ë¡œë“œ ì²´í¬ ë° í—¤ë”
    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì— ëŒ€í•œ ID ëª©ë¡
    file_upload_ids = []

    # 1) ê° ì´ë¯¸ì§€ íŒŒì¼ì— ëŒ€í•´ ì—…ë¡œë“œ ì²˜ë¦¬
    for img_path in IMG_PATHS:
        assert img_path.exists() and img_path.stat().st_size > 0  # íŒŒì¼ì´ ì¡´ì¬í•˜ê³  í¬ê¸°ê°€ 0ë³´ë‹¤ ì»¤ì•¼í•¨

        # ì—…ë¡œë“œ ì˜¤ë¸Œì íŠ¸ ìƒì„± (upload_url ë°›ê¸°)
        resp = requests.post(
            "https://api.notion.com/v1/file_uploads",
            json={"filename": img_path.name, "content_type": "image/png"},
            headers=hdr_json
        )
        resp.raise_for_status()
        fu = resp.json()
        file_upload_id = fu["id"]
        upload_url = fu.get("upload_url")

        # 2) ì‹¤ì œ íŒŒì¼ ì „ì†¡ (multipart/form-data)
        with open(img_path, "rb") as f:
            r2 = requests.post(
                f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
                headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
                files={"file": (img_path.name, f, "image/png")}
            )
        r2.raise_for_status()

        # (ì˜µì…˜) ìƒíƒœ í™•ì¸ ë° í´ë§: uploaded ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        for _ in range(10):
            r_chk = requests.get(
                f"https://api.notion.com/v1/file_uploads/{file_upload_id}",
                headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
            )
            r_chk.raise_for_status()
            status = r_chk.json().get("status")
            if status == "uploaded":
                break
            time.sleep(0.4)

        assert status == "uploaded", f"ì—…ë¡œë“œ ìƒíƒœê°€ {status} ì…ë‹ˆë‹¤. (uploaded ì—¬ì•¼ ì²¨ë¶€ ê°€ëŠ¥)"

        # ì—…ë¡œë“œëœ íŒŒì¼ ID ì €ì¥
        file_upload_ids.append(file_upload_id)

    # 3) í† ê¸€ ë¸”ë¡ ìƒì„± (ìƒì„¸ ROAS í‘œ)
    toggle_block = notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"type": "text", "text": {"content": "ìƒì„¸ ROAS í‘œ(í´ë¦­)"}}],
                    "children": []  # ì¼ë‹¨ ë¹„ì›Œë‘ 
                }
            }
        ]
    )

    toggle_id = toggle_block["results"][0]["id"]

    # 4) ì´ë¯¸ì§€ì™€ ë„í‘œ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
    for img_path, file_upload_id in zip(IMG_PATHS, file_upload_ids):
    # ì´ë¯¸ì§€ ì¶”ê°€ ë¡œì§
        notion.blocks.children.append(
            toggle_id,
            children=[
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        "caption": [
                            {
                                "type": "text",
                                "text": {
                                    "content": "ROAS í‘œ - ë‹¹ì›”ì˜ COSTëŠ” ì§‘ê³„ê¸°ê°„ë‚´ COST ê¸°ë°˜ ì¼í•  ê³„ì‚°ëœ ì¶”ì • ë‹¹ì›” ì†Œì§„ COSTì…ë‹ˆë‹¤."
                                }
                            }
                        ]
                    }
                }
            ]
        )

        # ì´ë¯¸ì§€íŒŒì¼ì˜ ì•ê¸€ì ê¸°ì¤€ Game ë§¤ì¹­
        game_key = img_path.name[:6]
        raw_df = raw_df_by_game[game_key]

        # Table RAW ì—…ë°ì´íŠ¸
        raw_toggle = notion.blocks.children.append(
            toggle_id,
            children=[
                {
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [
                            {"type": "text", "text": {"content": f"Table_RAW ({game_key})"}}
                        ],
                        "children": [
                            {
                                "object": "block",
                                "type": "table",
                                "table": {
                                    "table_width": len(raw_df.columns),
                                    "has_column_header": True,
                                    "has_row_header": False,
                                    "children": df_to_table_rows(raw_df, max_rows=100)
                                }
                            }
                        ]
                    }
                }
            ]
        )

    ########### (4) geminië¶„ì„ ë‚´ìš© ì²¨ë¶€
    ##(NYìˆ˜ì •) ë…¸ì…˜ì— ì—…ë°ì´íŠ¸ ë˜ëŠ” ê¸€ìê°€ 100ì¤„ì„ ë„˜ìœ¼ë©´ ì—ëŸ¬ê°€ ë°œìƒí•¨. ì´ë¶€ë¶„ ëŠì–´ì„œ ê°ˆ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë¡œì§ ì¶”ê°€í•¨

    # 1) Gemini ê²°ê³¼ â†’ Notion ë¸”ë¡ ë³€í™˜
    blocks = md_to_notion_blocks(response5.text + "\n\n\n")

    # 2) Notion APIì˜ children â‰¤ 100 ì œí•œ í•´ê²° â†’ 100ê°œì”© ë‚˜ëˆ  ë„£ê¸°
    def chunk_list(lst, size=100):
        for i in range(0, len(lst), size):
            yield lst[i:i+size]

    # 3) 100ê°œì”© append
    for chunk in chunk_list(blocks, 100):
        notion.blocks.children.append(
            block_id=PAGE_ID,
            children=chunk
        )

    print("âœ… Append ì™„ë£Œ")

    from datetime import datetime, timezone, timedelta
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_2_os_roas"} ## ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¶™ì¼ ìˆ˜ ìˆìŒ.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

    ### 1> os ë³„
    client = bigquery.Client()
    query = """
    WITH revraw AS(
    select  JoypleGameID, regmonth, osuser
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as regmonth
    ,  osuser ,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
        , case when OS = 'android' then 'And' when OS = 'ios' then 'IOS' else OS end as osuser
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
        and RegdateAuthAccountDateKST between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    )
    group by JoypleGameID, RegdateAuthAccountDateKST,  osuser
    ) group by JoypleGameID, regmonth,  osuser
    )




    , cost_raw AS(
    select joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) as regmonth   , os
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit,
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
    and cmpgndate between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    )
    group by  joyplegameid,gameid,  format_date('%Y-%m', cmpgndate)  , os
    )



    select
    ifnull(a.joyplegameid , b.joyplegameid) as joyplegameid
    ,ifnull(a.regmonth , b.regmonth) as regmonth
    , ifnull(a.osuser, b.os) as os
    , a.ru
    ,a.sales_D1, a.sales_D3, a.sales_D7, a.sales_D14, a.sales_D30, a.sales_D60, a.sales_D90 , a.sales_D120, a.sales_D150, a.sales_D180
    , a.sales_D210, a.sales_D240, a.sales_D270, a.sales_D300, a.sales_D330, a.sales_D360
    , b.cost, b.cost_exclude_credit
    from revraw as a
    full join cost_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.osuser = b.os

    """

    query_result_raw_os = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

    ############# OS ì „ì²˜ë¦¬
    # ROAS ê³„ì‚° (sales_d1 / cost, sales_d3 / cost, ..., sales_d360 / cost)
    cohort_columns = ['D1', 'D3', 'D7', 'D14', 'D30', 'D60', 'D90', 'D120', 'D150', 'D180', 'D210', 'D240', 'D270', 'D300', 'D330', 'D360']

    # ê° cohortì— ëŒ€í•´ ROAS ê³„ì‚°
    for cohort in cohort_columns:
        query_result_raw_os[f'{cohort.lower()}_roas'] = query_result_raw_os[f'sales_{cohort}'] / query_result_raw_os['cost_exclude_credit']


    query_result_raw_os['cpru'] = query_result_raw_os['cost_exclude_credit'] / query_result_raw_os['ru']

    # osë³„ cost ë¹„ìœ¨ ê³„ì‚°
    game_total_cost = query_result_raw_os.groupby(['joyplegameid', 'regmonth'])['cost_exclude_credit'].sum().reset_index(name='game_total_cost')

    # merge os_total_cost into query_result_raw2
    query_result_raw_os2 = pd.merge(query_result_raw_os, game_total_cost, on=['joyplegameid', 'regmonth'], how='left')

    query_result_raw_os2['os_cost_ratio'] = query_result_raw_os2['cost_exclude_credit'] / query_result_raw_os2['game_total_cost']

    mapping = {131:   "1.POTC", 133:   "2.GBTW", 30001: "3.WWMC", 30003: "4.DRSG",} # (NYìˆ˜ì •) ìœ ì‹¤ëœ mapping ë°ì´í„° ì¶”ê°€
    query_result_raw_os2["game_name"] = query_result_raw_os2["joyplegameid"].map(mapping)  # ë§¤í•‘ ì—†ìœ¼ë©´ NaN
    query_result_raw_os2 = query_result_raw_os2[query_result_raw_os2['os'].isin(['And', 'IOS'])]

    select_cols = ['game_name' ,'regmonth','os','cost_exclude_credit','os_cost_ratio','d1_roas','d3_roas','d7_roas','d14_roas','d30_roas','d60_roas','d90_roas','d120_roas','d150_roas']
    query_result_raw_os3 = query_result_raw_os2[select_cols]

    # í…Œì´ë¸” í˜•íƒœë¡œ ë¬¸ìì—´í™” (Markdown table)
    def df_to_md(df):
        header = "| " + " | ".join(df.columns) + " |"
        sep = "| " + " | ".join(["---"]*len(df.columns)) + " |"
        rows = "\n".join("| " + " | ".join(map(str, row)) + " |" for row in df.values)
        return "\n".join([header, sep, rows])

    df_os = df_to_md(query_result_raw_os3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(12))

    # ë°ì´í„° ì…ë ¥
    prompt_description = f"""
    ## ë°ì´í„° ì…ë ¥
    SLG ê²Œì„ 4ì¢…ì˜ ì›”ë³„ OSë³„ ROAS í˜„í™©ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë“¤ì´ì•¼.
    ê°’ì´ NAì¸ Cohortë³€ìˆ˜(dn_roas)ëŠ” ì•„ì§ matureë˜ì§€ ì•Šì€ ì§€í‘œì•¼.

    --- OSë³„ ROAS ë°ì´í„° í…Œì´ë¸” ---
    {df_os}

    """

    # Geminiì— ì „ë‹¬í•  ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_parts = [
        prompt_description,
        """
    ### OSë³„ ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± ìš”ì²­

    ì£¼ì–´ì§„ OSë³„ ROAS ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²Œì„ë³„ë¡œ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ì— ëŒ€í•´ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.

    1. **ì›”ë³„ OS íŠ¸ë Œë“œ:** ê²Œì„ë³„ë¡œ ìµœê·¼ 3ê°œì›”ì˜ ì›”ë³„ iOS, Androidì˜ Cost ë¹„ì¤‘, ROAS ì„±ê³¼ê°€ ì¦ê°€í•˜ëŠ”ì§€ ê°ì†Œí•˜ëŠ”ì§€ íŠ¸ë Œë“œë¥¼ ì–¸ê¸‰í•´ì¤˜.
    2. **OSë³„ ROAS ë¹„êµ (Cohort íŠ¸ë Œë“œ):** ì´ˆë°˜ Cohort (d7, d14 ë“±)ì™€ ì¥ê¸° Cohort (d90, d150 ë“±)ì˜ ROASë¥¼ ë¹„êµí•˜ì—¬ OS ê°„ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ (ì˜ˆ: 'ì´ˆë°˜ì€ Androidê°€ ë†’ìœ¼ë‚˜ ì¥ê¸° ì½”í˜¸íŠ¸ëŠ” iOSê°€ ë†’ë‹¤') ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜.
    3. **ê°€ì¥ ìµœê·¼ì›”(ë‹¹ì›”) í˜„í™©:**
        - ê°€ì¥ ìµœê·¼ì›”ì˜ OS Cost ë¹„ì¤‘ í˜„í™©ì„ ì–¸ê¸‰í•´ì¤˜.
        - ê°€ì¥ ìµœê·¼ì›”ì˜ OSë³„ ROASê°€ Android ëŒ€ë¹„ iOSê°€ ë†’ì€ì§€ ë‚®ì€ì§€ ëª…í™•íˆ ì–¸ê¸‰í•´ì¤˜.

    ì‘ì„± ì‹œ ì•„ë˜ì˜ ì˜ˆì‹œ ì¶œë ¥ í˜•íƒœë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„± ë¶€íƒí•´.

    --- ì˜ˆì‹œ ì¶œë ¥ í˜•íƒœ (ë§ˆí¬ë‹¤ìš´ í¬ë§·) ---

    ## ê²Œì„ëª… (ì˜ˆ: ## 1.POTC)

    1. **ì›”ë³„ OS íŠ¸ë Œë“œ:** iOS Cost ë¹„ì¤‘ì€ Xì›”ë¶€í„° Yì›”ê¹Œì§€ ğŸ”ºìƒìŠ¹(ë˜ëŠ” ğŸ”»í•˜ë½)í•˜ëŠ” ì¶”ì„¸ì´ë©°, iOS or Androidì˜ROAS ê°€ Xì›”ë¶€í„° ê°œì„ ë˜ëŠ” íë¦„ì„ ë³´ì„
    2. **OSë³„ ROAS Cohort íŠ¸ë Œë“œ:** ì´ˆë°˜ Cohort(d7~d30)ì˜ ROASëŠ” Androidê°€ iOS ëŒ€ë¹„ í‰ê· ì ìœ¼ë¡œ ë†’ìœ¼ë‚˜, ì¥ê¸° Cohort(d90~)ë¡œ ê°ˆìˆ˜ë¡ iOSê°€ Android ëŒ€ë¹„ ë†’ì€ ROASë¥¼ ë³´ì„.
    3. **ê°€ì¥ ìµœê·¼ì›” í˜„í™©:**
        - Cost ë¹„ì¤‘: ë‹¹ì›” iOS Cost ë¹„ì¤‘ì€ 00%ì„.
        - ROAS ë¹„êµ: ë‹¹ì›” ì´ˆë°˜ Cohort ROASëŠ” Android ëŒ€ë¹„ iOSê°€ ë‚®ê³ , ì¥ê¸° Cohort ROASëŠ” Android ëŒ€ë¹„ iOSê°€ ë†’ìŒ.

    """
    ]
    # Gemini API í˜¸ì¶œ (ì‚¬ìš©ìì˜ ì›ë˜ í˜¸ì¶œ ë°©ì‹)
    # response_os = model.generate_content(prompt_parts, labels=LABELS)
    # print(response_os.text)

    genai_client = GeminiClient(
        vertexai=True,
        location="global"      # genai í˜¸ì¶œìš©location ë³€ê²½
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response_os = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI ëª¨ë¸ëª…
        contents = prompt_parts
        ,config=config_3_optimized
    )
    print(response_os.text)

    pivot_df = query_result_raw_os3.pivot_table(
        index=['game_name', 'regmonth'] ,
        columns='os',
        values=['cost_exclude_credit', 'os_cost_ratio',  'd7_roas', 'd14_roas', 'd30_roas'],
        aggfunc='first'
    )

    '''
    pivot_df = pivot_df[['total_cost', 'os_cost_ratio', 'd7_roas', 'd14_roas', 'd30_roas']]

    nest_asyncio.apply()


    styled_df = pivot_df.style \
        .format({
            "total_cost": "{:,.0f}",  # total_costë¥¼ ì²œ ë‹¨ìœ„ë¡œ í¬ë§·
            **{col: "{:.1%}" for col in ['os_cost_ratio', 'd7_roas', 'd14_roas', 'd30_roas']}  # ë¹„ìœ¨ í¬ë§·
        }) \
        .bar(subset=['total_cost'], color='#fbe4e6') \
        .bar(subset=['d7_roas', 'd14_roas', 'd30_roas'], color='#b6d7a8') \
        .background_gradient(subset=['os_cost_ratio'], cmap='Reds')  # os_cost_ratioì— ë¹¨ê°„ìƒ‰ ê·¸ë¼ë°ì´ì…˜
    '''

    nest_asyncio.apply()

    # ì»¬ëŸ¼ ì´ë¦„ì„ ë‹¨ì¼ ì¸ë±ìŠ¤ë¡œ í‰íƒ„í™”
    pivot_df.columns = [f'{col}_{idx}' for col, idx in pivot_df.columns]

    pivot_df = pivot_df[['cost_exclude_credit_And', 'cost_exclude_credit_IOS', 'os_cost_ratio_IOS'
    ,'d7_roas_And', 'd7_roas_IOS', 'd14_roas_And', 'd14_roas_IOS', 'd30_roas_And', 'd30_roas_IOS']]


    '''
    # ê° ì›”ë³„ë¡œ And vs iOS ì¤‘ ë” í° ê°’ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ
    def highlight_max(s):
        is_max = s == s.max()
        return ['color: red; font-weight: bold' if v else '' for v in is_max]



    # formatê³¼ ìŠ¤íƒ€ì¼ ì ìš©
    styled_df = pivot_df.style \
        .format({
            "cost_exclude_credit_And": "{:,.0f}",  # total_costë¥¼ ì²œ ë‹¨ìœ„ë¡œ í¬ë§·
                    "cost_exclude_credit_IOS": "{:,.0f}",  # total_costë¥¼ ì²œ ë‹¨ìœ„ë¡œ í¬ë§·

            **{col: "{:.1%}" for col in ['os_cost_ratio_IOS', 'd7_roas_And', 'd14_roas_And', 'd30_roas_And', 'd7_roas_IOS', 'd14_roas_IOS', 'd30_roas_IOS']}  # ë¹„ìœ¨ í¬ë§·
        }) \
        .bar(subset=['total_cost_And','total_cost_IOS'], color='#fbe4e6') \
        .bar(subset=['d7_roas_And', 'd14_roas_And', 'd30_roas_And', 'd7_roas_IOS', 'd14_roas_IOS', 'd30_roas_IOS'], color='#b6d7a8') \
        .apply(highlight_max, subset=["d7_roas_And","d7_roas_IOS"], axis=1)
        .apply(highlight_max, subset=["d14_roas_And","d14_roas_IOS"], axis=1)
        .apply(highlight_max, subset=["d30_roas_And","d30_roas_IOS"], axis=1)
        .background_gradient(subset=['os_cost_ratio_IOS'], cmap='Reds')
    '''

    def highlight_max_bg(s):
        vmax = s.max(skipna=True)
        return [
            'background-color: pink' if (not pd.isna(v) and v == vmax) else ''
            for v in s
        ]



    styled_df = (
        pivot_df
        .style
        .format({
            "cost_exclude_credit_And": "{:,.0f}",
            "cost_exclude_credit_IOS": "{:,.0f}",
            **{col: "{:.1%}" for col in [
                "os_cost_ratio_IOS",
                "d7_roas_And","d14_roas_And","d30_roas_And",
                "d7_roas_IOS","d14_roas_IOS","d30_roas_IOS"
            ]}
        })
        .bar(subset=['cost_exclude_credit_And', 'cost_exclude_credit_IOS'], color="#fbe4e6")
        .bar(subset=[
            "d7_roas_And","d14_roas_And","d30_roas_And",
            "d7_roas_IOS","d14_roas_IOS","d30_roas_IOS"
        ], color="#b6d7a8")
        .apply(highlight_max_bg, subset=["d7_roas_And","d7_roas_IOS"], axis=1)
        .apply(highlight_max_bg, subset=["d14_roas_And","d14_roas_IOS"], axis=1)
        .apply(highlight_max_bg, subset=["d30_roas_And","d30_roas_IOS"], axis=1)
        .background_gradient(subset=["os_cost_ratio_IOS"], cmap="Reds")
    )



    nest_asyncio.apply()

    # HTML í…œí”Œë¦¿ ì •ì˜
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            table { border-collapse: collapse; font-size: 13px; }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: right;
            }
            th { background-color: #f0f0f0; }
        </style>
    </head>
    <body>
        <h2>OSë³„ í˜„í™©</h2>
        {{ table | safe }}
    </body>
    </html>
    """

    # HTML ë Œë”ë§ ë° ì €ì¥
    table_html = styled_df.to_html()
    rendered_html = Template(html_template).render(table=table_html)

    html_path = "os_roas.html"
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(rendered_html)

    # ì´ë¯¸ì§€ ìº¡ì²˜ ë¹„ë™ê¸° í•¨ìˆ˜
    async def capture_html_to_image():
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={"width": 1600, "height": 1000})
            await page.goto("file://" + os.path.abspath(html_path))
            await page.screenshot(path="os_roas.png", full_page=True)
            await browser.close()

    # Spyder or GCP Notebook í™˜ê²½ ëŒ€ì‘
    asyncio.get_event_loop().run_until_complete(capture_html_to_image())


    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "2) OSë³„ ROAS í˜„í™©" }}]
                },
            }
        ],
    )



    ########### (2) OS ì„œì‹í‘œ ì—…ë¡œë“œ

    # ì—…ë¡œë“œìš© ê²½ë¡œ ë³€ìˆ˜
    from pathlib import Path
    IMG_PATH = Path("os_roas.png").resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜(ê¶Œì¥)
    assert IMG_PATH.exists() and IMG_PATH.stat().st_size > 0

    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # 1) ì—…ë¡œë“œ ì˜¤ë¸Œì íŠ¸ ìƒì„± (upload_url ë°›ê¸°)
    resp = requests.post(
        "https://api.notion.com/v1/file_uploads",
        json={"filename": IMG_PATH.name, "content_type": "image/png"},
        headers=hdr_json
    )
    resp.raise_for_status()
    fu = resp.json()
    file_upload_id = fu["id"]
    upload_url = fu.get("upload_url")


    # 2) ì‹¤ì œ ì „ì†¡ (multipart/form-data) â‡’ status ê°€ uploaded ì—¬ì•¼ ì²¨ë¶€ ê°€ëŠ¥
    with open(IMG_PATH, "rb") as f:
        r2 = requests.post(
            # ê¶Œì¥: ëª…ì‹œì  send ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
            # ë˜ëŠ” upload_url ì‚¬ìš© ê°€ëŠ¥ (ë™ì¼ ë™ì‘): upload_url,
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
            files={"file": (IMG_PATH.name, f, "image/png")}
        )
    r2.raise_for_status()




    # í‘œ ì²¨ë¶€
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    #"caption": [
                    #    {"type": "text", "text": {"content": "ROAS & Cost (auto-upload)"}}
                    #]
                },
            }
        ]
    )




    ########### (3) geminië¶„ì„ ë‚´ìš© ì²¨ë¶€

    blocks = md_to_notion_blocks(response_os.text + "\n\n\n")

    notion.blocks.children.append(
        block_id=PAGE_ID,
        children=blocks
    )

    from datetime import datetime, timezone, timedelta
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")

    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_3_geo_roas"} ## ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¶™ì¼ ìˆ˜ ìˆìŒ.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

    # 2> êµ­ê°€ë³„
    client = bigquery.Client()
    query = """
    WITH revraw AS(
    select  JoypleGameID, regmonth, geo_user_group
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as regmonth
    , geo_user_group,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
        , case when OS = 'android' then 'And' when OS = 'ios' then 'IOS' else OS end as osuser
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
        and RegdateAuthAccountDateKST between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    )
    group by JoypleGameID, RegdateAuthAccountDateKST, geo_user_group
    ) group by JoypleGameID, regmonth, geo_user_group
    )




    , cost_raw AS(
    select joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) as regmonth   , geo_user_group
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit,
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
    and cmpgndate between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    )
    group by  joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) , geo_user_group
    )



    select
    ifnull(a.joyplegameid , b.joyplegameid) as joyplegameid
    ,ifnull(a.regmonth , b.regmonth) as regmonth
    , ifnull(a.geo_user_group, b.geo_user_group) as geo_user_group
    , a.ru
    ,a.sales_D1, a.sales_D3, a.sales_D7, a.sales_D14, a.sales_D30, a.sales_D60, a.sales_D90 , a.sales_D120, a.sales_D150, a.sales_D180
    , a.sales_D210, a.sales_D240, a.sales_D270, a.sales_D300, a.sales_D330, a.sales_D360
    , b.cost, b.cost_exclude_credit
    from revraw as a
    full join cost_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.geo_user_group = b.geo_user_group
    """

    query_result_raw_geo = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

    ########## êµ­ê°€ë³„ ì „ì²˜ë¦¬
    # ê° cohortì— ëŒ€í•´ ROAS ê³„ì‚°
    for cohort in cohort_columns:
        query_result_raw_geo[f'{cohort.lower()}_roas'] = query_result_raw_geo[f'sales_{cohort}'] / query_result_raw_geo['cost_exclude_credit']


    query_result_raw_geo['cpru'] = query_result_raw_geo['cost_exclude_credit'] / query_result_raw_geo['ru']

    query_result_raw_geo2 = pd.merge(query_result_raw_geo, game_total_cost, on=['joyplegameid', 'regmonth'], how='left')
    query_result_raw_geo2['geo_cost_ratio'] = query_result_raw_geo2['cost_exclude_credit'] / query_result_raw_geo2['game_total_cost']



    query_result_raw_geo2["game_name"] = query_result_raw_geo2["joyplegameid"].map(mapping)  # ë§¤í•‘ ì—†ìœ¼ë©´ NaN

    select_cols = ['game_name' ,'regmonth','geo_user_group','cost_exclude_credit','geo_cost_ratio','d1_roas','d3_roas','d7_roas','d14_roas','d30_roas','d60_roas','d90_roas','d120_roas','d150_roas']
    query_result_raw_geo3 = query_result_raw_geo2[select_cols]

    df_geo = df_to_md(query_result_raw_geo3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(30))
    #### gemini ë¶„ì„ ë°ì´í„° í•„í„°
    # ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ ì¶”ì¶œ

    recent_data2 = recent_data[[ "game_name","regmonth", "cost_exclude_credit","cpru" ,"d360roas_growth","month_start"]]


    # í…Œì´ë¸”ì— ëŒ€í•œ ì„¤ëª…

    # ë°ì´í„° í•™ìŠµ ë° ì •ì˜
    prompt_description = f"""
    SLG ê²Œì„ 4ì¢…ì˜ ì›”ë³„, OSë³„, ê¶Œì—­ë³„ ROAS í˜„í™©ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë“¤ì´ì•¼.
    ê°’ì´ NAì¸ Cohortë³€ìˆ˜(dn_roas)ëŠ” ì•„ì§ matureë˜ì§€ ì•Šì€ ì§€í‘œì•¼.

    [ì£¼ìš” ë³€ìˆ˜ ì„¤ëª…]
    - cost_exclude_credit: í¬ë ˆë”§ ì œì™¸ ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ (roas ê³„ì‚° ê¸°ì¤€, ë‹¹ì›”ì€ ì¼í• ê³„ì‚° ì¶”ì • ì›” cost)
    - cpru: ë‹¨ê°€
    - d360roas_growth: ë³µê·€ìœ ì € ë¯¸í¬í•¨ ì‹ ê·œìœ ì € d360 ì˜ˆì¸¡ì¹˜
    - month_start : ê¸°ì¤€ì´ ë˜ëŠ” ì›”

    ### ë°ì´í„°ì…‹
    1. ì „ì²´ ìœ ì € ê¸°ì¤€ ë°ì´í„°:
    {recent_data2}

    2. df_geo (êµ­ê°€ë³„):
    {df_geo}

    3. df_os (OSë³„):
    {df_os}
    """


    # 3. Geminiì— ì „ë‹¬í•  ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_parts = [
        prompt_description,

    """
    ### ë§ˆì¼€íŒ… ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± ìš”ì²­

    ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ì— ëŒ€í•´ ê²Œì„ë³„ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜

    1. **ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„:**
        -  `month_start` ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 3ê°œì›” ë°ì´í„°ì˜ íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜.
        - `d360roas_growth` ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ **ìµœê·¼ 3ê°œì›”ê°„**ì˜ **ì „ì›” ëŒ€ë¹„ ë‹¹ì›” ROAS ë³€í™”** ì¶”ì´ë¥¼ ìš”ì•½
        - `recent_data2`ì˜ **cost** ë° **cpru** ì§€í‘œë¥¼ ì°¸ê³ í•˜ì—¬ ìµœê·¼ 3ê°œì›”ê°„ì˜ **Costì™€ ë‹¨ê°€(CPRU) íŠ¸ë Œë“œ**ê°€ ì¦ê°€í•˜ëŠ”ì§€ ê°ì†Œí•˜ëŠ”ì§€ ì–¸ê¸‰í•´ì¤˜

    2. **ì›ì¸ ë¶„ì„ (êµ­ê°€/OS):**
        - `df_geo`ì™€ `df_os`ë¥¼ ì°¸ê³ í•˜ì—¬ **ê°€ì¥ ìµœê·¼ ì›”ì˜ ROAS ë³€ë™ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤€ ìš”ì¸** (êµ­ê°€ ë˜ëŠ” OS)ì„ ì°¾ê³ , í•´ë‹¹ ìš”ì¸ì˜ **ì›”ë³„ ì„±ê³¼**ë¥¼ ë¹„êµí•´ì¤˜
        - **ë¹„êµëŠ” ë°˜ë“œì‹œ ë™ì¼í•œ Cohort(dN)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì›”ë³„**ë¡œ ì§„í–‰í•´

    3. **ì•¡ì…˜ ì•„ì´í…œ ì œì•ˆ:**
        - ë¶„ì„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ **ì•¡ì…˜ ì•„ì´í…œ**ì„ ì œì•ˆí•´ì¤˜
        - ì œì•ˆì€ íŠ¹ì • ê¶Œì—­ì˜ íŠ¹ì • ê¸°ê°„ ì„±ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ **ì˜ˆì‚° ì¦/ê°ì†Œ** ë˜ëŠ” **íŠ¹ì • OS ì˜ˆì‚° ì¦/ê°ì†Œ** ë“±.

    ì‘ì„± ì‹œ ì•„ë˜ì˜ í˜•íƒœë¥¼ ì§€ì¼œì„œ ë§ˆí¬ë‹¤ìš´í˜•íƒœë¡œ ì‘ì„± ë¶€íƒí•´

    ## ê²Œì„ëª… (ì˜ˆ: ## 1.POTC)

    1. **ì „ì²´ íŠ¸ë Œë“œ:** ...
    2. **ì›ì¸ ë¶„ì„:**
        - êµ­ê°€ë³„ ë¶„ì„:...
        - OSë³„ ë¶„ì„: ...
    3. **ì œì•ˆ: ...
    """
    ]

    genai_client = GeminiClient(
        vertexai=True,
        location="global"      # genai í˜¸ì¶œìš©location ë³€ê²½
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response_total = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI ëª¨ë¸ëª…
        contents = prompt_parts
        ,config=config_3_optimized
    )
    print(response_total.text)

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "3) ì¢…í•© ê²°ë¡ " }}]
                },
            }
        ],
    )


    ## ì¢…í•© í•´ì„
    blocks = md_to_notion_blocks(response_total.text + "\n\n\n")
    notion.blocks.children.append(
        block_id=PAGE_ID,
        children=blocks
    )

    print("âœ… Append ì™„ë£Œ")

    if ref_person_ids:
        props_update = {
            "ì°¸ì¡°ì": {"people": [{"id": pid} for pid in ref_person_ids]}
        }
        updated_page = notion.pages.update(
            page_id=PAGE_ID,
            properties=props_update
        )
        print("âœ… Notion í˜ì´ì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì°¸ì¡°ì ì¶”ê°€):", updated_page["url"])


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=15),
}

with DAG(
    dag_id='Marketing_Monthly_Report',
    default_args=default_args,
    description='ì›”ê°„ ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸ ìƒì„± (notion + gemini)',
    schedule='10 5 * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['marketing', 'report', 'monthly'],
) as dag:
    
    # Task ì •ì˜
    task = PythonOperator(
        task_id='mkt_monthly_report_total',
        python_callable=mkt_monthly_report_total,
        dag=dag,
    )