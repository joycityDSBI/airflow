<<<<<<< HEAD
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator

=======
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
# Ìè∞Ìä∏ Ï∫êÏãú Ïû¨Íµ¨Ï∂ï
import matplotlib.font_manager as fm

# ÎùºÏù¥Î∏åÎü¨Î¶¨ import
import os
import re
<<<<<<< HEAD
import random
import logging
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
=======
import json
import time
import hashlib
import math
import random
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from pathlib import Path
from datetime import datetime, timedelta, timezone
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î∞è ÏãúÍ∞ÅÌôî
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from matplotlib.ticker import PercentFormatter
<<<<<<< HEAD
# import dataframe_image as dfi
# from PIL import Image
=======
import dataframe_image as dfi
from PIL import Image
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

# Google Cloud Í¥ÄÎ†®
from google.cloud import bigquery
from google.cloud import aiplatform
import pandas_gbq

# Gemini AI Í¥ÄÎ†®
<<<<<<< HEAD
# import vertexai
# from vertexai.generative_models import GenerativeModel
# import google.generativeai as genai
# from google.generativeai import GenerativeModel as GeminiModel # Ïù¥Î¶Ñ Ï∂©Îèå Î∞©ÏßÄÎ•º ÏúÑÌï¥ Î≥ÑÏπ≠ ÏÇ¨Ïö©
=======
import vertexai
from vertexai.generative_models import GenerativeModel
import google.generativeai as genai
from google.generativeai import GenerativeModel as GeminiModel # Ïù¥Î¶Ñ Ï∂©Îèå Î∞©ÏßÄÎ•º ÏúÑÌï¥ Î≥ÑÏπ≠ ÏÇ¨Ïö©
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

#Gemini 3.0 Í¥ÄÎ†®
# !pip install --upgrade google-genai
from google.genai import Client as GeminiClient
from google.genai.types import GenerateContentConfig
from google.genai import types

# Notion API
from notion_client import Client as NotionClient

# Ïõπ Í¥ÄÎ†® (HTML, CSS Î†åÎçîÎßÅ Îì±)
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio

<<<<<<< HEAD
=======
#airflow
from airflow.models import Variable
from airflow import DAG
from airflow.operators.python import PythonOperator


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
# IPython ÎîîÏä§ÌîåÎ†àÏù¥
from IPython.display import display

def get_var(key: str, default: str = None) -> str:
    """ÌôòÍ≤Ω Î≥ÄÏàò ÎòêÎäî Airflow Variable Ï°∞Ìöå"""
    return os.environ.get(key) or Variable.get(key, default_var=default)

# gemini ÏÑ§Ï†ï
os.environ['GOOGLE_CLOUD_PROJECT'] = 'data-science-division-216308'
os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'  #global

<<<<<<< HEAD
# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï Ìï®Ïàò
def set_korean_font():
    """OSÏóê Îî∞Î•∏ ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï"""
    candidates = ["Noto Sans CJK KR", "NanumGothic", "Malgun Gothic", "AppleGothic", "DejaVu Sans"]
    
    # 1. ÏãúÏä§ÌÖúÏóê ÏÑ§ÏπòÎêú Ìè∞Ìä∏ Ï§ë ÌõÑÎ≥¥Íµ∞Ïù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
    system_fonts = {f.name for f in fm.fontManager.ttflist}
    
    selected_font = None
    for font in candidates:
        if font in system_fonts:
            selected_font = font
            break
            
    if selected_font:
        mpl.rcParams["font.family"] = selected_font
    else:
        # 2. Î¶¨ÎàÖÏä§ ÌôòÍ≤Ω Îì±ÏóêÏÑú ÌäπÏ†ï Í≤ΩÎ°úÏùò Ìè∞Ìä∏ ÌååÏùº ÏãúÎèÑ (Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ Î∞è Í∞úÏÑ†)
        linux_font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
        if Path(linux_font_path).exists():
            fm.fontManager.addfont(linux_font_path)
            mpl.rcParams["font.family"] = 'NanumGothic'
        else:
            print("‚ö†Ô∏è ÌïúÍ∏Ä Ìè∞Ìä∏Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§. Í∏∞Î≥∏ Ìè∞Ìä∏Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.")

    mpl.rcParams["axes.unicode_minus"] = False  # ÎßàÏù¥ÎÑàÏä§ Íπ®Ïßê Î∞©ÏßÄ

set_korean_font()


NOTION_TOKEN = get_var("NOTION_TOKEN_MS")  # Airflow VariableÏóê Ï†ÄÏû•Îêú Notion ÌÜµÌï© ÌÜ†ÌÅ∞
NOTION_VERSION = "2022-06-28"

=======
# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏßÄÏ†ï: Î®ºÏ†Ä ÏÑ§ÏπòÎêú Í≤ÉÏùÑ Ïö∞ÏÑ†ÏúºÎ°ú, ÏóÜÏúºÎ©¥ Îã§Ïùå ÌõÑÎ≥¥Î°ú Ìè¥Î∞±
mpl.rcParams["font.family"] = ["Noto Sans CJK KR", "NanumGothic", "DejaVu Sans"]
mpl.rcParams["axes.unicode_minus"] = False  # ÎßàÏù¥ÎÑàÏä§ Íπ®Ïßê Î∞©ÏßÄ


names = sorted({f.name for f in fm.fontManager.ttflist})
[k for k in names if "Noto" in k or "Nanum" in k][:50]
names


# üîë ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï

NOTION_TOKEN = get_var("NOTION_TOKEN_MS")
NOTION_VERSION = "2022-06-28"

### beta test
#NOTION_PAGE_ID = "279ea67a568180b2878aebc4db00506e" # Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏûëÏÑ±Ìï† Notion ÌéòÏù¥ÏßÄÏùò ID
#NOTION_DATABASE_ID = "254ea67a568180d4bfd2d698ec1a1225"
#author_person_id = 'a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae'  #ÏûëÏÑ±Ïûê
#ref_person_id= 'a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae'  #Ï∞∏Ï°∞Ïûê

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
### beta released
NOTION_PAGE_ID = "24cea67a56818059a90aee3f616bc263" # Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏûëÏÑ±Ìï† Notion ÌéòÏù¥ÏßÄÏùò ID
NOTION_DATABASE_ID = "279ea67a5681807fb943e9894bad5c57"
author_person_id = 'a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae'  #ÏûëÏÑ±Ïûê
<<<<<<< HEAD
=======
# #ref_person_ids= ['a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae'  #Ï∞∏Ï°∞Ïûê(ÏñëÏ£ºÏó∞)
# #, 'a1a4ce7f-cf37-40b2-a1ef-8f00877e76ae' ]
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

ref_person_ids= [ 'ebd0514a-939d-4c80-bb34-f1413478d9d9',  #Ïò§ÏπòÏÑ±
                  '7b68811e-e587-45a1-8ad2-940c87dadf9a',  #Ïù¥ÌïúÎÇòÎ¶¨
                 '5e777130-5039-4f71-9ac7-64645f674737' , #Î∞ïÏ§ÄÏäπ
                 '262e5f51-9d68-4444-9713-5f1506b3eead' , #Ïù¥Î≥ëÏÑ†
                 '23c62fe1-a573-4b3f-b12c-c7df7dbe8c9b' , #ÏßÑÏ†ïÏôÑ
                 'ae87a94b-cf69-41fd-ae37-b0385b4e4bdf' , #Î∞ïÎØºÏû¨
                  '299d872b-594c-8174-9e5a-00028da23485', # ÍπÄÎèÑÏòÅ
                  '645651d3-c051-40ae-b551-b0c4ef4b49f1', #Í≥ÑÎèôÍ∑†
<<<<<<< HEAD
                  '096802f3-3ae8-4e2d-bc06-911d6dc4052c', #Ïã†Ï†ïÏóΩ
                 '8658b12e-cf6b-4247-abc2-c346381951ad'  #Ï†ÑÏûêÎûå
]

=======
                  '096802f3-3ae8-4e2d-bc06-911d6dc4052c' , #Ïã†Ï†ïÏóΩ
                 '8658b12e-cf6b-4247-abc2-c346381951ad'  #Ï†ÑÏûêÎûå
]


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
## ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏùÑ ÎÖ∏ÏÖòÏóê Í∑∏ÎåÄÎ°ú Ï†ÅÏö©ÏãúÏºúÏ£ºÎäî Ìï®Ïàò
# üëâ Markdown ÎÇ¥ **ÍµµÍ≤å** Ï≤òÎ¶¨ Î≥ÄÌôò
def parse_rich_text(md_text):
    """
    '**ÍµµÍ≤å**' ‚Üí Notion rich_text [{"text": {...}, "annotations": {"bold": True}}]
    """
    parts = re.split(r"(\*\*.*?\*\*)", md_text)  # **...** Í∏∞Ï§Ä split
    rich_text = []
    for part in parts:
        if part.startswith("**") and part.endswith("**"):
            rich_text.append({
                "type": "text",
                "text": {"content": part[2:-2]},
                "annotations": {"bold": True}
            })
        else:
            if part:
                rich_text.append({
                    "type": "text",
                    "text": {"content": part}
                })
    return rich_text

<<<<<<< HEAD
=======


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
# üëâ MarkdownÏùÑ Notion BlocksÎ°ú Î≥ÄÌôò
def md_to_notion_blocks(md_text, blank_blocks=3):
    blocks = []
    lines = md_text.splitlines()
    stack = [blocks]  # ÌòÑÏû¨ Í≥ÑÏ∏µ Ï∂îÏ†Å

    def detect_indent_unit(lines):
        indents = []
        for line in lines:
            if line.lstrip().startswith(("* ", "- ", "+ ")):  # Î¶¨Ïä§Ìä∏ Î¨∏Î≤ï Í∞êÏßÄ
                indent = len(line) - len(line.lstrip())
                if indent > 0:
                    indents.append(indent)
        return min(indents) if indents else 4  # fallback = 4Ïπ∏
    indent_unit = detect_indent_unit(lines)

    i = 0
    while i < len(lines):
        line = lines[i].rstrip()
        if not line:
            i += 1
            continue

        # Heading Ï≤òÎ¶¨
        if line.startswith("# "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": parse_rich_text(line[2:])}
            })
        elif line.startswith("## "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": parse_rich_text(line[3:])}
            })
        elif line.startswith("### "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": parse_rich_text(line[4:])}
            })

        # Î¶¨Ïä§Ìä∏ Ï≤òÎ¶¨
        elif line.lstrip().startswith(("* ", "- ", "+ ")):
            indent = len(line) - len(line.lstrip())  # Îì§Ïó¨Ïì∞Í∏∞ Î†àÎ≤®
            content = line.strip()[2:].strip()

            # Îã§Ïùå Ï§ÑÏù¥ Îì§Ïó¨Ïì∞Í∏∞Í∞Ä Îçî ÍπäÏùÄÏßÄ ÌôïÏù∏ÌïòÏó¨ ÏûêÏãù Î∏îÎ°ùÏù¥ ÏûàÎäîÏßÄ ÌåêÎã®
            has_children = False
            if i + 1 < len(lines):
                next_line = lines[i+1]
                next_indent = len(next_line) - len(next_line.lstrip())
                if next_indent > indent:
                    has_children = True

            block_data = {
                "rich_text": parse_rich_text(content),
            }
            # ÏûêÏãù Î∏îÎ°ùÏù¥ ÏûàÏùÑ Í≤ΩÏö∞ÏóêÎßå 'children' ÌÇ§Î•º Ï∂îÍ∞Ä
            if has_children:
                block_data["children"] = []

            block = {
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": block_data
            }

            # indent Í∏∞Î∞ò Í≥ÑÏ∏µ Ï≤òÎ¶¨
            level = indent // indent_unit + 1
            while len(stack) > level:
                stack.pop()
            stack[-1].append(block)

            # ÏûêÏãù Î∏îÎ°ùÏù¥ ÏûàÎäî Í≤ΩÏö∞ÏóêÎßå Ïä§ÌÉùÏóê ÏûêÏãù Î™©Î°ùÏùÑ Ï∂îÍ∞Ä
            if has_children:
                stack.append(block["bulleted_list_item"]["children"])
            else:
                # ÏûêÏãù Î∏îÎ°ùÏù¥ ÏóÜÎäî Í≤ΩÏö∞ ÌòÑÏû¨ Î†àÎ≤®Î°ú Ïä§ÌÉù Ïû¨ÏÑ§Ï†ï
                stack = stack[:level]

        else:
            stack = [blocks]
            # ÏùºÎ∞ò Î¨∏Îã®
            stack[-1].append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": parse_rich_text(line.strip())}
            })

        i += 1

    # ‚úÖ ÎßàÏßÄÎßâÏóê Îπà Î∏îÎ°ù Ï∂îÍ∞Ä (Í∞úÏàòÎäî ÌååÎùºÎØ∏ÌÑ∞ blank_blocksÎ°ú Ï†úÏñ¥)
    for _ in range(blank_blocks):
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": []}
        })

    return blocks

<<<<<<< HEAD
def mkt_monthly_report_total(**kwargs):
    from datetime import datetime, timezone, timedelta
    
    # Airflow ContextÏóêÏÑú Ïã§Ìñâ ÎÇ†Ïßú Í∞ÄÏ†∏Ïò§Í∏∞ (ÏóÜÏúºÎ©¥ ÌòÑÏû¨ ÏãúÍ∞Ñ)
    execution_date = kwargs.get('execution_date')
    if execution_date:
        # pendulum Í∞ùÏ≤¥Î•º datetimeÏúºÎ°ú Î≥ÄÌôò (KSTÎ°ú Î≥ÄÌôò)
        now_kst = execution_date.in_timezone(ZoneInfo("Asia/Seoul"))
    else:
        now_kst = datetime.now(ZoneInfo("Asia/Seoul"))

    RUN_ID = now_kst.strftime("%Y%m%d")
=======

# üëâ PostgreSQLÏóêÏÑú ÎßàÏºÄÌåÖ ÏõîÍ∞Ñ Î¶¨Ìè¨Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Î∞è NotionÏóê ÏûëÏÑ±

def marketing_monthly_report_to_notion(**kwargs):
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_1_total_roas"} ## ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î∂ôÏùº Ïàò ÏûàÏùå.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

<<<<<<< HEAD
    client = bigquery.Client()
    query = f"""
=======
    ## 1> cohortÎ≥Ñ Ï†ÑÏ≤¥ ÏßÄÌëú
    client = bigquery.Client()
    query = """

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    WITH revraw AS(
    select  JoypleGameID, Month
    ,sum(RU) as RU,#
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as Month,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
<<<<<<< HEAD
    and (JoypleGameID = 131 AND RegdateAuthAccountDateKST BETWEEN '{now_kst.year - 3}-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 133 AND RegdateAuthAccountDateKST BETWEEN '{now_kst.year - 3}-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30001 AND RegdateAuthAccountDateKST BETWEEN '{now_kst.year - 2}-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30003 AND RegdateAuthAccountDateKST BETWEEN '{now_kst.year - 1}-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
=======
    and (JoypleGameID = 131 AND RegdateAuthAccountDateKST BETWEEN '2021-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 133 AND RegdateAuthAccountDateKST BETWEEN '2021-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30001 AND RegdateAuthAccountDateKST BETWEEN '2022-05-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
    OR (JoypleGameID = 30003 AND RegdateAuthAccountDateKST BETWEEN '2024-01-01' AND DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY))
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    group by JoypleGameID, RegdateAuthAccountDateKST
    ) group by JoypleGameID, month
    )


    , final AS(
    select  JoypleGameID,  Month, RU,
    Sales_D1/RU as D1_LTV,
    Sales_D3/RU as D3_LTV,
    Sales_D7/RU as D7_LTV,
    Sales_D14/RU as D14_LTV,
    Sales_D30/RU as D30_LTV,
    Sales_D60/RU as D60_LTV,
    Sales_D90/RU as D90_LTV,
    Sales_D120/RU as D120_LTV,
    Sales_D150/RU as D150_LTV,
    Sales_D180/RU as D180_LTV,
    Sales_D210/RU as D210_LTV,
    Sales_D240/RU as D240_LTV,
    Sales_D270/RU as D270_LTV,
    Sales_D300/RU as D300_LTV,
    Sales_D330/RU as D330_LTV,
    Sales_D360/RU as D360_LTV,
    Sales_D14_p/RU as D14_LTV_p,
    Sales_D30_p/RU as D30_LTV_p,
    Sales_D60_p/RU as D60_LTV_p,
    Sales_D90_p/RU as D90_LTV_p,
    Sales_D120_p/RU as D120_LTV_p,
    Sales_D150_p/RU as D150_LTV_p,
    Sales_D180_p/RU as D180_LTV_p,
    Sales_D210_p/RU as D210_LTV_p,
    Sales_D240_p/RU as D240_LTV_p,
    Sales_D270_p/RU as D270_LTV_p,
    Sales_D300_p/RU as D300_LTV_p,
    Sales_D330_p/RU as D330_LTV_p,
    Sales_D360_p/RU as D360_LTV_p,
    D1D3_avg,  D3D7_avg , Sales_D7/RU as kpi_d7
    from(
    select *,
    CASE WHEN Sales_D14 is not null then null  ELSE  Sales_D7*D7D14_avg END as Sales_D14_p,
    CASE WHEN Sales_D30 is not null then null
    WHEN Sales_D30 is null and Sales_D14 is not null THEN Sales_D14*D14D30_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg END as Sales_D30_p,
    CASE WHEN Sales_D60 is not null then null
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg END as Sales_D60_p,
    CASE WHEN Sales_D90 is not null then null
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg END as Sales_D90_p,
    CASE WHEN Sales_D120 is not null then null
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg  END as Sales_D120_p,
    CASE WHEN Sales_D150 is not null then null
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg   END as Sales_D150_p,
    CASE WHEN Sales_D180 is not null then null
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg    END as Sales_D180_p,
    CASE WHEN Sales_D210 is not null then null
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg     END as Sales_D210_p,
    CASE WHEN Sales_D240 is not null then null
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg      END as Sales_D240_p,
    CASE WHEN Sales_D270 is not null then null
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg       END as Sales_D270_p,
    CASE WHEN Sales_D300 is not null then null
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg        END as Sales_D300_p,
    CASE WHEN Sales_D330 is not null then null
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg         END as Sales_D330_p,
    CASE WHEN Sales_D360 is not null then null
    WHEN Sales_D330 is not null THEN Sales_D330*D330D360_avg
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg *D330D360_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg*D330D360_avg          END as Sales_D360_p
    from(

    select *
    ,  LAST_VALUE(d1d3_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D1D3_avg
    ,  LAST_VALUE(d3d7_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D3D7_avg
    ,  LAST_VALUE(d7d14_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D7D14_avg
    ,  LAST_VALUE(d14d30_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D14D30_avg
    ,  LAST_VALUE(d30d60_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D30D60_avg
    ,  LAST_VALUE(d60d90_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D60D90_avg
    ,  LAST_VALUE(d90d120_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d90d120_avg
    ,  LAST_VALUE(d120d150_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d120d150_avg
    ,  LAST_VALUE(d150d180_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d150d180_avg
    ,  LAST_VALUE(d180d210_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d180d210_avg
    ,  LAST_VALUE(d210d240_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d210d240_avg
    ,  LAST_VALUE(d240d270_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d240d270_avg
    ,  LAST_VALUE(d270d300_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d270d300_avg
    ,  LAST_VALUE(d300d330_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d300d330_avg
    ,  LAST_VALUE(d330d360_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d330d360_avg

    from(
    select *,
    CASE WHEN Sales_D3 is null THEN null ELSE AVG(Sales_D3/Sales_D1) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING ) END AS d1d3_avg3, -- ÌòÑÏû¨ÏõîÏ†úÏô∏ kpiÍ≥ÑÏÇ∞Ïö©
    CASE WHEN Sales_D7 is null THEN null ELSE AVG(Sales_D7/Sales_D3) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING) END AS d3d7_avg3, -- ÌòÑÏû¨ÏõîÏ†úÏô∏ kpiÍ≥ÑÏÇ∞Ïö©
<<<<<<< HEAD
=======

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    CASE WHEN Sales_D14 is null THEN null ELSE AVG(Sales_D14/Sales_D7) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d7d14_avg3,
    CASE WHEN Sales_D30 is null THEN null ELSE AVG(Sales_D30/Sales_D14) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d14d30_avg3,
    CASE WHEN Sales_D60 is null THEN null ELSE AVG(Sales_D60/Sales_D30) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d30d60_avg3,
    CASE WHEN Sales_D90 is null THEN null ELSE AVG(Sales_D90/Sales_D60) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d60d90_avg3,
    CASE WHEN Sales_D120 is null THEN null ELSE AVG(Sales_D120/Sales_D90) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d90d120_avg3,
    CASE WHEN Sales_D150 is null THEN null ELSE AVG(Sales_D150/Sales_D120) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d120d150_avg3,
    CASE WHEN Sales_D180 is null THEN null ELSE AVG(Sales_D180/Sales_D150) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d150d180_avg3,
    CASE WHEN Sales_D210 is null THEN null ELSE AVG(Sales_D210/Sales_D180) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d180d210_avg3,
    CASE WHEN Sales_D240 is null THEN null ELSE AVG(Sales_D240/Sales_D210) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d210d240_avg3,
    CASE WHEN Sales_D270 is null THEN null ELSE AVG(Sales_D270/Sales_D240) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d240d270_avg3,
    CASE WHEN Sales_D300 is null THEN null ELSE AVG(Sales_D300/Sales_D270) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d270d300_avg3,
    CASE WHEN Sales_D330 is null THEN null ELSE AVG(Sales_D330/Sales_D300) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d300d330_avg3,
    CASE WHEN Sales_D360 is null THEN null ELSE AVG(Sales_D360/Sales_D330) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d330d360_avg3
    from revraw
    )
    )
    )
<<<<<<< HEAD
=======
    )
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620


    ,final2 AS(
    select a.*, b.cost, b.cost_exclude_credit
    from final as a
    left join (
    select joyplegameid,  format_date('%Y-%m', cmpgndate) as month
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and cmpgndate >='{now_kst.year - 3}-01-01'
=======
    and cmpgndate >='2021-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,  format_date('%Y-%m', cmpgndate)

    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.month = b.month
    )



    select joyplegameid, month, cost, cost_exclude_credit, ru, cost/ru as cpru,
    ru*d1_ltv/cost_exclude_credit as d1_roas,
    ru*d3_ltv/cost_exclude_credit as d3_roas,
    ru*d7_ltv/cost_exclude_credit as d7_roas,
    ru*d14_ltv/cost_exclude_credit as d14_roas,
    ru*d30_ltv/cost_exclude_credit as d30_roas,
    ru*d60_ltv/cost_exclude_credit as d60_roas,
    ru*d90_ltv/cost_exclude_credit as d90_roas,
    ru*d120_ltv/cost_exclude_credit as d120_roas,
    ru*d150_ltv/cost_exclude_credit as d150_roas,
    ru*d180_ltv/cost_exclude_credit as d180_roas,
    ru*d210_ltv/cost_exclude_credit as d210_roas,
    ru*d240_ltv/cost_exclude_credit as d240_roas,
    ru*d270_ltv/cost_exclude_credit as d270_roas,
    ru*d300_ltv/cost_exclude_credit as d300_roas,
    ru*d330_ltv/cost_exclude_credit as d330_roas,
    ru*d360_ltv/cost_exclude_credit as d360_roas,
    ru*d14_ltv_p/cost_exclude_credit as d14_roas_p,
    ru*d30_ltv_p/cost_exclude_credit as d30_roas_p,
    ru*d60_ltv_p/cost_exclude_credit as d60_roas_p,
    ru*d90_ltv_p/cost_exclude_credit as d90_roas_p,
    ru*d120_ltv_p/cost_exclude_credit as d120_roas_p,
    ru*d150_ltv_p/cost_exclude_credit as d150_roas_p,
    ru*d180_ltv_p/cost_exclude_credit as d180_roas_p,
    ru*d210_ltv_p/cost_exclude_credit as d210_roas_p,
    ru*d240_ltv_p/cost_exclude_credit as d240_roas_p,
    ru*d270_ltv_p/cost_exclude_credit as d270_roas_p,
    ru*d300_ltv_p/cost_exclude_credit as d300_roas_p,
    ru*d330_ltv_p/cost_exclude_credit as d330_roas_p,
    ru*d360_ltv_p/cost_exclude_credit as d360_roas_p
    from final2
    """

    query_result_pltv_growth = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()


    ### 2> pLTV_D360
    client = bigquery.Client()
<<<<<<< HEAD
    query = f"""with perfo_raw AS(
=======
    query = """with perfo_raw AS(
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    select a.*
    , b.countrycode, b.os
    , b.gcat, b.mediacategory, b.class, b.media, b.adsetname, b.adname, b.optim, b.oscam, b.geocam, b.targetgroup
    from(
    select *,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_low end as combined_rev_low,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_upp end as combined_rev_upp,
    FROM `data-science-division-216308.VU.Performance_pLTV`
<<<<<<< HEAD
    where authaccountregdatekst>='{now_kst.year - 1}-01-01'
=======
    where authaccountregdatekst>='2024-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and authaccountregdatekst <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    and joyplegameid in (131,133)
    ) as a
    left join (select  *
    from `dataplatform-reporting.DataService.V_0316_0000_AuthAccountInfo_V`
    ) as b
    on a.authaccountname = b.authaccountname
    and a.joyplegameid = b.joyplegameid
    )



    select joyplegameid,  format_datE('%Y-%m',AuthAccountRegDateKST) as regmonth
    ,count(distinct if(daysfromregisterdate = 0, authaccountname, null)) as ru
    ,sum(if(daysfromregisterdate <= 360, combined_rev, null)) as pred_d360
    , max(authaccountregdatekst) as maxdate
    from perfo_raw
    group by joyplegameid, format_datE('%Y-%m',AuthAccountRegDateKST)
    """
<<<<<<< HEAD
=======


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    query_result_pltv_model = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()




    ### 3> Î≥µÍ∑ÄÏú†Ï†Ä
    client = bigquery.Client()
<<<<<<< HEAD
    query = f"""
=======
    query = """
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    with raw AS(
    select *
    , sum(d90diff) over(partition by joyplegameid, authaccountname order by logdatekst) as cum_d90diff
    from(
    select *
    , date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day ) as daydiff_beforeaccess   -- authaccountlastaccessbeforedatekst : Access Í∏∞Ï§ÄÏúºÎ°ú Î°úÍπÖ
    , case when  date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day )  >= 90 then 1 else 0  end as d90diff
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    WHERE joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and logdatekst >= '{now_kst.year - 2}-01-01'
=======
    and logdatekst >= '2023-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and DaysFromRegisterDate >= 0 -- Í∞ÄÏûÖÏùºÏù¥ Ïù¥ÌõÑÏóê Ï∞çÌûå caseÏ†úÏô∏
    )
    )

    , raw2 AS(
    select *, date_diff(logdatekst, returndate, day) as daydiff_re -- Î≥µÍ∑ÄÏùº cohort
    -- , if(returndate = AuthAccountRegDateKST, 0,1) as return_yn -- Í∞ÄÏûÖÏùºÏù¥ Î®ºÏ†Ä Ï∞çÌûå case Ìè¨Ìï®
    , if(cum_d90diff = 0, 0,1) as return_yn -- Í∞ÄÏûÖÏùºÏù¥ Î®ºÏ†Ä Ï∞çÌûå case Ìè¨Ìï®
    from(
    select *
    , first_value(logdatekst) over(partition by joyplegameid, authaccountname, cum_d90diff order by logdatekst) as returndate
    from raw
    )
    )

    , ru_raw AS(
    -- Ïã†Í∑ú Ïú†Ï†Ä Í∏∞Ï§Ä
    select joyplegameid,  format_date('%Y-%m',authaccountregdatekst) as regmonth
    , count(distinct authaccountname) as ru
    , sum(if(DaysFromRegisterDate<=360, pricekrw, null)) as d360rev
    from raw2
<<<<<<< HEAD
    where  AuthAccountRegDateKST  >= '{now_kst.year - 2}-01-01'
=======
    where  AuthAccountRegDateKST  >= '2023-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and AuthAccountRegDateKST <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)

    group by joyplegameid,    format_date('%Y-%m',authaccountregdatekst)
    )

    , return_raw AS(
    -- Î≥µÍ∑ÄÏú†Ï†Ä
    select joyplegameid,  format_date('%Y-%m', returndate) as regmonth
    , count(distinct if(daydiff_re = 0 , authaccountname, null)) as ru
    , sum(if(daydiff_re<=360, pricekrw, null)) as d360rev_all
    from raw2
<<<<<<< HEAD
    where  returndate  >= '{now_kst.year - 2}-01-01'
=======
    where  returndate  >= '2023-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and returndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,   format_date('%Y-%m', returndate)
    )


    ,final AS(
    select
    ifnull(ifnull(a.joyplegameid , b.joyplegameid) , c.joyplegameid)  as joyplegameid
    ,ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) as regmonth
    , a.ru ,b.ru as ru_all,
    d360rev  AS rev_D360,
    d360rev_all  AS rev_D360_all
    ,  cost
    ,  cost_exclude_credit
    , d360rev_all - d360rev as rev_D360_return ,
    case WHEN DATE_DIFF(
            DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY),
            LAST_DAY(DATE(CONCAT(ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) , '-01'))),
            DAY
            ) >= 360 THEN 'mature'
        ELSE 'notmature'
    END AS status
    from ru_raw  as a
    full join return_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    full join (
    select joyplegameid,  format_date('%Y-%m',cmpgndate) as regmonth, sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and cmpgndate >='{now_kst.year - 2}-01-01'
=======
    and cmpgndate >='2023-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by  joyplegameid,  format_date('%Y-%m',cmpgndate)
    ) as c
    on a.joyplegameid = c.joyplegameid
    and a.regmonth = c.regmonth
    )


    #notmature Íµ¨Í∞Ñ ÏµúÍ∑º 6Í∞úÏõî ÌèâÍ∑†
    #POTC 2024/4 ~ 6Ïõî Î°úÍ∑∏Ïù∏ Ïù¥ÏäàÎ°ú Î≥µÍ∑ÄÏú†Ï†Ä Í∏∞Ïó¨ÎèÑ ÎÇÆÏïÑ Ï†úÏô∏
    , return_user_proas AS(
    select joyplegameid, avg(rev_D360_return/cost_exclude_credit) as d360_return_roas
    , approx_quantiles(rev_D360_return/cost_exclude_credit, 2)[OFFSET(1)] as d360_return_roas_med
    from(
    select *, row_number() over (partition by joyplegameid order by regmonth desc) as rownum
    from final
    where status = 'mature'
    and ((joyplegameid = 131 and regmonth not in ('2024-04','2024-05','2024-06'))
    or joyplegameid in (133,30001,30003))
    )
    where rownum <= 6 -- ÏµúÍ∑º 6Í∞úÏõî
    group by joyplegameid
    )

<<<<<<< HEAD
=======


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    select a.*
    , rev_D360_return/cost_exclude_credit as d360_plus_return_actual
    , case when status = 'mature' then rev_D360_return/cost_exclude_credit
    else b.d360_return_roas_med  end as d360_plus_return_expected

    from final  as a
    left join return_user_proas as b
    on a.joyplegameid = b.joyplegameid

    """
<<<<<<< HEAD
=======

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    query_result_return = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()



    ### 4> BEP
    client = bigquery.Client()
<<<<<<< HEAD
    query = f"""
=======
    query = """
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    with raw AS(
    select a.*, b.value
    , case when b.value is not null then b.value
    when b.value is null and a.PGName = 'Google' then 0.3
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 131 then 0.33
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 133 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30001 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30003 then 0.31
    when b.value is null and a.PGName = 'Xsolla' and a.joyplegameid = 131 then 0.15
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 133 then 0.10
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30001 then 0.09
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30003 then 0.08
    when b.value is null and  a.PGName = 'Danal' then 0.03
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 131 then 0.3
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 133 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30001 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30003 then 0.24
    when b.value is null and  a.PGName = 'Facebook Gaming' then 0.3
    when b.value is null and a.PGName = 'Steam' then 0.3
    else 0.3
    end as commission_rate
    from(
    select JoypleGameID, format_date('%Y-%m', authaccountregdatekst) as regmonth , t2.PGName, sum(t2.PGPriceKRW) as sales
    from  dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and authaccountregdatekst >='{now_kst.year - 1}-01-01'
=======
    and authaccountregdatekst >='2024-01-01'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    group by JoypleGameID, format_date('%Y-%m', authaccountregdatekst) , t2.PGName
    ) as a
    left join (
    select joyplegameid, regmonth, PGName,  value
    , case when PGName = 'One_Store' then 'One Store'
    when PGName = 'Facebook_Gaming' then 'Facebook Gaming'
    else PGName end as pgname2
    from `data-science-division-216308.Common.pg_commission_rate_copy2`
    unpivot (
    value for PGName in (Google,Apple, Xsolla	,Danal,	One_Store	,Facebook_Gaming,	Steam)
    )
    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.pgname = b.pgname2
    )

    -- BEP Í≥ÑÏÇ∞
<<<<<<< HEAD
=======

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    select * , sum(sales) over(partition by joyplegameid , regmonth) as cumsales
    , sales /  sum(sales) over(partition by joyplegameid , regmonth) as sales_p
    , sum(commission) over(partition by joyplegameid , regmonth) as cumcommission
    , sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth) as total_commssion_rate
    , case when joyplegameid in (131,133) then 1/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
    when joyplegameid in (30001,30003) then 1.08/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
    end as bep_commission
    from(
    select  *, sales*commission_rate as commission
    from raw
    )
    """

    query_result_pg = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

    # 5> kpi roas (NY_Ï∂îÍ∞Ä)
    client = bigquery.Client()
    query = """select * from data-science-division-216308.MetaData.roas_kpi
    where userType = 'Ïã†Í∑úÏú†Ï†Ä'and operationStatus = 'Ïö¥ÏòÅ Ï§ë'"""

<<<<<<< HEAD
    query_result_kpi = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()
=======

    query_result_kpi = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    query_result_pltv_growth= query_result_pltv_growth.rename(columns={'month':'regmonth'})

    # 1. dXXX_roas Ìå®ÌÑ¥Ïùò Ïª¨Îüº Ï∞æÍ∏∞
    roas_cols = [col for col in query_result_pltv_growth.columns if re.fullmatch(r"d\d+_roas", col)]

    # 2. Í∞Å roas Ïª¨ÎüºÏóê ÎåÄÌï¥ ÎåÄÏùëÌïòÎäî _p Ïª¨ÎüºÏù¥ ÏûàÏúºÎ©¥ _growth Ïó¥ ÏÉùÏÑ±
    for col in roas_cols:
        p_col = f"{col}_p"
        if p_col in query_result_pltv_growth.columns:
            growth_col = col.replace("_roas", "roas_growth")
            query_result_pltv_growth[growth_col] = query_result_pltv_growth[col].fillna(query_result_pltv_growth[p_col])

    # 1.  ÏµúÏ¢Ö ÏÑ†ÌÉù Ïª¨Îüº
    base_cols = ['joyplegameid','regmonth', 'cost', 'cost_exclude_credit', 'ru','cpru', 'd7_roas']
    growth_cols = [col for col in query_result_pltv_growth.columns if re.fullmatch(r"d\d+roas_growth", col)]
    selected_cols = base_cols + growth_cols

    # 4. ÌïÑÌÑ∞ÎßÅÎêú ÌÖåÏù¥Î∏î Ï∂îÏ∂ú
    query_result_pltv_growth2 = query_result_pltv_growth[selected_cols]


    final = pd.merge(query_result_pltv_growth2, query_result_pltv_model[['joyplegameid','regmonth','pred_d360']]
                                    ,    on=['joyplegameid', 'regmonth'],how = 'left' )


    final = pd.merge(final, query_result_return[['joyplegameid','regmonth','d360_plus_return_actual','d360_plus_return_expected','status']]
            ,on = ['joyplegameid','regmonth'], how = 'left')


    query_result_pg= query_result_pg.rename(columns={'month':'regmonth'})
    query_result_pg= query_result_pg.rename(columns={'JoypleGameID':'joyplegameid'})
    pg_distinct = (
        query_result_pg.loc[:, ["regmonth", "joyplegameid", "bep_commission"]]
                    .drop_duplicates(ignore_index=True)
    )

    final = pd.merge(final, pg_distinct,on = ['joyplegameid','regmonth'], how = 'left')

    ##game_name
    mapping = {
        131:   "1.POTC",
        133:   "2.GBTW",
        30001: "3.WWMC",
        30003: "4.DRSG",
    }

    final["game_name"] = final["joyplegameid"].map(mapping)  # Îß§Ìïë ÏóÜÏúºÎ©¥ NaN

    final['bep_base'] = final['game_name'].map({
        '1.POTC': 1.429,
        '2.GBTW': 1.429,
        '3.WWMC': 1.543,
        '4.DRSG': 1.543
    })


    ## Í∏∞Í∞Ñ ÌïÑÌÑ∞

    # Í∏∞Ï§ÄÏùºÍ≥º ÎûòÍ∑∏
<<<<<<< HEAD
    asof = pd.Timestamp(now_kst.date()) # tz ÏóÜÏùå
=======
    asof = pd.Timestamp.today().normalize()   # <-- Ïó¨Í∏∞ ÌïµÏã¨ (tz ÏóÜÏùå)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    LAG_DAYS = 8
    obs_end_candidate = asof - pd.Timedelta(days=LAG_DAYS)  # naive

    # ÏµúÍ∑º 12Í∞úÏõîÎßå ÌïÑÌÑ∞ÎßÅ
    start_month = (asof.to_period("M") - 12).to_timestamp()
    final['regmonth_ts'] = pd.to_datetime(final['regmonth'] + "-01")

    final2 = final.copy()


    ## ÎãπÏõî ÏòàÏÉÅ COST
    # ÏõîÏ¥à/ÏõîÎßê, ÏõîÏùºÏàò
    final2['month_start'] = pd.to_datetime(final2['regmonth'] + '-01')
    final2['days_in_month'] = final2['month_start'].dt.daysinmonth
    final2['month_end'] = final2['month_start'] + pd.to_timedelta(final2['days_in_month'] - 1, unit='D')

    # 1) Ïö∞ÏÑ† ÏõîÎßêÍ≥º obs_end_candidate Ï§ë ÏûëÏùÄ Í∞íÏùÑ obs_endÎ°ú
    final2['obs_end'] = final2['month_end'].where(
        final2['month_end'] <= obs_end_candidate,  # Îëò Îã§ Timestamp
        other=obs_end_candidate
    )

    # 2) obs_end_candidateÍ∞Ä ÏõîÏ¥àÎ≥¥Îã§ Îπ†Î•¥Î©¥(=Í∑∏ Îã¨ÏùÄ ÏïÑÏßÅ ÏßëÍ≥Ñ ÏãúÏûë Ï†Ñ), NaTÎ°ú
    final2.loc[final2['month_start'] > obs_end_candidate, 'obs_end'] = pd.NaT

    # Í¥ÄÏ∏°ÏãúÏûëÏùºÏùÄ ÏõîÏ¥àÎ°ú Í∞ÄÏ†ï
    final2['obs_start'] = final2['month_start']

    # Í¥ÄÏ∏°ÏùºÏàò(ÏùåÏàò/NaT Î∞©ÏßÄ)
    final2['observed_days'] = (
        (final2['obs_end'] - final2['obs_start']).dt.days + 1
    ).clip(lower=0).fillna(0).astype(int)

    # Ïõî ÏùºÌï† ÏòàÏÉÅÎπÑÏö©
    final2['cost_raw'] = final2['cost_exclude_credit']  # ÏõêÎ≥∏ Î≥¥Í¥Ä(ÏòµÏÖò)
    final2['cost_exclude_credit'] = np.where(
        final2['observed_days'] > 0,
        final2['cost_raw'] * final2['days_in_month'] / final2['observed_days'],
        np.nan
    )

    # ÏõêÎ≥∏ Î≥¥Í¥Ä
    final2['regmonth_base'] = final2['regmonth']

    # Ï†ÑÏ≤¥ÏóêÏÑú ÎßàÏßÄÎßâ Ïõî(YYYY-MM)
    last_month_str = final2['month_start'].max().to_period('M').strftime('%Y-%m')

    def _label_last_global(r):
        if r['regmonth_base'] == last_month_str and pd.notna(r['obs_end']):
            return f"{r['regmonth_base']} ( ~ {r['obs_end'].strftime('%m/%d')})"
        else:
            return r['regmonth_base']

    final2['regmonth'] = final2.apply(_label_last_global, axis=1)

    final3 = final2.eval("""
                        d360roas_pltv = pred_d360/cost_raw
                        d360roas_growth_plus_return_real = d360roas_growth+ d360_plus_return_actual
                        d360roas_growth_plus_return_expected = d360roas_growth + d360_plus_return_expected
                        bep_diff = bep_commission - bep_base
                        """)
    '''
    # Î≥¥Ï°∞ Ïª¨Îüº Ï†ïÎ¶¨
    drop_cols = ['month_start','month_end','obs_start','obs_end','observed_days','days_in_month','regmonth_ts','cost','cost_raw']
    final3 = final3.drop(columns=drop_cols)

    final3.head()
    '''

    df = final3.copy()
    df = final3[final3['regmonth_ts'] >= start_month].copy()

    # Î≥¥Ï°∞ Ïª¨Îüº Ï†ïÎ¶¨
    drop_cols = ['month_start','month_end','obs_start','obs_end','observed_days','days_in_month','regmonth_ts','cost','cost_raw']
    df = df.drop(columns=drop_cols)

    # DRSGÎßå d180roas_growthÎëêÍ∏∞
    if "d180roas_growth" in df.columns:
        mask = df["game_name"].astype(str).str.contains(r"\bDRSG\b", case=False, na=False)
        df.loc[~mask, "d180roas_growth"] = np.nan


<<<<<<< HEAD
    ## ÌïúÍ∏ÄÍπ®Ïßê Î∞©ÏßÄ (Ïù¥ÎØ∏ ÏúÑÏóêÏÑú ÏÑ§Ï†ïÌñàÏßÄÎßå, ÌïÑÏöîÏãú Ïû¨ÌôïÏù∏)
    # set_korean_font() # Ïù¥ÎØ∏ Ï†ÑÏó≠ÏóêÏÑú Ìò∏Ï∂úÎê®
=======
    ## ÌïúÍ∏ÄÍπ®Ïßê Î∞©ÏßÄÎ•º ÏúÑÌï¥ Ìè∞Ìä∏ ÏßÄÏ†ï
    font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
    if Path(font_path).exists():
        fm.fontManager.addfont(font_path)       # ÏàòÎèô Îì±Î°ù
        mpl.rc('font', family='NanumGothic')    # Í∏∞Î≥∏ Ìè∞Ìä∏ ÏßÄÏ†ï
        mpl.rc('axes', unicode_minus=False)     # ÎßàÏù¥ÎÑàÏä§ Íπ®Ïßê Î∞©ÏßÄ
    else:
        print("‚ö†Ô∏è NanumGothic ÏÑ§Ïπò Ïã§Ìå®. Îã§Î•∏ Ìè∞Ìä∏Î•º Ïç®Ïïº Ìï©ÎãàÎã§.")
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620




    # ÌïÑÏöî Ïª¨Îüº Ï°¥Ïû¨ Ï≤¥ÌÅ¨(ÏóÜÏúºÎ©¥ KeyError Î∞©ÏßÄÏö©ÏúºÎ°ú Í±∏Îü¨ÎÉÑ)
    line_cols_all = [
        "bep_commission",
        "d360roas_growth",
        "d360roas_pltv",
        "d360roas_growth_plus_return_real",
        "d360roas_growth_plus_return_expected",
        "d180roas_growth",
    ]
    line_cols = [c for c in line_cols_all if c in df.columns]

    # Ïª¨ÎüºÎ≥Ñ ÏÉâÏÉÅ(ÏõêÌïòÎäî ÏÉâ/HEXÎ°ú Î∞îÍøîÎèÑ Îê®)
    line_colors = {
        "bep_commission": "grey",
        "d360roas_growth": "DarkOrange",
        "d180roas_growth": "DarkOrange",
        "d360roas_pltv": "green",
        "d360roas_growth_plus_return_real": "brown",
        "d360roas_growth_plus_return_expected": "brown"
    }


    # Ï†êÏÑ†ÏúºÎ°ú Í∑∏Î¶¥ ÎåÄÏÉÅ
    linestyles = {
        "bep_commission": "--",
        "d360roas_growth_plus_return_real": "--",
        "d180roas_growth": "--",
    }


    # Ï†ïÎ†¨ Î∞è xÎùºÎ≤® Ï§ÄÎπÑ
    # 1) Ï†ïÎ†¨
    df["regmonth_dt"] = pd.to_datetime(df["regmonth"], errors="coerce")
    df = df.sort_values(["game_name", "regmonth_dt", "regmonth"], kind="mergesort").reset_index(drop=True)

    # 2) Í≥†Ïú† x Ï¢åÌëú(Ï†ïÏàò)ÏôÄ ÎùºÎ≤® ÎßåÎì§Í∏∞ -- Ï≤´Îã¨Îßå Í≤åÏûÑÎ™Ö ÌëúÍ∏∞
    df["xpos"] = np.arange(len(df))
    first_mask = df.groupby("game_name").cumcount() == 0
    xticklabels = np.where(
        first_mask,
        df["game_name"].astype(str) + " | " + df["regmonth"].astype(str),  # Í≤åÏûÑÎ≥Ñ Ï≤´ Îã¨
        df["regmonth"].astype(str)                                         # ÎÇòÎ®∏ÏßÄ Îã¨
    )


    ## graph
<<<<<<< HEAD
=======

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    fig, ax1 = plt.subplots(figsize=(15, 8))
    handles, labels = [], []
    fig.suptitle(
        "Monthly ROAS & Cost",           # Î©îÏù∏ Ï†úÎ™©
        fontsize=16, fontweight="bold", y=0.98
    )


<<<<<<< HEAD
=======



>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    # ÏÑ†: Í≤åÏûÑÎ≥ÑÎ°ú segmentÎ•º ÎÇòÎà† Í∑∏Î¶¨ÎØÄÎ°ú Í≤åÏûÑ ÏÇ¨Ïù¥Í∞Ä ÏûêÎèôÏúºÎ°ú "ÎÅäÍπÄ"
    for j, col in enumerate([c for c in line_cols if c in df.columns]):
        first_for_legend = True
        for g, gdf in df.groupby("game_name", sort=False):
            ln, = ax1.plot(
                gdf["xpos"], gdf[col],
                linestyle=linestyles.get(col, "-"),
                linewidth=2.0,
                #marker="o", markersize=3.5,
                color=line_colors.get(col),
                label=(col if first_for_legend else None)
            )
            first_for_legend = False
        handles.append(ln); labels.append(col)

    # ÎπÑÏú® yÏ∂ï(0~300% Í≥†Ï†ï, ÌçºÏÑºÌä∏ ÌëúÍ∏∞)
    ax1.set_ylim(0, 3.0) #yÏ∂ï 300%ÍπåÏßÄ
    ax1.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))
    ax1.grid(axis="y", alpha=0.25)
    ax1.set_ylabel("ROAS (%, lines)")

    # Ïù¥Ï§ëÏ∂ï ÎßâÎåÄ(cost)
    ax2 = ax1.twinx()

    bar_color = "#adb5bd"        # Ïó∞ÌöåÏÉâ
    edge_color = "#495057"       # ÏßÑÌöåÏÉâ ÌÖåÎëêÎ¶¨
    bar = ax2.bar(df["xpos"], df["cost_exclude_credit"], alpha=0.35, label="cost"
                ,color=bar_color, edgecolor=edge_color)
    handles += [bar]; labels += ["cost"]
    ax2.set_ylabel("Cost")


    # xÏ∂ï ÎààÍ∏àÏóê Ïª§Ïä§ÌÖÄ ÎùºÎ≤® Ï†ÅÏö©
    ax1.set_xticks(df["xpos"])
    ax1.set_xticklabels(xticklabels, rotation=45, ha="right")

    # (ÏÑ†ÌÉù) Í≤åÏûÑ Í≤ΩÍ≥ÑÏóê ÏÑ∏Î°ú Íµ¨Î∂ÑÏÑ†
    boundary_pos = df.index[df["game_name"].ne(df["game_name"].shift()) & (df.index != 0)]
    for bp in boundary_pos:
        ax1.axvline(bp-0.5, color="lightgray", lw=1, alpha=0.6)


    ax1.legend(handles, labels, loc="best")
    plt.tight_layout()
<<<<<<< HEAD
    plt.tight_layout()
    
    # ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨Ïóê Ï†ÄÏû•
    temp_dir = tempfile.mkdtemp()
    roas_graph_path = os.path.join(temp_dir, 'roas_graph.png')
    plt.savefig(roas_graph_path, dpi=160)
    # plt.show() # Airflow ÌôòÍ≤ΩÏóêÏÑúÎäî show() Î∂àÌïÑÏöî

    # ÌäπÏ†ï ÌååÏùºÏùò Ï†àÎåÄ Í≤ΩÎ°ú ÌôïÏù∏
    # ÌäπÏ†ï ÌååÏùºÏùò Ï†àÎåÄ Í≤ΩÎ°ú ÌôïÏù∏
    file_path = os.path.abspath(roas_graph_path)
=======
    plt.savefig('roas_graph.png', dpi=160)
    plt.show()

    # ÌäπÏ†ï ÌååÏùºÏùò Ï†àÎåÄ Í≤ΩÎ°ú ÌôïÏù∏
    import os
    file_path = os.path.abspath("roas_graph.png")
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    print("Ï†ÄÏû•Îêú ÌååÏùº Ï†àÎåÄ Í≤ΩÎ°ú:", file_path)

    mapping = {"POTC": "1.POTC", "GBTW": "2.GBTW", "WWM": "3.WWMC", "DRSG": "4.DRSG", "RESU" : "5.RESU"}
    query_result_kpi['game'] = query_result_kpi['project'].map(mapping)
    cols = ['game', 'kpi_d1', 'kpi_d3', 'kpi_d7', 'kpi_d14', 'kpi_d30', 'kpi_d60',
    'kpi_d90', 'kpi_d120', 'kpi_d150', 'kpi_d180', 'kpi_d210',
    'kpi_d240', 'kpi_d270', 'kpi_d300', 'kpi_d330', 'kpi_d360']

    kpi = query_result_kpi[cols]


    kpi_by_game = {
        "1.POTC": kpi[kpi["game"] == "1.POTC"],
        "2.GBTW": kpi[kpi["game"] == "2.GBTW"],
        "3.WWMC": kpi[kpi["game"] == "3.WWMC"],
        "4.DRSG": kpi[kpi["game"] == "4.DRSG"],
    }

    for game in kpi_by_game.keys():

        df_tmp = kpi_by_game[game].copy()

        # Ïà´Ïûê Ïª¨Îüº Î¶¨Ïä§Ìä∏ (game Ï†úÏô∏)
        num_cols = [c for c in df_tmp.columns if c != 'game']

        # "%" ÌòïÌÉúÎ°ú Î≥ÄÌôò
        df_tmp[num_cols] = df_tmp[num_cols].applymap(
            lambda x: f"{x*100:.2f}%" if pd.notna(x) else ""
        )

        kpi_by_game[game] = df_tmp

    base_cols = ['game_name','regmonth','cost_exclude_credit','cpru','d7_roas','regmonth_ts']
    growth_cols = [col for col in final3.columns if re.fullmatch(r"d\d+roas_growth", col)]
    base_cols2 = ['d360roas_pltv','d360_plus_return_actual','d360roas_growth_plus_return_real'
                ,'d360_plus_return_expected','d360roas_growth_plus_return_expected'
                ,'bep_base','bep_commission','bep_diff']

    selected_cols = base_cols + growth_cols+base_cols2

    final4 = final3[selected_cols].sort_values(by = ['game_name', 'regmonth'])
<<<<<<< HEAD
    final4 = final4[final4['regmonth_ts'] >= f'{now_kst.year}-01-01']
=======
    final4 = final4[final4['regmonth_ts'] >= '2024-01-01']
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

    df_numeric = final4.copy()
    df_numeric = df_numeric.reset_index(drop=True)
    df_numeric = df_numeric.drop(columns='regmonth_ts')

<<<<<<< HEAD

    nest_asyncio.apply()


=======
    nest_asyncio.apply()

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ########### growth ÏòàÏ∏°Ïπò ÌöåÏÉâ ÏùåÏòÅ Î∞òÏòÅ
    # ÏòàÏ∏° Í∏∞Ï§Ä dnÏùÑ Í∞Å row(regmonth)Î≥ÑÎ°ú Ï∂îÎ°†
    def infer_cohort_dn_map(df):
        cohort_map = {}
        for idx, row in df.iterrows():
            regmonth = idx[1] if isinstance(idx, tuple) else row.get('regmonth', None)
            for col in df.columns:
                if re.fullmatch(r'd\d+_roas_p', col):
                    if pd.notna(row[col]):
                        dn = int(re.findall(r'\d+', col)[0])
                        # ÏòàÏ∏°ÏπòÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Í∞ÄÏû• ÏûëÏùÄ dn Í∞íÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÑ§Ï†ï
                        if regmonth not in cohort_map or dn < cohort_map[regmonth]:
                            cohort_map[regmonth] = dn
        return cohort_map


    # ÏòàÏ∏° cohort ÌôïÏù∏
    cohort_dn_map = infer_cohort_dn_map(query_result_pltv_growth)
    cohort_dn_map

    # Ïª¨Îüº Ïù¥Î¶ÑÏóêÏÑú dn Í∞í Ï∂îÏ∂ú
    def extract_dn(col):
        match = re.match(r'd(\d+)', col)  # d Îí§Ïùò Ïà´ÏûêÎßå Ï∂îÏ∂ú
        return int(match.group(1)) if match else None


    dn_growth_columns = [col for col in df_numeric.columns if re.fullmatch(r'd\d+roas_growth', col)]
    dn_values = {col: extract_dn(col) for col in dn_growth_columns}


    # regmonth Î¨∏ÏûêÏïàÏóê Ìè¨Ìï®ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
    def resolve_cohort_dn(regmonth, cohort_dn_map):
        r = str(regmonth or "")
        # Ìè¨Ìï® Îß§Ïπ≠ÎêòÎäî Î™®Îì† ÌÇ§ Ï§ë Í∞ÄÏû• Í∏¥ ÌÇ§(Îçî Íµ¨Ï≤¥Ï†Å)Î•º Ïö∞ÏÑ†
        candidates = [(k, v) for k, v in cohort_dn_map.items() if k and str(k) in r]
        if not candidates:
            return np.inf
        # ÌÇ§ Í∏∏Ïù¥ Í∏∞Ï§ÄÏúºÎ°ú ÏµúÏû• Îß§Ïπ≠ Ïö∞ÏÑ†
        k, v = max(candidates, key=lambda kv: len(str(kv[0])))
        return v
<<<<<<< HEAD

    ## ÌÖåÏä§Ìä∏ ÏΩîÎìú

=======
    
    ## ÌÖåÏä§Ìä∏ ÏΩîÎìú
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    # --- [1] Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω Îßµ Ï†ïÏùò ---
    rename_dict = {
        "d7_roas": "growth_d7",
        "d360roas_pltv": "pltv_d360",
        "d360_plus_return_actual": "return_d360",
        "d360roas_growth_plus_return_real": "gr_plus_ret_act_d360",
        "d360_plus_return_expected": "return_exp_d360",
        "d360roas_growth_plus_return_expected": "gr_plus_ret_exp_d360"
    }
    for d in [14, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]:
        rename_dict[f"d{d}roas_growth"] = f"growth_d{d}"

    # --- [2] Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ïª¨ÎüºÎ™Ö ÏÑ†Ï†ú Î≥ÄÍ≤Ω ---
    df_numeric = df_numeric.rename(columns=rename_dict)

    # --- [3] Î≥ÄÍ≤ΩÎêú Ïù¥Î¶Ñ Í∏∞Ï§Ä Ìó¨Ìçº Î≥ÄÏàò Ïû¨ÏÑ§Ï†ï ---
    # Ïù¥Ï†ú growth_dÎ°ú ÏãúÏûëÌïòÎäî Ïª¨ÎüºÏóêÏÑú Ïà´ÏûêÎ•º Ï∂îÏ∂úÌï©ÎãàÎã§.
    def extract_dn_new(col):
        match = re.search(r'd(\d+)', col)
        return int(match.group(1)) if match else None

    # Î∞îÎÄê Ïª¨ÎüºÎ™Ö Í∏∞Î∞òÏúºÎ°ú dn_values ÏÉùÏÑ±
    dn_growth_columns = [col for col in df_numeric.columns if col.startswith('growth_d')]
    dn_values = {col: extract_dn_new(col) for col in dn_growth_columns}

    # --- [4] Ïä§ÌÉÄÏùº Ìï®ÏàòÎì§ ÏóÖÎç∞Ïù¥Ìä∏ (ÏÉà Ïù¥Î¶Ñ Î∞òÏòÅ) ---
<<<<<<< HEAD
=======

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    def highlight_based_on_dn(row):
        regmonth = row['regmonth']
        cohort_dn = resolve_cohort_dn(regmonth, cohort_dn_map)
        return [
            (
                'background-color: lightgray'
                if (
                    col.startswith('growth_d') and # Ï°∞Í±¥ Î≥ÄÍ≤Ω
                    isinstance(dn_values.get(col), (int, float)) and
                    isinstance(cohort_dn, (int, float)) and
                    dn_values[col] >= cohort_dn and
                    pd.notna(row[col])
                ) else ''
            )
            for col in row.index
        ]

    def highlight_roas_vs_bep(row):
        styles = []
        for col in row.index:
            style = ""
            try:
                val = row[col]
                # % Ï≤òÎ¶¨ Î°úÏßÅÏùÄ ÎèôÏùº
                roas_val = float(val.replace('%', '')) / 100 if isinstance(val, str) and val.endswith('%') else val

                # growth_d Î°ú ÏãúÏûëÌïòÎäî Ïª¨Îüº Ï≤¥ÌÅ¨
                if col.startswith("growth_d") and pd.notnull(row.get("bep_base")):
                    if pd.notnull(roas_val) and roas_val > row["bep_base"]:
                        style = "background-color: #fbe4e6"

                # Î∞îÎÄê d360 ÌîåÎü¨Ïä§ Ïª¨ÎüºÎ™Ö Ï≤¥ÌÅ¨
                elif col in ["gr_plus_ret_act_d360", "gr_plus_ret_exp_d360"] and pd.notnull(row.get("bep_commission")):
                    if pd.notnull(roas_val) and roas_val > row["bep_commission"]:
                        style = "background-color: #fbe4e6"
            except: pass
            styles.append(style)
        return styles

    def highlight_over_kpi(row, kpi_df):
        game = row['game_name']
        target = kpi_df[kpi_df['game'] == game]
        if target.empty: return [''] * len(row)
        target_row = target.iloc[0]
        styles = []

        for col in row.index:
            base_style = ""
            # d7_roas -> growth_d7
            if col == "growth_d7":
                kpi_val = target_row.get("kpi_d7")
                if pd.notna(row[col]) and pd.notna(kpi_val) and row[col] >= kpi_val:
                    base_style = "color: red; font-weight: bold;"

            # growth_d?? ÌòïÌÉú Ï≤¥ÌÅ¨
            elif col.startswith("growth_d"):
                dn = extract_dn_new(col)
                kpi_col = f"kpi_d{dn}"
                if kpi_col in target_row:
                    kpi_val = target_row[kpi_col]
                    if pd.notna(row[col]) and pd.notna(kpi_val) and row[col] >= kpi_val:
                        base_style = "color: red; font-weight: bold;"

            styles.append(base_style)
        return styles

    # --- [5] Í≤åÏûÑÎ≥Ñ Ìëú Î∂ÑÎ¶¨ Î∞è Styler/RAW ÏÉùÏÑ± ---
    game_groups = df_numeric.groupby('game_name')
    styled_tables = {}
    raw_df_by_game = {}

    # Ìè¨Îß∑ÌåÖ ÎåÄÏÉÅ Ïª¨Îüº Î¶¨Ïä§Ìä∏ (ÏÉàÎ°úÏö¥ Ïª¨ÎüºÎ™Ö Í∏∞Ï§Ä)
    format_percent_cols = [
        "growth_d7", "pltv_d360", "return_d360", "bep_base", "bep_commission", "bep_diff",
        "gr_plus_ret_act_d360", "gr_plus_ret_exp_d360", "return_exp_d360",
    ]
    format_comma_cols = ["cost_exclude_credit", "cpru"]

    for game, game_df in game_groups:
        # A. Ïù¥ÎØ∏ÏßÄÏö© Ïä§ÌÉÄÏùº ÏÉùÏÑ± (Ïù¥Ï†ÑÍ≥º ÎèôÏùº)
        growth_cols = [col for col in game_df.columns if col.startswith("growth_d")]

        styled_game = game_df.style\
            .format({
                "cost_exclude_credit": "{:,.0f}",
                "cpru": "{:,.0f}",
                # Î™®Îì† growth Í¥ÄÎ†® Î∞è ÏßÄÌëú Ïª¨ÎüºÏóê % Ï†ÅÏö©
                **{col: "{:.2%}" for col in game_df.columns if col.startswith("growth") or any(x in col for x in ["pltv", "return", "ret", "bep"])}
            })\
            .bar(subset=['cost_exclude_credit', 'bep_diff'], color='#f4cccc')\
            .bar(subset=['cpru'], color='#b6d7a8')\
            .bar(subset=growth_cols, color='#c9daf8')\
            .bar(subset=['return_d360', 'return_exp_d360'], color='#ffe599')\
            .set_table_styles([
                {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('font-weight', 'bold')]}
            ])\
            .apply(highlight_based_on_dn, axis=1)\
            .apply(highlight_roas_vs_bep, axis=1)\
            .apply(highlight_over_kpi, axis=1, kpi_df=kpi)

        styled_tables[game] = styled_game

        # B. ÎÖ∏ÏÖò ÏóÖÎ°úÎìúÏö© RAW Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ (Ïã§Ï†ú Í∞íÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò)
        notion_df = game_df.copy()

        # 1. ÏΩ§Îßà Ï†ÅÏö© (ÎπÑÏö©, CPRU)
        for col in format_comma_cols:
            if col in notion_df.columns:
                notion_df[col] = notion_df[col].map(lambda x: f"{x:,.0f}" if pd.notnull(x) else "")

        # 2. ÌçºÏÑºÌä∏ Ï†ÅÏö© (Î™®Îì† growth_ Í¥ÄÎ†® Î∞è KPI Í¥ÄÎ†®)
        for col in notion_df.columns:
            # Ïª¨ÎüºÎ™ÖÏù¥ growthÎ°ú ÏãúÏûëÌïòÍ±∞ÎÇò, ÌçºÏÑºÌä∏ ÎåÄÏÉÅ Î¶¨Ïä§Ìä∏Ïóê Ìè¨Ìï®Îêú Í≤ΩÏö∞
            if col.startswith("growth") or col in format_percent_cols:
                notion_df[col] = notion_df[col].map(lambda x: f"{x:.2%}" if pd.notnull(x) else "")

        raw_df_by_game[game] = notion_df

<<<<<<< HEAD
    # styled_tables["1.POTC"]
    # print(f"Checking {game}: {styled_table.columns.tolist()[:30]}...")

=======
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ## ÎÖ∏ÏÖò ÌÖåÏù¥Î∏î ÌòïÌÉúÎ°ú ÎèÑÌëú ÌòïÏÑ±
    def df_to_table_rows(df, max_rows=100):
        rows = []

        # header
        rows.append({
            "object": "block",
            "type": "table_row",
            "table_row": {
                "cells": [
                    [{"type": "text", "text": {"content": str(col)}}]
                    for col in df.columns
                ]
            }
        })

        # data
        for _, r in df.head(max_rows - 1).iterrows():
            rows.append({
                "object": "block",
                "type": "table_row",
                "table_row": {
                    "cells": [
                        [{"type": "text", "text": {"content": "" if pd.isna(v) else str(v)}}]
                        for v in r.tolist()
                    ]
                }
            })

        return rows

    # HTML ÌÖúÌîåÎ¶ø Ï†ïÏùò
    # (NY_Ï∂îÍ∞Ä) ÌÖåÏù¥Î∏î ÌÅ¨Í∏∞ Ï°∞Ï†ï & ÌÖåÏù¥Î∏î Ï∂îÍ∞Ä
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            table { border-collapse: collapse; font-size: 20px; }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: right;
            }
            th { background-color: #f0f0f0; }
        </style>
    </head>
    <body>

        <h2>{{ game_name }} ÏõîÍ∞Ñ ROAS raw</h2>
        {{ table | safe }}

        <hr>

        <h2>{{ game_name }} KPI Table</h2>
        {{ kpi_table | safe }}
    </body>
    </html>
    """


    # Í∞Å Í≤åÏûÑÎ≥ÑÎ°ú HTML ÌååÏùº Ï†ÄÏû•
    for game, styled_table in styled_tables.items():
        # ÌÖåÏù¥Î∏îÏùÑ HTMLÎ°ú Î≥ÄÌôò
        table_html = styled_table.to_html()

        #(NY_Ï∂îÍ∞Ä) ROAS kpi ÎèÑÌëúÎèÑ ÌïòÎã®Ïóê Ï∂îÍ∞Ä
        kpi_html = kpi_by_game[game].to_html(index=False, classes="kpi-table",
        escape=False   # % Í∏∞Ìò∏ Ïú†ÏßÄ
                                            )

        # HTML ÌååÏùºÎ°ú Ï†ÄÏû•
        rendered_html = Template(html_template).render(game_name=game, table=table_html, kpi_table=kpi_html)

<<<<<<< HEAD
        # Ï†ÄÏû•Ìï† ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï (ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÇ¨Ïö©)
        html_path = os.path.join(temp_dir, f"{game}_roas_table.html")
=======
        # Ï†ÄÏû•Ìï† ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï
        html_path = f"{game}_roas_table.html"
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

    # HTML ÌååÏùº Ï†ÄÏû•
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(rendered_html)

        print(f"{game} ÌÖåÏù¥Î∏î HTMLÎ°ú Ï†ÄÏû• ÏôÑÎ£å: {html_path}")

        # dfi.export(
        #     styled_table,
        #     html_path,
        #     table_conversion='playwright', # ÌòπÏùÄ 'playwright'
        #     dpi=500
        # )

        # print(f"{game} ÌÖåÏù¥Î∏î Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å (dfi Î∞©Ïãù): {html_path}")


    # Ïù¥ÎØ∏ÏßÄ Ï∫°Ï≤ò ÎπÑÎèôÍ∏∞ Ìï®Ïàò
    # ÏàòÏ†ïÎêú capture_html_to_image Ìï®Ïàò
    async def capture_html_to_image(html_path, output_image_path):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)

            # device_scale_factorÎ•º 2 ÎòêÎäî 3ÏúºÎ°ú ÏÑ§Ï†ï (Ïà´ÏûêÍ∞Ä ÎÜíÏùÑÏàòÎ°ù Í≥†ÌôîÏßà)
            # ÎÑàÎπÑ(width)ÎèÑ ÌÖåÏù¥Î∏îÏù¥ ÏûòÎ¶¨ÏßÄ ÏïäÎèÑÎ°ù ÎÑâÎÑâÌïòÍ≤å ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.
            context = await browser.new_context(
                viewport={"width": 600, "height": 400},
                device_scale_factor= 1  # 4Î∞∞ ÏÑ†Î™ÖÌïòÍ≤å Ï∫°Ï≤ò
            )
            page = await context.new_page()

            await page.goto("file://" + os.path.abspath(html_path), wait_until="networkidle")

            # Ïù¥ÎØ∏ÏßÄ Ï∫°Ï≥ê
            await page.screenshot(path=output_image_path, full_page=True, animations="disabled")

            await browser.close()

    # # Í≤åÏûÑÎ≥Ñ HTMLÏùÑ Ïù¥ÎØ∏ÏßÄÎ°ú Ï†ÄÏû•ÌïòÍ∏∞
    async def save_images_for_all_games():
        for game in styled_tables.keys():
<<<<<<< HEAD
            html_path = os.path.join(temp_dir, f"{game}_roas_table.html")  # HTML ÌååÏùº Í≤ΩÎ°ú
            output_image_path = os.path.join(temp_dir, f"{game}_roas_table.png")  # Ï†ÄÏû•Îê† Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú
=======
            html_path = f"{game}_roas_table.html"  # HTML ÌååÏùº Í≤ΩÎ°ú
            output_image_path = f"{game}_roas_table.png"  # Ï†ÄÏû•Îê† Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

            await capture_html_to_image(html_path, output_image_path)
            print(f"{game} ÌÖåÏù¥Î∏îÏùÑ Ïù¥ÎØ∏ÏßÄÎ°ú Ï†ÄÏû• ÏôÑÎ£å: {output_image_path}")

    # ÎπÑÎèôÍ∏∞ Ïã§Ìñâ (SpyderÎÇò GCP ÌôòÍ≤ΩÏóêÏÑú)
    asyncio.get_event_loop().run_until_complete(save_images_for_all_games())

    #### gemini Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞
    # ÏµúÍ∑º 6Í∞úÏõî Îç∞Ïù¥ÌÑ∞Îßå Ï∂îÏ∂ú
    recent_data = final3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(6)
    #recent_data = final3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(7).sort_values(['game_name', 'regmonth']).groupby('game_name').head(6)

    exclude_cols = ['d360_plus_return_actual', 'd360_plus_return_expected','joyplegameid','pred_d360','cost','cost_raw','d360roas_pltv','ru','d360roas_growth_plus_return_real']
    recent_data = recent_data.drop(columns=exclude_cols)


    # ÌÖåÏù¥Î∏î ÌòïÌÉúÎ°ú Î¨∏ÏûêÏó¥Ìôî (Markdown table)
    def df_to_md(df):
        header = "| " + " | ".join(df.columns) + " |"
        sep = "| " + " | ".join(["---"]*len(df.columns)) + " |"
        rows = "\n".join("| " + " | ".join(map(str, row)) + " |" for row in df.values)
        return "\n".join([header, sep, rows])

    recent_data_md = df_to_md(recent_data)

    PROJECTS = [
        {"project": "POTC", "database_id": "1eeea67a56818058a431dc9a754beeab"},
        {"project": "GBTW", "database_id": "1eeea67a568180f18048dcc7769a3621"},
        {"project": "WWMC", "database_id": "1eeea67a568180449ca2d06c15779d6e"},
        {"project": "DRSG", "database_id": "1eeea67a5681809b8da6ddb8e6e9d0d5"},
    ]


    # Notion API Î†àÏù¥Ìä∏Î¶¨Î∞ã(ÌèâÍ∑† 3rps Í∂åÏû•) ‚Üí ÏïàÏ†ÑÏä¨Î¶Ω
    NOTION_SLEEP_SEC = 0.4


    # Í∏∞Ï§ÄÏùº(ÏµúÍ∑º 8Ïùº Ï†Ñ) ‚Äî ÏõîÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÌÉêÏÉâ Ïãú Í∏∞Ï§ÄÏù¥ ÎêòÎäî ÎÇ†Ïßú
<<<<<<< HEAD
    REF_DT = now_kst - timedelta(days=8)
=======
    REF_DT = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=8)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

    notion = NotionClient(auth=NOTION_TOKEN, timeout_ms=60_000, log_level=logging.WARNING, notion_version=NOTION_VERSION)
    print(NOTION_VERSION)

    def sleep():
        time.sleep(NOTION_SLEEP_SEC + random.uniform(0, 0.25))  # + ÏßÄÌÑ∞


    # Î™©Ï†Å: ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïä§ÌÇ§ÎßàÏóêÏÑú "title" ÌÉÄÏûÖ ÏÜçÏÑ±Ïùò Ïã§Ï†ú ÏÜçÏÑ±Î™ÖÏùÑ Ï°∞Ìöå
    # ÏûÖÎ†•: database_id (str)
    # Ï∂úÎ†•: Ï†úÎ™© ÏÜçÏÑ±Î™Ö (str) ‚Äî Ïòà: "Name", "Ï†úÎ™©" Îì±
    def get_title_prop_name(database_id: str) -> str:
        schema = notion.databases.retrieve(database_id=database_id)
        sleep()
        print(NOTION_VERSION)

        for name, prop in schema.get("properties", {}).items():
            if prop.get("type") == "title":
                return name
        raise RuntimeError("title ÌÉÄÏûÖ Ïª¨ÎüºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.")

    # Î™©Ï†Å: datetime ‚Üí 'YYÎÖÑ MÏõî' ÌïúÍµ≠Ïãù Ïó∞/Ïõî Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
    # ÏûÖÎ†•: dt (datetime)
    # Ï∂úÎ†•: Ïòà) '25ÎÖÑ 9Ïõî'
    def format_kor_year_month(dt: datetime) -> str:
        # '25ÎÖÑ 9Ïõî'
        yy = dt.year % 100
        m  = dt.month
        return f"{yy}ÎÖÑ {m}Ïõî"

    # Î™©Ï†Å: datetime ‚Üí 'YYYY-MM' ÌòïÌÉúÏùò Ïõî ÌÇ§ ÏÉùÏÑ±(Î©îÌÉÄ ÌÇ§/Î¶¨Ïä§Ìä∏Î¶≠Ìä∏ Ïö©)
    # ÏûÖÎ†•: dt (datetime)
    # Ï∂úÎ†•: Ïòà) '2025-09'
    def ym_key(dt: datetime) -> str:
        # 'YYYY-MM' (restricts/Î©îÌÉÄ ÌÇ§)
        return f"{dt.year:04d}-{dt.month:02d}"


    # Î™©Ï†Å: ÌîÑÎ°úÏ†ùÌä∏ ÏõîÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÌéòÏù¥ÏßÄ 1Í±¥ÏùÑ Notion DBÏóêÏÑú Ï†úÎ™© Ìå®ÌÑ¥ÏúºÎ°ú Í≤ÄÏÉâ
    #  - Ï†úÎ™© Ìå®ÌÑ¥: f"[{project}] {YY}ÎÖÑ {M}Ïõî ÎàÑÏ†Å UA Î¶¨Ìè¨Ìä∏"
    #  - '9Ïõî'/'09Ïõî' Î™®Îëê Îß§Ïπ≠(OR)
    # ÏûÖÎ†•: database_id (str), project (str), ref_dt (datetime|None)
    # Ï∂úÎ†•: {project, page_id, ym_title, ym, page_url} ÎòêÎäî None
    def find_month_page_by_title(database_id: str, project: str, ref_dt: Optional[datetime] = None):
        if ref_dt is None:
            ref_dt = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=8)
        ym_kor = format_kor_year_month(ref_dt)       # '25ÎÖÑ 9Ïõî'
        ym_kor_zero = f"{ref_dt.year%100}ÎÖÑ {ref_dt.month:02d}Ïõî"  # '25ÎÖÑ 09Ïõî' (Ï†úÎ™©Ïóê 0Ìå®Îî©Ïùº Îïå ÎåÄÎπÑ)

        title_prop = get_title_prop_name(database_id)

        # contains AND (project / Í≥†Ï†ïÎ¨∏Íµ¨) + (Ïõî ÌëúÍ∏∞Îäî OR)
        filt = {
            "and": [
                {"property": title_prop, "title": {"contains": f"[{project}]"}},
                {"property": title_prop, "title": {"contains": "ÎàÑÏ†Å UA Î¶¨Ìè¨Ìä∏"}},
                {"or": [
                    {"property": title_prop, "title": {"contains": ym_kor}},
                    {"property": title_prop, "title": {"contains": ym_kor_zero}},
                ]}
            ]
        }
        print(NOTION_VERSION)

        q = notion.databases.query(
            **{
                "database_id": database_id,
                "filter": filt,
                "sorts": [{"timestamp": "last_edited_time", "direction": "descending"}],
                "page_size": 1
            }
        )
        sleep()
        results = q.get("results", [])
        if not results:
            return None
        page = results[0]
        page_id = page["id"]
        return {
            "project": project,
            "page_id": page_id,
            "ym_title": ym_kor,
            "ym": ym_key(ref_dt),
            "page_url": f"https://www.notion.so/{page_id.replace('-','')}",


        }

    ################################ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
    # ==== 2) ÌéòÏù¥ÏßÄ ‚Üí ÏÑπÏÖò(Ï£ºÍ∞Ñ) Ï∂îÏ∂ú(Ïû¨Í∑Ä + ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò) =========================
    def list_all_children(block_id: str):
        results, cursor = [], None
        while True:
            resp = notion.blocks.children.list(block_id=block_id, start_cursor=cursor)
            results.extend(resp["results"])
            sleep()
            if not resp.get("has_more"):
                break
            cursor = resp.get("next_cursor")
        return results

    def rts_to_text(rts):  # rich_text ‚Üí plain
        return "".join(rt.get("plain_text","") for rt in (rts or []))

    def block_to_text(block, include_children=True) -> str:
        t = block["type"]; b = block.get(t, {}); lines: List[str] = []
        def add(x):
            if x and x.strip():
                lines.append(x.strip())

        if t in ("paragraph","heading_1","heading_2","heading_3",
                "bulleted_list_item","numbered_list_item","to_do","quote","callout","code"):
            txt = rts_to_text(b.get("rich_text", []))
            if t=="bulleted_list_item" and txt: txt = "- " + txt
            if t=="numbered_list_item" and txt: txt = "1. " + txt
            if t=="to_do": txt = f"{'[x]' if b.get('checked') else '[ ]'} {txt}"
            add(txt)
        elif t=="bookmark":
            add(b.get("url",""))
        elif t=="table":
            # table_row children ‚Üí cell ÌÖçÏä§Ìä∏ ' | 'Î°ú Í≤∞Ìï©
            for row in list_all_children(block["id"]):
                if row["type"]=="table_row":
                    cells = row["table_row"]["cells"]
                    row_txt = " | ".join(rts_to_text(c) for c in cells)
                    add(row_txt)

            # has_childrenÏù¥ TrueÏùº ÎïåÎßå ÏûêÏãù Î∏îÎ°ùÏùÑ Ï≤òÎ¶¨
        if include_children and block.get("has_children"):
            for ch in list_all_children(block["id"]):
                sub = block_to_text(ch, include_children=True)
                if sub:
                    lines.append(sub)

        return "\n".join(lines)


    def detect_heading_level(top_blocks):
        """top Î∏îÎ°ùÎì§ÏóêÏÑú ÏÇ¨Ïö©Ìï† Ìó§Îî© Î†àÎ≤®ÏùÑ ÏûêÎèô ÏÑ†ÌÉù"""
        for ht in ("heading_1", "heading_2", "heading_3"):
            if any(b["type"] == ht for b in top_blocks):
                return ht
        return None

    def extract_sections_by_heading_auto(page_id: str, include_preface=False):
        """
        - heading_1 ÏûàÏúºÎ©¥ Í∑∏Í±∏Î°ú ÏÑπÏÖòÌôî
        - ÏóÜÏúºÎ©¥ heading_2 ‚Üí ÏóÜÏúºÎ©¥ heading_3
        - Ïñ¥Îñ§ Ìó§Îî©ÎèÑ ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÑ 1Í∞ú ÏÑπÏÖòÏúºÎ°ú Î∞òÌôò
        - include_preface=TrueÎ©¥ Ï≤´ Ìó§Îî© Ï†Ñ Î¨∏Îã®ÏùÑ '(ÏÑúÎ¨∏)' ÏÑπÏÖòÏúºÎ°ú Ìè¨Ìï®
        """
        top = list_all_children(page_id)
        heading_type = detect_heading_level(top)

        # Ìó§Îî©Ïù¥ Ï†ÑÌòÄ ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ Î¨∂Ïñ¥ÏÑú Î∞òÌôò
        if not heading_type:
            all_lines = []
            for blk in top:
                t = block_to_text(blk, include_children=True)
                if t: all_lines.append(t)
            content = "\n".join(all_lines).strip()
            return [{"title": "(Ï†ÑÏ≤¥)", "content": content}] if content else []

        sections = []
        current = None
        preface_lines = []

        i = 0
        while i < len(top):
            blk = top[i]
            btype = blk["type"]

            if btype == heading_type:
                # Ïù¥Ï†Ñ ÏÑπÏÖò ÎßàÍ∞ê
                if current:
                    sections.append(current)

                # Ï†úÎ™©(ÏûêÏãù Ï†úÏô∏)
                title = block_to_text(blk, include_children=False) or "Untitled"
                current = {"title": title, "content_lines": []}

                # Ïù¥ Ìó§Îî©Ïùò ÏûêÏãù(ÌÜ†Í∏Ä/Ìëú/Î¶¨Ïä§Ìä∏ Îì±) Ìè¨Ìï®
                if blk.get("has_children"):
                    for ch in list_all_children(blk["id"]):
                        sub = block_to_text(ch, include_children=True)
                        if sub:
                            current["content_lines"].append(sub)

                i += 1
                continue

            # Ï≤´ Ìó§Îî© ÎÇòÏò§Í∏∞ Ï†Ñ Î¨∏Îã®ÏùÄ ÌîÑÎ¶¨ÌéòÏù¥Ïä§Î°ú Î™®ÏùÑ Ïàò ÏûàÏùå
            if current is None and include_preface:
                txt = block_to_text(blk, include_children=True)
                if txt:
                    preface_lines.append(txt)
            elif current is not None:
                # Îã§Ïùå ÎèôÏùº Î†àÎ≤® Ìó§Îî© Ï†ÑÍπåÏßÄ ÌòïÏ†ú Î∏îÎ°ùÏùÑ Î≥∏Î¨∏Ïóê Ìè¨Ìï®
                txt = block_to_text(blk, include_children=True)
                if txt:
                    current["content_lines"].append(txt)

            i += 1

        # ÎßàÏßÄÎßâ ÏÑπÏÖò ÎßàÍ∞ê
        if current:
            sections.append(current)

        # ÌîÑÎ¶¨ÌéòÏù¥Ïä§ ÏÑπÏÖò ÏÇΩÏûÖ
        if include_preface and preface_lines:
            preface = "\n".join(preface_lines).strip()
            if preface:
                sections.insert(0, {"title": "(ÏÑúÎ¨∏)", "content": preface})

        # content_lines Î≥ëÌï© + Îπà ÏÑπÏÖò Ï†úÍ±∞
        out = []
        for s in sections:
            content = "\n".join(s.get("content_lines", [])).strip()
            if content:
                out.append({"title": s["title"], "content": content})

        return out
<<<<<<< HEAD

=======
    
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ############################### ÎÖ∏ÏÖò Weekly Report Ï†ÑÎ¨∏ Ï∂îÏ∂ú

    weekly_report_db3 = []

    # Î™©Ï†Å: Í∞Å ÌîÑÎ°úÏ†ùÌä∏Î≥Ñ ÏõîÍ∞Ñ ÌéòÏù¥ÏßÄÎ•º Ï∞æÏïÑ ÏÑπÏÖò ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂ú ‚Üí LLM ÏöîÏïΩ ‚Üí ÏöîÏïΩÎ≥∏ DBÏóê Ï†ÅÏû¨

    for p in PROJECTS:
        print(f"\n=== {p['project']} ===")
        sel = find_month_page_by_title(p["database_id"], p["project"], ref_dt=REF_DT)
        print(f"  [SELECT] {sel}")
        if not sel:
            print("  [WARN] ÎãπÏõî Ï†úÎ™© Ìå®ÌÑ¥ ÌéòÏù¥ÏßÄÎ•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.")
            weekly_report_db3.append({
                "project": p['project'],
                "summary": "(Ìï¥Îãπ Ïõî Ï£ºÍ∞Ñ Î¶¨Ìè¨Ìä∏ Ïª®ÌÖçÏä§Ìä∏ ÏóÜÏùå)"
            })
            continue

        page_id = sel["page_id"]
        sections = extract_sections_by_heading_auto(page_id, include_preface=False)

        if not sections:
            print("  [HINT] ÎèôÏùº Î†àÎ≤® Ìó§Îî©Ïù¥ ÏóÜÍ≥† Î≥∏Î¨∏Ïù¥ childrenÏóêÎßå ÏûàÏùÑ Ïàò ÏûàÏñ¥ ÏµúÏÉÅÏúÑ Î∏îÎ°ù Íµ¨Ï°∞Î•º Ïû¨Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.")
            weekly_report_db3.append({
                "project": p['project'],
                "summary": "Ï∂îÏ∂ú Í∞ÄÎä•Ìïú Î≥∏Î¨∏ ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§."
            })
            continue

        # Ï†ÑÏ≤¥ ÏÑπÏÖò ÎÇ¥Ïö©ÏùÑ ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú Í≤∞Ìï©
        full_report_text = "\n\n".join([s['content'] for s in sections])

        ## gemini 3.0 high ÏúÑÌïú ÏΩîÎìú Ï∂îÍ∞Ä
        weekly_report_db3.append({
            "project": p['project'],
            "full_report_text": full_report_text
        })

<<<<<<< HEAD
    # ÏãúÏä§ÌÖú ÏßÄÏãú_ÌéòÎ•¥ÏÜåÎÇò Î∞è Ï†úÏïΩÏ°∞Í±¥
=======
        # ÏãúÏä§ÌÖú ÏßÄÏãú_ÌéòÎ•¥ÏÜåÎÇò Î∞è Ï†úÏïΩÏ°∞Í±¥
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

    system_instruction = """
    ÎÑàÎäî Ï†ÑÎ¨∏ ÎßàÏºÄÌåÖ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∞ÄÏïº.
    Ï£ºÏñ¥ÏßÑ ROAS Îç∞Ïù¥ÌÑ∞ÏôÄ ÌçºÌè¨Î®ºÏä§ÌåÄÏùò ÏõêÎ¨∏ Î¶¨Ìè¨Ìä∏Î•º **Ï†àÎåÄ Ïò§Î•ò ÏóÜÏù¥ Î∂ÑÏÑù**ÌïòÍ≥†, ÏöîÏ≤≠Îêú **Î™®Îì† Ï∂úÎ†• ÌòïÏãù Í∑úÏπô**ÏùÑ ÏóÑÍ≤©ÌïòÍ≤å Ï§ÄÏàòÌïòÏó¨ Î¶¨Ìè¨Ìä∏Î•º ÏûëÏÑ±Ìï¥ÏïºÌï¥.

    [Îç∞Ïù¥ÌÑ∞ Ï†ïÌï©ÏÑ± ÏµúÏö∞ÏÑ† Í∑úÏπô]
    1. Î™®Îì† ÏàòÏπò ÎπÑÍµê (BEP Îã¨ÏÑ±, Ï¶ùÍ∞êÎ•† Í≥ÑÏÇ∞)Îäî Ïò§ÏßÅ Ï†úÍ≥µÎêú ÌÖåÏù¥Î∏î Îç∞Ïù¥ÌÑ∞ÎßåÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏàòÌñâÌï¥
    2. ÌÖåÏù¥Î∏îÏóê ÏóÜÎäî Îç∞Ïù¥ÌÑ∞ÎÇò Ï∂îÎ°†ÏùÄ ÏóÑÍ∏àÌïòÎ©∞, ÎπÑÍµê ÎåÄÏÉÅÏùÄ ÎèôÏùºÌïú Í≤åÏûÑ ÎÇ¥ÏóêÏÑú ÏÑúÎ°ú Îã§Î•∏ ÏãúÏ†ê(Ïõî)Ïùò ÎèôÏùºÌïú ÏßÄÌëú(Ïó¥)Ïù¥Ïïº

    [ÌëúÍ∏∞Î≤ï Í∑úÏπô]
    - cost, install ru, CPI, cpruÎäî Ï≤úÎã®ÏúÑ ÏâºÌëú(,)Î•º ÏÇ¨Ïö©
    - ROAS Í¥ÄÎ†® ÏßÄÌëúÎäî ÏÜåÏàòÏ†ê Ï≤´Ïß∏ ÏûêÎ¶¨ÍπåÏßÄ ÌëúÍ∏∞ÌïòÍ≥† '%' Îã®ÏúÑÎ•º ÏÇ¨Ïö©
    - Ï¶ùÍ∞êÎ•†ÏùÑ Ïù¥ÏïºÍ∏∞Ìï† ÎïåÎäî +- Í∏∞Ìò∏ ÎåÄÏã† üî∫(ÏÉÅÏäπ) ÎòêÎäî üîª(ÌïòÎùΩ) Í∏∞Ìò∏Î•º Ïà´ÏûêÏïûÏóê ÏÇ¨Ïö©Ìï¥Ï§ò
    - Î≥ÄÏàòÎ™ÖÏóê ÎåÄÌïú Ïñ∏Í∏â Ï†úÏô∏

    [Ï∂úÎ†•ÌòïÏãù Í∑úÏπô]
    - ÎßàÌÅ¨Îã§Ïö¥ Ìè¨Îß∑: ÎÖ∏ÏÖò ÎßàÌÅ¨Îã§Ïö¥ Ìè¨Îß∑ÏùÑ ÏÇ¨Ïö©Ìï¥
    - Î¶¨Ìè¨Ìä∏ ÏûëÏÑ± ÏôÑÎ£åÌñàÎã§Îäî ÎÇ¥Ïö©ÏùÄ Î≥ÑÎèÑÎ°ú Ïñ∏Í∏âÌïòÏßÄÎßà

    """
    # 2. Îç∞Ïù¥ÌÑ∞ ÌïôÏäµ
    prompt_description_3_optimized = f"""
    ## Îç∞Ïù¥ÌÑ∞ ÏûÖÎ†•
    SLG Í≤åÏûÑ 4Ï¢ÖÏùò ÏõîÎ≥Ñ ROAS ÌòÑÌô© Îç∞Ïù¥ÌÑ∞ Î∞è Ï£ºÏöî ÏßÄÌëú ÏÑ§Î™ÖÏù¥Ïïº

    [Ï£ºÏöî Î≥ÄÏàò ÏÑ§Î™Ö]
    - cost_exclude_credit: ÌÅ¨Î†àÎîß Ï†úÏô∏ ÏõîÎ≥Ñ ÎßàÏºÄÌåÖÎπÑ (roas Í≥ÑÏÇ∞ Í∏∞Ï§Ä, ÎãπÏõîÏùÄ ÏùºÌï†Í≥ÑÏÇ∞ Ï∂îÏ†ï Ïõî cost)
    - cpru: Îã®Í∞Ä
    - d360roas_growth: Î≥µÍ∑ÄÏú†Ï†Ä ÎØ∏Ìè¨Ìï® Ïã†Í∑úÏú†Ï†Ä d360 ÏòàÏ∏°Ïπò
    - d360roas_growth_plus_return_expected: Î≥µÍ∑ÄÏú†Ï†Ä Ìè¨Ìï® d360 ÏòàÏ∏°Ïπò
    - bep_commission: ÏàòÏàòÎ£å Í≥†Î†§Ìïú BEP

    --- ROAS Îç∞Ïù¥ÌÑ∞ ÌÖåÏù¥Î∏î ---
    Ïã§Ï†ú ÏõîÎ≥Ñ ÎßàÏºÄÌåÖ Îç∞Ïù¥ÌÑ∞Ïïº
    ÏßÄÌëúÏóê ÎåÄÌï¥ Ïñ∏Í∏âÌï† ÎïåÎäî Ìï¥Îãπ ÌÖåÏù¥Î∏î ÏàòÏπòÎßåÏùÑ ÏÇ¨Ïö©Ìï¥
    {recent_data_md}

    --- ÌçºÌè¨Î®ºÏä§ÌåÄ Ï£ºÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÏõêÎ¨∏ ÎÇ¥Ïö© ---
    ÌçºÌè¨Î®ºÏä§ÌåÄÏóêÏÑú ÏûëÏÑ±Ìïú Ï£ºÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÎÇ¥Ïö©Ïù¥Ïïº
    ÎãπÏõîÏùò Ï£ºÏöî Ïù¥ÏäàÏÇ¨Ìï≠ÏùÑ ÏïÑÎûò Î¶¨Ìè¨Ìä∏ ÎÇ¥Ïö©ÏùÑ Ï∞∏Í≥† Ìï¥
    {weekly_report_db3}
    """

    prompt_parts_3_final = [
        prompt_description_3_optimized,
        """
    ### ÎßàÏºÄÌåÖ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏûëÏÑ± ÏöîÏ≤≠

    Ï£ºÏñ¥ÏßÑ ROAS Îç∞Ïù¥ÌÑ∞ Î∞è ÏõêÎ¨∏ Î¶¨Ìè¨Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Îã§ÏùåÏùò 4Í∞ÄÏßÄ Ìï≠Î™©Ïóê ÎåÄÌï¥ Í≤åÏûÑÎ≥Ñ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏Î•º ÏûëÏÑ±Ìï¥Ï§ò

    1. BEP Îã¨ÏÑ± Ïó¨Î∂Ä ÌåêÎã® Í∞ÄÏû• ÏµúÍ∑ºÏõîÏùò Î≥µÍ∑ÄÏú†Ï†Ä Ìè¨Ìï® D360 ROASÏôÄ ÏàòÏàòÎ£å Í≥†Î†§ BEP(bep_commission)Î•º ÎπÑÍµêÌïòÏó¨ Îã¨ÏÑ± Ïó¨Î∂ÄÎ•º Î™ÖÌôïÌûà ÌåêÎã®Ìï¥
    2. Ïã†Í∑ú Ïú†Ï†Ä ROAS BEP Ï¥àÍ≥ºÌïú ÏµúÏÜå Cohort Î∂ÑÏÑù: Í∞ÄÏû• ÏµúÍ∑ºÏõî Î≥µÍ∑ÄÏú†Ï†Ä Í≥†Î†§ÌïòÏßÄ ÏïäÏùÄ Ïã†Í∑úÏú†Ï†Ä d360 ROAS(d360roas_growth)Í∞Ä BEPÎ•º Ï¥àÍ≥ºÌïú Í≤ΩÏö∞ÏóêÎäîd90roas_growthÎ∂ÄÌÑ∞ d360roas_growthÍπåÏßÄÏùò cohort Ï§ë BEP(bep_commission) Ïù¥ÏÉÅÏù∏ ÏµúÏÜå cohort dnÏùÑ Ïñ∏Í∏âÌï¥Ï§ò. Îã®, d360roas_growthÍ∞Ä BEPÎ•º Ï¥àÍ≥ºÌïòÏßÄ ÏïäÏïòÎã§Î©¥ Ïù¥ Ìï≠Î™©ÏùÄ Ïñ∏Í∏âÌïòÏßÄ Îßà
    3. Ï†ÑÏõî ÎåÄÎπÑ Ï¶ùÍ∞êÎ•† Í≥ÑÏÇ∞: ÎπÑÏö©(cost_exclude_credit), Îã®Í∞Ä(cpru), Î≥µÍ∑ÄÏú†Ï†Ä Ìè¨Ìï® ROAS, Ïã†Í∑úÏú†Ï†Ä ROASÏùò Ï†ÑÏõî ÎåÄÎπÑ Ï¶ùÍ∞êÎ•†ÏùÑ Í≥ÑÏÇ∞Ìï¥. Ï†ÑÏõî ÏàòÏπòÏôÄ ÎãπÏõî ÏàòÏπòÎèÑ Ìï®Íªò ÌëúÍ∏∞ÌïòÍ≥†, ÌäπÏù¥Ï†êÏùÑ Í∞ÑÍ≤∞ÌïòÍ≤å Ïñ∏Í∏âÌï¥
    4. Ï£ºÏöî Ïù¥Ïäà Ï†ïÎ¶¨: ÌçºÌè¨Î®ºÏä§ÌåÄÏùò Ï£ºÍ∞Ñ Î¶¨Ìè¨Ìä∏ ÏõêÎ¨∏ ÎÇ¥Ïö©ÏùÑ Ï∞∏Í≥†Ìï¥ÏÑú ÎãπÏõîÏùò Ï£ºÏöî Ïù¥ÏäàÎÇò ÌûàÏä§ÌÜ†Î¶¨Î•º Ï†ïÎ¶¨Ìï¥ÏÑú Ï∂îÍ∞ÄÌï¥Ï§ò

    ÏûëÏÑ± Ïãú ÏïÑÎûòÏùò ÌòïÌÉúÎ•º ÏßÄÏºúÏÑú ÏûëÏÑ± Î∂ÄÌÉÅÌï¥

    1. **BEP Îã¨ÏÑ± Ïó¨Î∂Ä:** ‚Ä¶
    2. **Ïã†Í∑ú Ïú†Ï†Ä ROAS BEP Ï¥àÍ≥º Ïó¨Î∂Ä:** ‚Ä¶
    3. **Ï†ÑÏõî ÎåÄÎπÑ Ï¶ùÍ∞êÎ•†:**
        - ÎπÑÏö©: ‚Ä¶
        - Îã®Í∞Ä: ‚Ä¶
        - Î≥µÍ∑ÄÏú†Ï†Ä Ìè¨Ìï® ROAS: ‚Ä¶
        - Ïã†Í∑úÏú†Ï†Ä ROAS: ‚Ä¶
        - ÌäπÏù¥Ï†ê : ...
    4. **Ï£ºÏöî Ïù¥Ïäà:** ‚Ä¶
    """]

    genai_client = GeminiClient(
<<<<<<< HEAD
        vertexai=True,
        location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
=======
    vertexai=True,
    location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response5 = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI Î™®Îç∏Î™Ö
        contents = prompt_parts_3_final
        ,config=config_3_optimized
    )
    print(response5.text)

    # Notion API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
    notion = NotionClient(auth=NOTION_TOKEN, notion_version=NOTION_VERSION)

    title_prop: str = "Ïù¥Î¶Ñ"
<<<<<<< HEAD
    page_title = f"SLG ÏõîÎ≥Ñ ÎßàÏºÄÌåÖ ÌòÑÌô© Î¶¨Î∑∞_{now_kst.strftime('%y%m%d')}"
=======
    page_title = f"SLG ÏõîÎ≥Ñ ÎßàÏºÄÌåÖ ÌòÑÌô© Î¶¨Î∑∞_{datetime.today().strftime('%y%m%d')}"
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    project_list = ["GBTW","POTC","DRSG","WWM"]


    # DB ÏÜçÏÑ± Íµ¨ÏÑ±
    props = {
        title_prop: {"title": [{"text": {"content": page_title}}]},
<<<<<<< HEAD
        "Îì±Î°ù ÎÇ†Ïßú": {"date": {"start": now_kst.isoformat()}},
=======
        "Îì±Î°ù ÎÇ†Ïßú": {"date": {"start": datetime.today().isoformat()}},
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

        # 'ÌîÑÎ°úÏ†ùÌä∏' ÏÜçÏÑ± (Rich Text ÎòêÎäî Select)
        "ÌîÑÎ°úÏ†ùÌä∏": {"multi_select": [{"name": project} for project in project_list  ] },
        "Î¶¨Ìè¨Ìä∏ Ï¢ÖÎ•ò": {"multi_select": [{"name": "ÎßàÏºÄÌåÖÎ∂ÑÏÑù"}]}}
    #  if author_person_id:
    #      props["ÏûëÏÑ±Ïûê"] = {"people": [{"id": author_person_id}]}


    # if ref_person_id:
    #     props["Ï∞∏Ï°∞Ïûê"] = {"people": [{"id": ref_person_id}]}

    # ÎÇòÏú§ ÌÖåÏä§Ìä∏ Î≤†Ïù¥Ïä§
    NOTION_DATABASE_ID = "2ccea67a56818069b6abc52e5b5ca372"

    # ÌéòÏù¥ÏßÄ ÏÉùÏÑ±
    new_page = notion.pages.create(
        parent={"database_id": NOTION_DATABASE_ID},
    #   properties=props
    )

    # ÏÉùÏÑ±Îêú ÌéòÏù¥ÏßÄ ID Í∞ÄÏ†∏Ïò§Í∏∞
    PAGE_ID = new_page["id"]

    # ÏÉùÏÑ±Îêú ÌéòÏù¥ÏßÄ URL Ï∂úÎ†•
    print("‚úÖ Notion ÌéòÏù¥ÏßÄ ÏÉùÏÑ± ÏôÑÎ£å:", new_page["url"])
    print("üÜî ÏÉùÏÑ±Îêú ÌéòÏù¥ÏßÄ ID:", PAGE_ID)

<<<<<<< HEAD

=======
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ############## Ïù¥ÎØ∏ÏßÄ ÏóÖÎ°úÎìú##############

    import os, time, json, requests
    from pathlib import Path


    # ÏóÖÎ°úÎìúÏö© Í≤ΩÎ°ú Î≥ÄÏàò
<<<<<<< HEAD
    IMG_PATH = Path(roas_graph_path).resolve()  # Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î≥ÄÌôò(Í∂åÏû•)
=======
    IMG_PATH = Path("roas_graph.png").resolve()  # Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î≥ÄÌôò(Í∂åÏû•)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    assert IMG_PATH.exists() and IMG_PATH.stat().st_size > 0

    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # 1) ÏóÖÎ°úÎìú Ïò§Î∏åÏ†ùÌä∏ ÏÉùÏÑ± (upload_url Î∞õÍ∏∞)
    resp = requests.post(
        "https://api.notion.com/v1/file_uploads",
        json={"filename": IMG_PATH.name, "content_type": "image/png"},
        headers=hdr_json
    )
    resp.raise_for_status()
    fu = resp.json()
    file_upload_id = fu["id"]
    upload_url = fu.get("upload_url")


    # 2) Ïã§Ï†ú Ï†ÑÏÜ° (multipart/form-data) ‚áí status Í∞Ä uploaded Ïó¨Ïïº Ï≤®Î∂Ä Í∞ÄÎä•
    with open(IMG_PATH, "rb") as f:
        r2 = requests.post(
            # Í∂åÏû•: Î™ÖÏãúÏ†Å send ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÇ¨Ïö©
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
            # ÎòêÎäî upload_url ÏÇ¨Ïö© Í∞ÄÎä• (ÎèôÏùº ÎèôÏûë): upload_url,
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
            files={"file": (IMG_PATH.name, f, "image/png")}
        )
    r2.raise_for_status()


    # (ÏòµÏÖò) ÏÉÅÌÉú ÌôïÏù∏ Î∞è Ìè¥ÎßÅ: uploaded Îê† ÎïåÍπåÏßÄ Ïû†Íπê ÎåÄÍ∏∞
    for _ in range(10):
        r_chk = requests.get(
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}",
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
        )
        r_chk.raise_for_status()
        status = r_chk.json().get("status")
        if status == "uploaded":
            break
        time.sleep(0.4)
    assert status == "uploaded", f"ÏóÖÎ°úÎìú ÏÉÅÌÉúÍ∞Ä {status} ÏûÖÎãàÎã§. (uploaded Ïó¨Ïïº Ï≤®Î∂Ä Í∞ÄÎä•)"

    ## Î≥∏Î¨∏ Í∏∞Ï°¥
    ########### (1) Ï†úÎ™©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "1) Ï†ÑÏ≤¥ Ïú†Ï†Ä ROAS ÌòÑÌô©" }}]
                },
            }
        ],
    )

<<<<<<< HEAD
=======


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ########### (2) Í∑∏ÎûòÌîÑ Ï≤®Î∂Ä
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    "caption": [
                        {"type": "text", "text": {"content": "ÎãπÏõîÏùò COSTÎäî ÏßëÍ≥ÑÍ∏∞Í∞ÑÎÇ¥ COST Í∏∞Î∞ò ÏùºÌï† Í≥ÑÏÇ∞Îêú Ï∂îÏ†ï ÎãπÏõî ÏÜåÏßÑ COSTÏûÖÎãàÎã§."}}
                    ]
                },
            }
        ]
    )

<<<<<<< HEAD
    ########### (3) Ìëú Ï≤®Î∂Ä
    # ÏóÖÎ°úÎìúÏö© Í≤ΩÎ°ú
    ########### (3) Ìëú Ï≤®Î∂Ä
    # ÏóÖÎ°úÎìúÏö© Í≤ΩÎ°ú (temp_dir ÏÇ¨Ïö©)
    IMG_PATHS = [
        Path(os.path.join(temp_dir, "1.POTC_roas_table.png")).resolve(),
        Path(os.path.join(temp_dir, "2.GBTW_roas_table.png")).resolve(),
        Path(os.path.join(temp_dir, "3.WWMC_roas_table.png")).resolve(),
        Path(os.path.join(temp_dir, "4.DRSG_roas_table.png")).resolve(),
=======


    ########### (3) Ìëú Ï≤®Î∂Ä

    # ÏóÖÎ°úÎìúÏö© Í≤ΩÎ°ú
    IMG_PATHS = [
        Path("1.POTC_roas_table.png").resolve(),
        Path("2.GBTW_roas_table.png").resolve(),
        Path("3.WWMC_roas_table.png").resolve(),
        Path("4.DRSG_roas_table.png").resolve(),
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ]


    # ÏóÖÎ°úÎìú Ï≤¥ÌÅ¨ Î∞è Ìó§Îçî
    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # ÏóÖÎ°úÎìúÎêú ÌååÏùºÎì§Ïóê ÎåÄÌïú ID Î™©Î°ù
    file_upload_ids = []

    # 1) Í∞Å Ïù¥ÎØ∏ÏßÄ ÌååÏùºÏóê ÎåÄÌï¥ ÏóÖÎ°úÎìú Ï≤òÎ¶¨
    for img_path in IMG_PATHS:
        assert img_path.exists() and img_path.stat().st_size > 0  # ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÍ≥† ÌÅ¨Í∏∞Í∞Ä 0Î≥¥Îã§ Ïª§ÏïºÌï®

        # ÏóÖÎ°úÎìú Ïò§Î∏åÏ†ùÌä∏ ÏÉùÏÑ± (upload_url Î∞õÍ∏∞)
        resp = requests.post(
            "https://api.notion.com/v1/file_uploads",
            json={"filename": img_path.name, "content_type": "image/png"},
            headers=hdr_json
        )
        resp.raise_for_status()
        fu = resp.json()
        file_upload_id = fu["id"]
        upload_url = fu.get("upload_url")

        # 2) Ïã§Ï†ú ÌååÏùº Ï†ÑÏÜ° (multipart/form-data)
        with open(img_path, "rb") as f:
            r2 = requests.post(
                f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
                headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
                files={"file": (img_path.name, f, "image/png")}
            )
        r2.raise_for_status()

        # (ÏòµÏÖò) ÏÉÅÌÉú ÌôïÏù∏ Î∞è Ìè¥ÎßÅ: uploaded Îê† ÎïåÍπåÏßÄ ÎåÄÍ∏∞
        for _ in range(10):
            r_chk = requests.get(
                f"https://api.notion.com/v1/file_uploads/{file_upload_id}",
                headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION}
            )
            r_chk.raise_for_status()
            status = r_chk.json().get("status")
            if status == "uploaded":
                break
            time.sleep(0.4)

        assert status == "uploaded", f"ÏóÖÎ°úÎìú ÏÉÅÌÉúÍ∞Ä {status} ÏûÖÎãàÎã§. (uploaded Ïó¨Ïïº Ï≤®Î∂Ä Í∞ÄÎä•)"

        # ÏóÖÎ°úÎìúÎêú ÌååÏùº ID Ï†ÄÏû•
        file_upload_ids.append(file_upload_id)

    # 3) ÌÜ†Í∏Ä Î∏îÎ°ù ÏÉùÏÑ± (ÏÉÅÏÑ∏ ROAS Ìëú)
    toggle_block = notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"type": "text", "text": {"content": "ÏÉÅÏÑ∏ ROAS Ìëú(ÌÅ¥Î¶≠)"}}],
                    "children": []  # ÏùºÎã® ÎπÑÏõåÎë†
                }
            }
        ]
    )

    toggle_id = toggle_block["results"][0]["id"]

    # 4) Ïù¥ÎØ∏ÏßÄÏôÄ ÎèÑÌëú Ï∂îÍ∞ÄÌïòÎäî Î∞©ÏãùÏúºÎ°ú Î≥ÄÍ≤Ω
    for img_path, file_upload_id in zip(IMG_PATHS, file_upload_ids):
    # Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä Î°úÏßÅ
        notion.blocks.children.append(
            toggle_id,
            children=[
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        "caption": [
                            {
                                "type": "text",
                                "text": {
                                    "content": "ROAS Ìëú - ÎãπÏõîÏùò COSTÎäî ÏßëÍ≥ÑÍ∏∞Í∞ÑÎÇ¥ COST Í∏∞Î∞ò ÏùºÌï† Í≥ÑÏÇ∞Îêú Ï∂îÏ†ï ÎãπÏõî ÏÜåÏßÑ COSTÏûÖÎãàÎã§."
                                }
                            }
                        ]
                    }
                }
            ]
        )

        # Ïù¥ÎØ∏ÏßÄÌååÏùºÏùò ÏïûÍ∏ÄÏûê Í∏∞Ï§Ä Game Îß§Ïπ≠
        game_key = img_path.name[:6]
        raw_df = raw_df_by_game[game_key]

        # Table RAW ÏóÖÎç∞Ïù¥Ìä∏
        raw_toggle = notion.blocks.children.append(
            toggle_id,
            children=[
                {
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [
                            {"type": "text", "text": {"content": f"Table_RAW ({game_key})"}}
                        ],
                        "children": [
                            {
                                "object": "block",
                                "type": "table",
                                "table": {
                                    "table_width": len(raw_df.columns),
                                    "has_column_header": True,
                                    "has_row_header": False,
                                    "children": df_to_table_rows(raw_df, max_rows=100)
                                }
                            }
                        ]
                    }
                }
            ]
        )

    ########### (4) geminiÎ∂ÑÏÑù ÎÇ¥Ïö© Ï≤®Î∂Ä
    ##(NYÏàòÏ†ï) ÎÖ∏ÏÖòÏóê ÏóÖÎç∞Ïù¥Ìä∏ ÎêòÎäî Í∏ÄÏûêÍ∞Ä 100Ï§ÑÏùÑ ÎÑòÏúºÎ©¥ ÏóêÎü¨Í∞Ä Î∞úÏÉùÌï®. Ïù¥Î∂ÄÎ∂Ñ ÎÅäÏñ¥ÏÑú Í∞à Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Î°úÏßÅ Ï∂îÍ∞ÄÌï®

    # 1) Gemini Í≤∞Í≥º ‚Üí Notion Î∏îÎ°ù Î≥ÄÌôò
    blocks = md_to_notion_blocks(response5.text + "\n\n\n")

    # 2) Notion APIÏùò children ‚â§ 100 Ï†úÌïú Ìï¥Í≤∞ ‚Üí 100Í∞úÏî© ÎÇòÎà† ÎÑ£Í∏∞
    def chunk_list(lst, size=100):
        for i in range(0, len(lst), size):
            yield lst[i:i+size]

    # 3) 100Í∞úÏî© append
    for chunk in chunk_list(blocks, 100):
        notion.blocks.children.append(
            block_id=PAGE_ID,
            children=chunk
        )

    print("‚úÖ Append ÏôÑÎ£å")

<<<<<<< HEAD
    from datetime import datetime, timezone, timedelta
=======
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_2_os_roas"} ## ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î∂ôÏùº Ïàò ÏûàÏùå.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

    ### 1> os Î≥Ñ
    client = bigquery.Client()
<<<<<<< HEAD
    query = f"""
=======
    query = """
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    WITH revraw AS(
    select  JoypleGameID, regmonth, osuser
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as regmonth
    ,  osuser ,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
        , case when OS = 'android' then 'And' when OS = 'ios' then 'IOS' else OS end as osuser
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
<<<<<<< HEAD
        and RegdateAuthAccountDateKST between '{now_kst.year}-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
=======
        and RegdateAuthAccountDateKST between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )
    group by JoypleGameID, RegdateAuthAccountDateKST,  osuser
    ) group by JoypleGameID, regmonth,  osuser
    )




    , cost_raw AS(
    select joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) as regmonth   , os
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit,
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and cmpgndate between '{now_kst.year}-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
=======
    and cmpgndate between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )
    group by  joyplegameid,gameid,  format_date('%Y-%m', cmpgndate)  , os
    )



    select
    ifnull(a.joyplegameid , b.joyplegameid) as joyplegameid
    ,ifnull(a.regmonth , b.regmonth) as regmonth
    , ifnull(a.osuser, b.os) as os
    , a.ru
    ,a.sales_D1, a.sales_D3, a.sales_D7, a.sales_D14, a.sales_D30, a.sales_D60, a.sales_D90 , a.sales_D120, a.sales_D150, a.sales_D180
    , a.sales_D210, a.sales_D240, a.sales_D270, a.sales_D300, a.sales_D330, a.sales_D360
    , b.cost, b.cost_exclude_credit
    from revraw as a
    full join cost_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.osuser = b.os

    """
<<<<<<< HEAD

    query_result_raw_os = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

=======
    query_result_raw_os = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()


>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ############# OS Ï†ÑÏ≤òÎ¶¨
    # ROAS Í≥ÑÏÇ∞ (sales_d1 / cost, sales_d3 / cost, ..., sales_d360 / cost)
    cohort_columns = ['D1', 'D3', 'D7', 'D14', 'D30', 'D60', 'D90', 'D120', 'D150', 'D180', 'D210', 'D240', 'D270', 'D300', 'D330', 'D360']

    # Í∞Å cohortÏóê ÎåÄÌï¥ ROAS Í≥ÑÏÇ∞
    for cohort in cohort_columns:
        query_result_raw_os[f'{cohort.lower()}_roas'] = query_result_raw_os[f'sales_{cohort}'] / query_result_raw_os['cost_exclude_credit']


    query_result_raw_os['cpru'] = query_result_raw_os['cost_exclude_credit'] / query_result_raw_os['ru']

    # osÎ≥Ñ cost ÎπÑÏú® Í≥ÑÏÇ∞
    game_total_cost = query_result_raw_os.groupby(['joyplegameid', 'regmonth'])['cost_exclude_credit'].sum().reset_index(name='game_total_cost')

    # merge os_total_cost into query_result_raw2
    query_result_raw_os2 = pd.merge(query_result_raw_os, game_total_cost, on=['joyplegameid', 'regmonth'], how='left')

    query_result_raw_os2['os_cost_ratio'] = query_result_raw_os2['cost_exclude_credit'] / query_result_raw_os2['game_total_cost']

    mapping = {131:   "1.POTC", 133:   "2.GBTW", 30001: "3.WWMC", 30003: "4.DRSG",} # (NYÏàòÏ†ï) Ïú†Ïã§Îêú mapping Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
    query_result_raw_os2["game_name"] = query_result_raw_os2["joyplegameid"].map(mapping)  # Îß§Ìïë ÏóÜÏúºÎ©¥ NaN
    query_result_raw_os2 = query_result_raw_os2[query_result_raw_os2['os'].isin(['And', 'IOS'])]

    select_cols = ['game_name' ,'regmonth','os','cost_exclude_credit','os_cost_ratio','d1_roas','d3_roas','d7_roas','d14_roas','d30_roas','d60_roas','d90_roas','d120_roas','d150_roas']
    query_result_raw_os3 = query_result_raw_os2[select_cols]

    # ÌÖåÏù¥Î∏î ÌòïÌÉúÎ°ú Î¨∏ÏûêÏó¥Ìôî (Markdown table)
    def df_to_md(df):
        header = "| " + " | ".join(df.columns) + " |"
        sep = "| " + " | ".join(["---"]*len(df.columns)) + " |"
        rows = "\n".join("| " + " | ".join(map(str, row)) + " |" for row in df.values)
        return "\n".join([header, sep, rows])

    df_os = df_to_md(query_result_raw_os3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(12))

    # Îç∞Ïù¥ÌÑ∞ ÏûÖÎ†•
    prompt_description = f"""
    ## Îç∞Ïù¥ÌÑ∞ ÏûÖÎ†•
    SLG Í≤åÏûÑ 4Ï¢ÖÏùò ÏõîÎ≥Ñ OSÎ≥Ñ ROAS ÌòÑÌô©ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Îç∞Ïù¥ÌÑ∞Îì§Ïù¥Ïïº.
    Í∞íÏù¥ NAÏù∏ CohortÎ≥ÄÏàò(dn_roas)Îäî ÏïÑÏßÅ matureÎêòÏßÄ ÏïäÏùÄ ÏßÄÌëúÏïº.

    --- OSÎ≥Ñ ROAS Îç∞Ïù¥ÌÑ∞ ÌÖåÏù¥Î∏î ---
    {df_os}

    """

    # GeminiÏóê Ï†ÑÎã¨Ìï† Ï†ÑÏ≤¥ ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
    prompt_parts = [
        prompt_description,
        """
    ### OSÎ≥Ñ ÎßàÏºÄÌåÖ Ìä∏Î†åÎìú Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏûëÏÑ± ÏöîÏ≤≠

    Ï£ºÏñ¥ÏßÑ OSÎ≥Ñ ROAS Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Í≤åÏûÑÎ≥ÑÎ°ú Îã§Ïùå 3Í∞ÄÏßÄ Ìï≠Î™©Ïóê ÎåÄÌï¥ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏Î•º ÏûëÏÑ±Ìï¥Ï§ò.

    1. **ÏõîÎ≥Ñ OS Ìä∏Î†åÎìú:** Í≤åÏûÑÎ≥ÑÎ°ú ÏµúÍ∑º 3Í∞úÏõîÏùò ÏõîÎ≥Ñ iOS, AndroidÏùò Cost ÎπÑÏ§ë, ROAS ÏÑ±Í≥ºÍ∞Ä Ï¶ùÍ∞ÄÌïòÎäîÏßÄ Í∞êÏÜåÌïòÎäîÏßÄ Ìä∏Î†åÎìúÎ•º Ïñ∏Í∏âÌï¥Ï§ò.
    2. **OSÎ≥Ñ ROAS ÎπÑÍµê (Cohort Ìä∏Î†åÎìú):** Ï¥àÎ∞ò Cohort (d7, d14 Îì±)ÏôÄ Ïû•Í∏∞ Cohort (d90, d150 Îì±)Ïùò ROASÎ•º ÎπÑÍµêÌïòÏó¨ OS Í∞Ñ Ï∞®Ïù¥Í∞Ä Ïñ¥ÎñªÍ≤å ÎÇòÌÉÄÎÇòÎäîÏßÄ (Ïòà: 'Ï¥àÎ∞òÏùÄ AndroidÍ∞Ä ÎÜíÏúºÎÇò Ïû•Í∏∞ ÏΩîÌò∏Ìä∏Îäî iOSÍ∞Ä ÎÜíÎã§') ÏÉÅÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï§ò.
    3. **Í∞ÄÏû• ÏµúÍ∑ºÏõî(ÎãπÏõî) ÌòÑÌô©:**
        - Í∞ÄÏû• ÏµúÍ∑ºÏõîÏùò OS Cost ÎπÑÏ§ë ÌòÑÌô©ÏùÑ Ïñ∏Í∏âÌï¥Ï§ò.
        - Í∞ÄÏû• ÏµúÍ∑ºÏõîÏùò OSÎ≥Ñ ROASÍ∞Ä Android ÎåÄÎπÑ iOSÍ∞Ä ÎÜíÏùÄÏßÄ ÎÇÆÏùÄÏßÄ Î™ÖÌôïÌûà Ïñ∏Í∏âÌï¥Ï§ò.

    ÏûëÏÑ± Ïãú ÏïÑÎûòÏùò ÏòàÏãú Ï∂úÎ†• ÌòïÌÉúÎ•º Ï∞∏Í≥†ÌïòÏó¨ ÏûëÏÑ± Î∂ÄÌÉÅÌï¥.

    --- ÏòàÏãú Ï∂úÎ†• ÌòïÌÉú (ÎßàÌÅ¨Îã§Ïö¥ Ìè¨Îß∑) ---

    ## Í≤åÏûÑÎ™Ö (Ïòà: ## 1.POTC)

    1. **ÏõîÎ≥Ñ OS Ìä∏Î†åÎìú:** iOS Cost ÎπÑÏ§ëÏùÄ XÏõîÎ∂ÄÌÑ∞ YÏõîÍπåÏßÄ üî∫ÏÉÅÏäπ(ÎòêÎäî üîªÌïòÎùΩ)ÌïòÎäî Ï∂îÏÑ∏Ïù¥Î©∞, iOS or AndroidÏùòROAS Í∞Ä XÏõîÎ∂ÄÌÑ∞ Í∞úÏÑ†ÎêòÎäî ÌùêÎ¶ÑÏùÑ Î≥¥ÏûÑ
    2. **OSÎ≥Ñ ROAS Cohort Ìä∏Î†åÎìú:** Ï¥àÎ∞ò Cohort(d7~d30)Ïùò ROASÎäî AndroidÍ∞Ä iOS ÎåÄÎπÑ ÌèâÍ∑†Ï†ÅÏúºÎ°ú ÎÜíÏúºÎÇò, Ïû•Í∏∞ Cohort(d90~)Î°ú Í∞àÏàòÎ°ù iOSÍ∞Ä Android ÎåÄÎπÑ ÎÜíÏùÄ ROASÎ•º Î≥¥ÏûÑ.
    3. **Í∞ÄÏû• ÏµúÍ∑ºÏõî ÌòÑÌô©:**
        - Cost ÎπÑÏ§ë: ÎãπÏõî iOS Cost ÎπÑÏ§ëÏùÄ 00%ÏûÑ.
        - ROAS ÎπÑÍµê: ÎãπÏõî Ï¥àÎ∞ò Cohort ROASÎäî Android ÎåÄÎπÑ iOSÍ∞Ä ÎÇÆÍ≥†, Ïû•Í∏∞ Cohort ROASÎäî Android ÎåÄÎπÑ iOSÍ∞Ä ÎÜíÏùå.

    """
    ]
<<<<<<< HEAD
    # Gemini API Ìò∏Ï∂ú (ÏÇ¨Ïö©ÏûêÏùò ÏõêÎûò Ìò∏Ï∂ú Î∞©Ïãù)
    # response_os = model.generate_content(prompt_parts, labels=LABELS)
    # print(response_os.text)

    genai_client = GeminiClient(
        vertexai=True,
        location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
=======

    genai_client = GeminiClient(
    vertexai=True,
    location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response_os = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI Î™®Îç∏Î™Ö
        contents = prompt_parts
        ,config=config_3_optimized
    )
    print(response_os.text)

    pivot_df = query_result_raw_os3.pivot_table(
<<<<<<< HEAD
        index=['game_name', 'regmonth'] ,
        columns='os',
        values=['cost_exclude_credit', 'os_cost_ratio',  'd7_roas', 'd14_roas', 'd30_roas'],
        aggfunc='first'
=======
    index=['game_name', 'regmonth'] ,
    columns='os',
    values=['cost_exclude_credit', 'os_cost_ratio',  'd7_roas', 'd14_roas', 'd30_roas'],
    aggfunc='first'
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )

    '''
    pivot_df = pivot_df[['total_cost', 'os_cost_ratio', 'd7_roas', 'd14_roas', 'd30_roas']]

    nest_asyncio.apply()


    styled_df = pivot_df.style \
        .format({
            "total_cost": "{:,.0f}",  # total_costÎ•º Ï≤ú Îã®ÏúÑÎ°ú Ìè¨Îß∑
            **{col: "{:.1%}" for col in ['os_cost_ratio', 'd7_roas', 'd14_roas', 'd30_roas']}  # ÎπÑÏú® Ìè¨Îß∑
        }) \
        .bar(subset=['total_cost'], color='#fbe4e6') \
        .bar(subset=['d7_roas', 'd14_roas', 'd30_roas'], color='#b6d7a8') \
        .background_gradient(subset=['os_cost_ratio'], cmap='Reds')  # os_cost_ratioÏóê Îπ®Í∞ÑÏÉâ Í∑∏ÎùºÎç∞Ïù¥ÏÖò
    '''

    nest_asyncio.apply()

    # Ïª¨Îüº Ïù¥Î¶ÑÏùÑ Îã®Ïùº Ïù∏Îç±Ïä§Î°ú ÌèâÌÉÑÌôî
    pivot_df.columns = [f'{col}_{idx}' for col, idx in pivot_df.columns]

    pivot_df = pivot_df[['cost_exclude_credit_And', 'cost_exclude_credit_IOS', 'os_cost_ratio_IOS'
    ,'d7_roas_And', 'd7_roas_IOS', 'd14_roas_And', 'd14_roas_IOS', 'd30_roas_And', 'd30_roas_IOS']]


    '''
    # Í∞Å ÏõîÎ≥ÑÎ°ú And vs iOS Ï§ë Îçî ÌÅ∞ Í∞íÏùÑ Îπ®Í∞ÑÏÉâÏúºÎ°ú ÌëúÏãú
    def highlight_max(s):
        is_max = s == s.max()
        return ['color: red; font-weight: bold' if v else '' for v in is_max]



    # formatÍ≥º Ïä§ÌÉÄÏùº Ï†ÅÏö©
    styled_df = pivot_df.style \
        .format({
            "cost_exclude_credit_And": "{:,.0f}",  # total_costÎ•º Ï≤ú Îã®ÏúÑÎ°ú Ìè¨Îß∑
                    "cost_exclude_credit_IOS": "{:,.0f}",  # total_costÎ•º Ï≤ú Îã®ÏúÑÎ°ú Ìè¨Îß∑

            **{col: "{:.1%}" for col in ['os_cost_ratio_IOS', 'd7_roas_And', 'd14_roas_And', 'd30_roas_And', 'd7_roas_IOS', 'd14_roas_IOS', 'd30_roas_IOS']}  # ÎπÑÏú® Ìè¨Îß∑
        }) \
        .bar(subset=['total_cost_And','total_cost_IOS'], color='#fbe4e6') \
        .bar(subset=['d7_roas_And', 'd14_roas_And', 'd30_roas_And', 'd7_roas_IOS', 'd14_roas_IOS', 'd30_roas_IOS'], color='#b6d7a8') \
        .apply(highlight_max, subset=["d7_roas_And","d7_roas_IOS"], axis=1)
        .apply(highlight_max, subset=["d14_roas_And","d14_roas_IOS"], axis=1)
        .apply(highlight_max, subset=["d30_roas_And","d30_roas_IOS"], axis=1)
        .background_gradient(subset=['os_cost_ratio_IOS'], cmap='Reds')
    '''

    def highlight_max_bg(s):
        vmax = s.max(skipna=True)
        return [
            'background-color: pink' if (not pd.isna(v) and v == vmax) else ''
            for v in s
        ]



    styled_df = (
        pivot_df
        .style
        .format({
            "cost_exclude_credit_And": "{:,.0f}",
            "cost_exclude_credit_IOS": "{:,.0f}",
            **{col: "{:.1%}" for col in [
                "os_cost_ratio_IOS",
                "d7_roas_And","d14_roas_And","d30_roas_And",
                "d7_roas_IOS","d14_roas_IOS","d30_roas_IOS"
            ]}
        })
        .bar(subset=['cost_exclude_credit_And', 'cost_exclude_credit_IOS'], color="#fbe4e6")
        .bar(subset=[
            "d7_roas_And","d14_roas_And","d30_roas_And",
            "d7_roas_IOS","d14_roas_IOS","d30_roas_IOS"
        ], color="#b6d7a8")
        .apply(highlight_max_bg, subset=["d7_roas_And","d7_roas_IOS"], axis=1)
        .apply(highlight_max_bg, subset=["d14_roas_And","d14_roas_IOS"], axis=1)
        .apply(highlight_max_bg, subset=["d30_roas_And","d30_roas_IOS"], axis=1)
        .background_gradient(subset=["os_cost_ratio_IOS"], cmap="Reds")
    )



    nest_asyncio.apply()

    # HTML ÌÖúÌîåÎ¶ø Ï†ïÏùò
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            table { border-collapse: collapse; font-size: 13px; }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: right;
            }
            th { background-color: #f0f0f0; }
        </style>
    </head>
    <body>
        <h2>OSÎ≥Ñ ÌòÑÌô©</h2>
        {{ table | safe }}
    </body>
    </html>
    """

    # HTML Î†åÎçîÎßÅ Î∞è Ï†ÄÏû•
<<<<<<< HEAD
    # HTML Î†åÎçîÎßÅ Î∞è Ï†ÄÏû•
    table_html = styled_df.to_html()
    rendered_html = Template(html_template).render(table=table_html)

    html_path = os.path.join(temp_dir, "os_roas.html")
=======
    table_html = styled_df.to_html()
    rendered_html = Template(html_template).render(table=table_html)

    html_path = "os_roas.html"
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(rendered_html)

    # Ïù¥ÎØ∏ÏßÄ Ï∫°Ï≤ò ÎπÑÎèôÍ∏∞ Ìï®Ïàò
    async def capture_html_to_image():
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={"width": 1600, "height": 1000})
            await page.goto("file://" + os.path.abspath(html_path))
<<<<<<< HEAD
            out_path = os.path.join(temp_dir, "os_roas.png")
            await page.screenshot(path=out_path, full_page=True)
=======
            await page.screenshot(path="os_roas.png", full_page=True)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
            await browser.close()

    # Spyder or GCP Notebook ÌôòÍ≤Ω ÎåÄÏùë
    asyncio.get_event_loop().run_until_complete(capture_html_to_image())

<<<<<<< HEAD

=======
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    ########### (1) Ï†úÎ™©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "2) OSÎ≥Ñ ROAS ÌòÑÌô©" }}]
                },
            }
        ],
    )



    ########### (2) OS ÏÑúÏãùÌëú ÏóÖÎ°úÎìú

    # ÏóÖÎ°úÎìúÏö© Í≤ΩÎ°ú Î≥ÄÏàò
<<<<<<< HEAD
    # from pathlib import Path # Already imported
    IMG_PATH = Path(os.path.join(temp_dir, "os_roas.png")).resolve()  # Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î≥ÄÌôò(Í∂åÏû•)
=======
    from pathlib import Path
    IMG_PATH = Path("os_roas.png").resolve()  # Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î≥ÄÌôò(Í∂åÏû•)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    assert IMG_PATH.exists() and IMG_PATH.stat().st_size > 0

    hdr_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

    # 1) ÏóÖÎ°úÎìú Ïò§Î∏åÏ†ùÌä∏ ÏÉùÏÑ± (upload_url Î∞õÍ∏∞)
    resp = requests.post(
        "https://api.notion.com/v1/file_uploads",
        json={"filename": IMG_PATH.name, "content_type": "image/png"},
        headers=hdr_json
    )
    resp.raise_for_status()
    fu = resp.json()
    file_upload_id = fu["id"]
    upload_url = fu.get("upload_url")


    # 2) Ïã§Ï†ú Ï†ÑÏÜ° (multipart/form-data) ‚áí status Í∞Ä uploaded Ïó¨Ïïº Ï≤®Î∂Ä Í∞ÄÎä•
    with open(IMG_PATH, "rb") as f:
        r2 = requests.post(
            # Í∂åÏû•: Î™ÖÏãúÏ†Å send ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÇ¨Ïö©
            f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send",
            # ÎòêÎäî upload_url ÏÇ¨Ïö© Í∞ÄÎä• (ÎèôÏùº ÎèôÏûë): upload_url,
            headers={"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION},
            files={"file": (IMG_PATH.name, f, "image/png")}
        )
    r2.raise_for_status()




    # Ìëú Ï≤®Î∂Ä
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    #"caption": [
                    #    {"type": "text", "text": {"content": "ROAS & Cost (auto-upload)"}}
                    #]
                },
            }
        ]
    )




    ########### (3) geminiÎ∂ÑÏÑù ÎÇ¥Ïö© Ï≤®Î∂Ä

    blocks = md_to_notion_blocks(response_os.text + "\n\n\n")

    notion.blocks.children.append(
        block_id=PAGE_ID,
        children=blocks
    )

<<<<<<< HEAD
    # RUN_ID = now_kst.strftime("%Y%m%d") # Already defined
=======

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620

    LABELS = {"datascience_division_service": "monthly_mkt_framework",
            "run_id": RUN_ID,
            "datascience_division_service_sub" : "mkt_monthly_3_geo_roas"} ## ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î∂ôÏùº Ïàò ÏûàÏùå.
    print("RUN_ID=", RUN_ID, "LABEL_ID=", LABELS)

    # 2> Íµ≠Í∞ÄÎ≥Ñ
    client = bigquery.Client()
<<<<<<< HEAD
    query = f"""
=======
    query = """
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    WITH revraw AS(
    select  JoypleGameID, regmonth, geo_user_group
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as regmonth
    , geo_user_group,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
        , case when OS = 'android' then 'And' when OS = 'ios' then 'IOS' else OS end as osuser
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID in (131,133,30001,30003)
<<<<<<< HEAD
        and RegdateAuthAccountDateKST between '{now_kst.year}-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
=======
        and RegdateAuthAccountDateKST between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )
    group by JoypleGameID, RegdateAuthAccountDateKST, geo_user_group
    ) group by JoypleGameID, regmonth, geo_user_group
    )




    , cost_raw AS(
    select joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) as regmonth   , geo_user_group
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit,
    from(
    select *, case when countrycode = 'KR' then '1.KR'
        when countrycode = 'US' then '2.US'
        when countrycode = 'JP' then '3.JP'
        when countrycode in ('UK','FR','DE','GB') then '4.WEU'
        else '5.ETC' end as geo_user_group
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid in (131,133,30001,30003)
<<<<<<< HEAD
    and cmpgndate between '{now_kst.year}-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
=======
    and cmpgndate between '2025-01-01' and DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )
    group by  joyplegameid,gameid,  format_date('%Y-%m', cmpgndate) , geo_user_group
    )



    select
    ifnull(a.joyplegameid , b.joyplegameid) as joyplegameid
    ,ifnull(a.regmonth , b.regmonth) as regmonth
    , ifnull(a.geo_user_group, b.geo_user_group) as geo_user_group
    , a.ru
    ,a.sales_D1, a.sales_D3, a.sales_D7, a.sales_D14, a.sales_D30, a.sales_D60, a.sales_D90 , a.sales_D120, a.sales_D150, a.sales_D180
    , a.sales_D210, a.sales_D240, a.sales_D270, a.sales_D300, a.sales_D330, a.sales_D360
    , b.cost, b.cost_exclude_credit
    from revraw as a
    full join cost_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.geo_user_group = b.geo_user_group
    """

    query_result_raw_geo = client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()

    ########## Íµ≠Í∞ÄÎ≥Ñ Ï†ÑÏ≤òÎ¶¨
    # Í∞Å cohortÏóê ÎåÄÌï¥ ROAS Í≥ÑÏÇ∞
    for cohort in cohort_columns:
        query_result_raw_geo[f'{cohort.lower()}_roas'] = query_result_raw_geo[f'sales_{cohort}'] / query_result_raw_geo['cost_exclude_credit']


    query_result_raw_geo['cpru'] = query_result_raw_geo['cost_exclude_credit'] / query_result_raw_geo['ru']

    query_result_raw_geo2 = pd.merge(query_result_raw_geo, game_total_cost, on=['joyplegameid', 'regmonth'], how='left')
    query_result_raw_geo2['geo_cost_ratio'] = query_result_raw_geo2['cost_exclude_credit'] / query_result_raw_geo2['game_total_cost']



    query_result_raw_geo2["game_name"] = query_result_raw_geo2["joyplegameid"].map(mapping)  # Îß§Ìïë ÏóÜÏúºÎ©¥ NaN

    select_cols = ['game_name' ,'regmonth','geo_user_group','cost_exclude_credit','geo_cost_ratio','d1_roas','d3_roas','d7_roas','d14_roas','d30_roas','d60_roas','d90_roas','d120_roas','d150_roas']
    query_result_raw_geo3 = query_result_raw_geo2[select_cols]

    df_geo = df_to_md(query_result_raw_geo3.sort_values(['game_name', 'regmonth']).groupby('game_name').tail(30))
    #### gemini Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞
    # ÏµúÍ∑º 6Í∞úÏõî Îç∞Ïù¥ÌÑ∞Îßå Ï∂îÏ∂ú

    recent_data2 = recent_data[[ "game_name","regmonth", "cost_exclude_credit","cpru" ,"d360roas_growth","month_start"]]


    # ÌÖåÏù¥Î∏îÏóê ÎåÄÌïú ÏÑ§Î™Ö

    # Îç∞Ïù¥ÌÑ∞ ÌïôÏäµ Î∞è Ï†ïÏùò
    prompt_description = f"""
    SLG Í≤åÏûÑ 4Ï¢ÖÏùò ÏõîÎ≥Ñ, OSÎ≥Ñ, Í∂åÏó≠Î≥Ñ ROAS ÌòÑÌô©ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Îç∞Ïù¥ÌÑ∞Îì§Ïù¥Ïïº.
    Í∞íÏù¥ NAÏù∏ CohortÎ≥ÄÏàò(dn_roas)Îäî ÏïÑÏßÅ matureÎêòÏßÄ ÏïäÏùÄ ÏßÄÌëúÏïº.

    [Ï£ºÏöî Î≥ÄÏàò ÏÑ§Î™Ö]
    - cost_exclude_credit: ÌÅ¨Î†àÎîß Ï†úÏô∏ ÏõîÎ≥Ñ ÎßàÏºÄÌåÖÎπÑ (roas Í≥ÑÏÇ∞ Í∏∞Ï§Ä, ÎãπÏõîÏùÄ ÏùºÌï†Í≥ÑÏÇ∞ Ï∂îÏ†ï Ïõî cost)
    - cpru: Îã®Í∞Ä
    - d360roas_growth: Î≥µÍ∑ÄÏú†Ï†Ä ÎØ∏Ìè¨Ìï® Ïã†Í∑úÏú†Ï†Ä d360 ÏòàÏ∏°Ïπò
    - month_start : Í∏∞Ï§ÄÏù¥ ÎêòÎäî Ïõî

    ### Îç∞Ïù¥ÌÑ∞ÏÖã
    1. Ï†ÑÏ≤¥ Ïú†Ï†Ä Í∏∞Ï§Ä Îç∞Ïù¥ÌÑ∞:
    {recent_data2}

    2. df_geo (Íµ≠Í∞ÄÎ≥Ñ):
    {df_geo}

    3. df_os (OSÎ≥Ñ):
    {df_os}
    """


    # 3. GeminiÏóê Ï†ÑÎã¨Ìï† Ï†ÑÏ≤¥ ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
    prompt_parts = [
        prompt_description,

    """
    ### ÎßàÏºÄÌåÖ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏûëÏÑ± ÏöîÏ≤≠

    ÏÑ∏ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Í∏∞Î∞òÏúºÎ°ú Îã§Ïùå 3Í∞ÄÏßÄ Ìï≠Î™©Ïóê ÎåÄÌï¥ Í≤åÏûÑÎ≥Ñ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏Î•º ÏûëÏÑ±Ìï¥Ï§ò

    1. **Ï†ÑÏ≤¥ Ìä∏Î†åÎìú Î∂ÑÏÑù:**
        -  `month_start` Í∏∞Ï§ÄÏúºÎ°ú ÏµúÍ∑º 3Í∞úÏõî Îç∞Ïù¥ÌÑ∞Ïùò Ìä∏Î†åÎìúÎ•º ÏÑ§Î™ÖÌï¥Ï§ò.
        - `d360roas_growth` ÏßÄÌëúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ **ÏµúÍ∑º 3Í∞úÏõîÍ∞Ñ**Ïùò **Ï†ÑÏõî ÎåÄÎπÑ ÎãπÏõî ROAS Î≥ÄÌôî** Ï∂îÏù¥Î•º ÏöîÏïΩ
        - `recent_data2`Ïùò **cost** Î∞è **cpru** ÏßÄÌëúÎ•º Ï∞∏Í≥†ÌïòÏó¨ ÏµúÍ∑º 3Í∞úÏõîÍ∞ÑÏùò **CostÏôÄ Îã®Í∞Ä(CPRU) Ìä∏Î†åÎìú**Í∞Ä Ï¶ùÍ∞ÄÌïòÎäîÏßÄ Í∞êÏÜåÌïòÎäîÏßÄ Ïñ∏Í∏âÌï¥Ï§ò

    2. **ÏõêÏù∏ Î∂ÑÏÑù (Íµ≠Í∞Ä/OS):**
        - `df_geo`ÏôÄ `df_os`Î•º Ï∞∏Í≥†ÌïòÏó¨ **Í∞ÄÏû• ÏµúÍ∑º ÏõîÏùò ROAS Î≥ÄÎèôÏóê Í∞ÄÏû• ÌÅ∞ ÏòÅÌñ•ÏùÑ Ï§Ä ÏöîÏù∏** (Íµ≠Í∞Ä ÎòêÎäî OS)ÏùÑ Ï∞æÍ≥†, Ìï¥Îãπ ÏöîÏù∏Ïùò **ÏõîÎ≥Ñ ÏÑ±Í≥º**Î•º ÎπÑÍµêÌï¥Ï§ò
        - **ÎπÑÍµêÎäî Î∞òÎìúÏãú ÎèôÏùºÌïú Cohort(dN)Î•º Í∏∞Ï§ÄÏúºÎ°ú ÏõîÎ≥Ñ**Î°ú ÏßÑÌñâÌï¥

    3. **Ïï°ÏÖò ÏïÑÏù¥ÌÖú Ï†úÏïà:**
        - Î∂ÑÏÑù ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú Î™ÖÌôïÌïòÍ≥† Ïã§Ìñâ Í∞ÄÎä•Ìïú **Ïï°ÏÖò ÏïÑÏù¥ÌÖú**ÏùÑ Ï†úÏïàÌï¥Ï§ò
        - Ï†úÏïàÏùÄ ÌäπÏ†ï Í∂åÏó≠Ïùò ÌäπÏ†ï Í∏∞Í∞Ñ ÏÑ±Í≥ºÎ•º Í∏∞Ï§ÄÏúºÎ°ú **ÏòàÏÇ∞ Ï¶ù/Í∞êÏÜå** ÎòêÎäî **ÌäπÏ†ï OS ÏòàÏÇ∞ Ï¶ù/Í∞êÏÜå** Îì±.

    ÏûëÏÑ± Ïãú ÏïÑÎûòÏùò ÌòïÌÉúÎ•º ÏßÄÏºúÏÑú ÎßàÌÅ¨Îã§Ïö¥ÌòïÌÉúÎ°ú ÏûëÏÑ± Î∂ÄÌÉÅÌï¥

    ## Í≤åÏûÑÎ™Ö (Ïòà: ## 1.POTC)

    1. **Ï†ÑÏ≤¥ Ìä∏Î†åÎìú:** ...
    2. **ÏõêÏù∏ Î∂ÑÏÑù:**
        - Íµ≠Í∞ÄÎ≥Ñ Î∂ÑÏÑù:...
        - OSÎ≥Ñ Î∂ÑÏÑù: ...
    3. **Ï†úÏïà: ...
    """
    ]

    genai_client = GeminiClient(
<<<<<<< HEAD
        vertexai=True,
        location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
=======
    vertexai=True,
    location="global"      # genai Ìò∏Ï∂úÏö©location Î≥ÄÍ≤Ω
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    )

    config_3_optimized = GenerateContentConfig(
        temperature=1.0,
        thinking_config=types.ThinkingConfig(thinking_level="high"),
        system_instruction=system_instruction,
        labels=LABELS
    )

    response_total = genai_client.models.generate_content(
        model="gemini-3-pro-preview",   # Vertex AI Î™®Îç∏Î™Ö
        contents = prompt_parts
        ,config=config_3_optimized
    )
    print(response_total.text)

    ########### (1) Ï†úÎ™©
    notion.blocks.children.append(
        PAGE_ID,
        children=[
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": "3) Ï¢ÖÌï© Í≤∞Î°†" }}]
                },
            }
        ],
    )


    ## Ï¢ÖÌï© Ìï¥ÏÑù
<<<<<<< HEAD
    blocks = md_to_notion_blocks(response_total.text + "\n\n\n")
=======

    blocks = md_to_notion_blocks(response_total.text + "\n\n\n")

>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
    notion.blocks.children.append(
        block_id=PAGE_ID,
        children=blocks
    )

    print("‚úÖ Append ÏôÑÎ£å")

    if ref_person_ids:
        props_update = {
            "Ï∞∏Ï°∞Ïûê": {"people": [{"id": pid} for pid in ref_person_ids]}
        }
        updated_page = notion.pages.update(
            page_id=PAGE_ID,
            properties=props_update
        )
        print("‚úÖ Notion ÌéòÏù¥ÏßÄ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å (Ï∞∏Ï°∞Ïûê Ï∂îÍ∞Ä):", updated_page["url"])


# DAG Í∏∞Î≥∏ ÏÑ§Ï†ï
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=15),
}

with DAG(
    dag_id='Marketing_Monthly_Report',
    default_args=default_args,
<<<<<<< HEAD
    description='ÏõîÍ∞Ñ ÎßàÏºÄÌåÖ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± (notion + gemini)',
    schedule='10 5 * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['marketing', 'report', 'monthly'],
) as dag:
    
    # Task Ï†ïÏùò
    task = PythonOperator(
        task_id='mkt_monthly_report_total',
        python_callable=mkt_monthly_report_total,
        provide_context=True, # Airflow Context Ï†ÑÎã¨ÏùÑ ÏúÑÌï¥ ÌïÑÏöî (Airflow Î≤ÑÏ†ÑÏóê Îî∞Îùº ÏÉùÎûµ Í∞ÄÎä•ÌïòÎÇò Î™ÖÏãú)
=======
    description='ÏõîÍ∞Ñ ÎßàÏºÄÌåÖ Î¶¨Ìè¨Ìä∏ ÏûêÎèô ÏÉùÏÑ± Î∞è Notion ÏóÖÎ°úÎìú',
    schedule='15 5 * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['marketing', 'notion', 'reporting'],
) as dag:


    task = PythonOperator(
        task_id='marketing_monthly_report_to_notion',
        python_callable=marketing_monthly_report_to_notion,
>>>>>>> 83ab6133687c51c69719e4b33218037d4a475620
        dag=dag,
    )