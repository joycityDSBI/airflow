import time
import pandas as pd
from google.cloud import bigquery
from google import genai
from google.genai import types
from goole.cloud import storage
from vertexai import rag
import vertexai
from google.genai import Client
from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore

# ì¸ì¦ê´€ë ¨
import google.auth
from google.auth.transport.requests import Request

# ê·¸ë˜í”„ ê´€ë ¨ íŒ¨í‚¤ì§€
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter, StrMethodFormatter, PercentFormatter, MultipleLocator
import matplotlib as mpl
import matplotlib.font_manager as fm
from matplotlib import cm
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont # 2ê°€ì§€ íŒŒì¼ í•©ì¹˜ê¸°
import matplotlib.dates as mdates
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio
import IPython.display as IPd
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List, Tuple

# ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€
import numpy as np
import re
import os 
import math
import time
import pandas as pd
from notion_client import Client
import requests
import json
from datetime import datetime, timezone, timedelta
from adjustText import adjust_text
from airflow.models import Variable
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
from pathlib import Path



# í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)


# ë³€ìˆ˜ ìƒì„±
t0 = time.time()
PROJECT_ID = "data-science-division-216308"
LOCATION = "us-central1"
MODEL_NAME = "gemini-2.5-flash"

NOTION_TOKEN=get_var("MS_TEAM_NOTION_TOKEN") # MSíŒ€ API í‚¤
NOTION_VERSION=get_var("NOTION_API_VERSION")
DATABASE_ID=get_var("GAMEFRAMEWORK_GBTW_NOTION_DB_ID")
CREDENTIALS_JSON = get_var('GOOGLE_CREDENTIAL_JSON')

cred_dict = json.loads(CREDENTIALS_JSON)
credentials, project_id = google.auth.default(
    scopes=["https://www.googleapis.com/auth/cloud-platform"]
)
credentials.refresh(Request())


# í´ë¼ì´ì–¸íŠ¸ ëª¨ìŒ
genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)
bigquery_client = bigquery.Client(project=PROJECT_ID, credentials=credentials)# location=LOCATION ## us-central1 ë¡œ í•  ê²½ìš° í—ˆë¸Œ ì¡°íšŒë¶ˆê°€ëŠ¥
notion = Client(auth=NOTION_TOKEN)
gcs_client = storage.Client.from_service_account_info(cred_dict)
bucket = gcs_client.bucket('game-framework1')


#### ì œë¯¸ë‚˜ì´ ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ 
SYSTEM_INSTRUCTION = [
                "You're a Game Data Analyst.",
                "Your task is to analyze the metrics of a given mobile game and identify the causes of any changes.",
                "Your answers must be in Korean.",
                "The unit of amount in the Sales or Revenue, Cost Data is Korean Won.",
                "You must answer in Notion's Markdown format, but do not use title syntax.",
            ]


## í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ //////////// task í•¨ìˆ˜
def make_gameframework_notion_page(gameidx: str, **context):

    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    # íƒ€ì„ì¡´ ì§€ì •
    kst = ZoneInfo("Asia/Seoul")
    # ì˜¤ëŠ˜
    today_kst = datetime.now(kst).date()
    # ì–´ì œ
    yesterday_kst = today_kst - timedelta(days=1)

    # ì´ë²ˆ ë‹¬ 1ì¼ (ì–´ì œ ë‚ ì§œ ê¸°ì¤€)
    first_day = yesterday_kst.replace(day=1)

    # íƒ€ì´í‹€ ë¬¸ìì—´ ë§Œë“¤ê¸°
    title = f"{yesterday_kst.strftime('%y')}ë…„ {yesterday_kst.month}ì›” ë§¤ì¶œí˜„í™©( ~ {yesterday_kst})"
    print(f"{title} : {gameidx}")

    # í˜ì´ì§€ ìƒì„± ìš”ì²­ ë°”ë””
    data = {
        "parent": {"database_id": DATABASE_ID},
        "properties": {
            "ì´ë¦„": {
                "title": [
                    {"text": {"content": title }}
                ]
            },
            "ë“±ë¡ ë‚ ì§œ": {
                "date": {"start": today_kst.isoformat() }
            },
            "í”„ë¡œì íŠ¸": {
                "multi_select": [
                    {"name": {gameidx}}   # ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜
                ]
            },
            "ë¦¬í¬íŠ¸ ì¢…ë¥˜": {
                "multi_select": [
                    {"name": "ê²Œì„ë¶„ì„"}   # ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜
                ]
            },
            "ì‘ì„±ì": {
                "people": [
                    {"id": "ce95f16a-6b6b-447d-a996-a9c5f0cc0113"},  # Notion user_id
                    {"id": "662575bc-731c-481c-afc7-13b2fdf5482a"}  # Notion user_id
                ]
            }
        }
    }

    res = requests.post(url, headers=headers, json=data)

    if res.status_code == 200:
        page_info = res.json() # âœ… í˜ì´ì§€ ID page_info["id"]
        print(f"âœ… í˜ì´ì§€ ìƒì„± ì„±ê³µ âœ… í˜ì´ì§€ ID : {page_info["id"]}")
    else:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {res.status_code} >> {res.text}")

    notion.blocks.children.append(
        block_id=page_info["id"] ,
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {
                            "type": "text",
                            "text": {"content": " â—¾ ëª©ì°¨"},
                            "annotations": {"bold": True}
                        }
                    ]
                }
            }
        ]
    )

    # ëª©ì°¨ ë¸”ë¡ ì¶”ê°€
    notion.blocks.children.append(
        block_id=page_info["id"] ,
        children=[
            {
                "object": "block",
                "type": "table_of_contents",
                "table_of_contents": {
                    "color": "default"  # "gray", "brown", "orange", "yellow", "green", "blue", "purple", "pink", "red" ê°€ëŠ¥
                }
            }
        ],
    )
    
    if res.status_code == 200:
        page_info = res.json()
        print(f"âœ… í˜ì´ì§€ ìƒì„± ì„±ê³µ âœ… í˜ì´ì§€ ID : {page_info['id']}")
    else:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {res.status_code} >> {res.text}")

    context['task_instance'].xcom_push(key='page_info', value=page_info)

    return page_info 



################# notion í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ ì‹¤í–‰ ############################

# ğŸ‘‰ Markdown ë‚´ **êµµê²Œ** ì²˜ë¦¬ ë³€í™˜
def parse_rich_text(md_text):
    """
    '**êµµê²Œ**' â†’ Notion rich_text [{"text": {...}, "annotations": {"bold": True}}]
    """
    parts = re.split(r"(\*\*.*?\*\*)", md_text)  # **...** ê¸°ì¤€ split
    rich_text = []
    for part in parts:
        if part.startswith("**") and part.endswith("**"):
            rich_text.append({
                "type": "text",
                "text": {"content": part[2:-2]},
                "annotations": {"bold": True}
            })
        else:
            if part:
                rich_text.append({
                    "type": "text",
                    "text": {"content": part}
                })
    return rich_text



# ğŸ‘‰ Markdownì„ Notion Blocksë¡œ ë³€í™˜
def md_to_notion_blocks(md_text, blank_blocks=3):
    blocks = []
    lines = md_text.splitlines()
    stack = [blocks]  # í˜„ì¬ ê³„ì¸µ ì¶”ì 

    def detect_indent_unit(lines):
        indents = []
        for line in lines:
            if line.lstrip().startswith(("* ", "- ", "+ ")):  # ë¦¬ìŠ¤íŠ¸ ë¬¸ë²• ê°ì§€
                indent = len(line) - len(line.lstrip())
                if indent > 0:
                    indents.append(indent)
        return min(indents) if indents else 4  # fallback = 4ì¹¸
    indent_unit = detect_indent_unit(lines)

    i = 0
    while i < len(lines):
        line = lines[i].rstrip()
        if not line:
            i += 1
            continue

        # Heading ì²˜ë¦¬
        if line.startswith("# "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": parse_rich_text(line[2:])}
            })
        elif line.startswith("## "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": parse_rich_text(line[3:])}
            })
        elif line.startswith("### "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": parse_rich_text(line[4:])}
            })

        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        elif line.lstrip().startswith("* "):
            indent = len(line) - len(line.lstrip())  # ë“¤ì—¬ì“°ê¸° ë ˆë²¨
            content = line.strip()[2:].strip()

            block = {
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": parse_rich_text(content),
                    "children": []
                }
            }

            # indent ê¸°ë°˜ ê³„ì¸µ ì²˜ë¦¬
            level = indent // indent_unit + 1
            while len(stack) > level:
                stack.pop()
            stack[-1].append(block)
            stack.append(block["bulleted_list_item"]["children"])
        else:
            stack = [blocks]
            # ì¼ë°˜ ë¬¸ë‹¨
            stack[-1].append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": parse_rich_text(line.strip())}
            })

        i += 1

    # âœ… ë§ˆì§€ë§‰ì— ë¹ˆ ë¸”ë¡ ì¶”ê°€ (ê°œìˆ˜ëŠ” íŒŒë¼ë¯¸í„° blank_blocksë¡œ ì œì–´)
    for _ in range(blank_blocks):
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": []}
        })

    return blocks


def df_to_notion_table_under_toggle(
    notion: Client,
    page_id: str,
    df: pd.DataFrame,
    toggle_title: str = "ğŸ“Š Data Table",
    max_first_batch_rows: int = 90,
    batch_size: int = 100,
    has_column_header: bool = True,
    has_row_header: bool = False,
    ):
    """
    Notion í˜ì´ì§€ì— í† ê¸€ì„ ë§Œë“¤ê³ , ê·¸ ì•„ë˜ì— Pandas DataFrameì„ í‘œ(Table)ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    - ìµœì´ˆ ìƒì„± ì‹œ í…Œì´ë¸”ì˜ header + ì´ˆê¸° í–‰ë“¤ì„ table.children ì•ˆì— í¬í•¨
    - ì´í›„ ë‚¨ì€ í–‰ë“¤ì€ table_rowë¡œ ë°°ì¹˜ append

    Parameters
    ----------
    notion : notion_client.Client
        Notion SDK í´ë¼ì´ì–¸íŠ¸ (Client(auth=...) ë¡œ ìƒì„±)
    page_id : str
        í…Œì´ë¸”ì„ ì¶”ê°€í•  í˜ì´ì§€(í˜¹ì€ ë¸”ë¡) ID
    df : pandas.DataFrame
        ì—…ë¡œë“œí•  ë°ì´í„°í”„ë ˆì„
    toggle_title : str
        í† ê¸€ íƒ€ì´í‹€
    max_first_batch_rows : int
        í…Œì´ë¸” ìµœì´ˆ ìƒì„± ì‹œ í¬í•¨í•  ì´ˆê¸° í–‰ ê°œìˆ˜(Too many children ë°©ì§€ìš©)
    batch_size : int
        ì´í›„ ë°°ì¹˜ append ì‹œ ë¬¶ìŒ í¬ê¸°
    has_column_header : bool
        Notion í…Œì´ë¸” ì˜µì…˜ - ì»¬ëŸ¼ í—¤ë” ì‚¬ìš© ì—¬ë¶€
    has_row_header : bool
        Notion í…Œì´ë¸” ì˜µì…˜ - í–‰ í—¤ë” ì‚¬ìš© ì—¬ë¶€

    Returns
    -------
    dict : {"toggle_id": str, "table_id": str, "rows_created": int}
    """
    # 1) í† ê¸€ ìƒì„±
    toggle_resp = notion.blocks.children.append(
        page_id,
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {
                            "type": "text",
                            "text": {"content": toggle_title[:2000]},
                            "annotations": {
                                "bold": True,
                                "italic": False,
                                "underline": False,
                                "strikethrough": False,
                                "code": False,
                                "color": "blue"   # â† ìƒ‰ìƒ ì§€ì •
                            },
                        }
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    # 2) í—¤ë”/ì´ˆê¸°í–‰ ì¤€ë¹„
    table_width = len(df.columns)

    # í—¤ë”(ì»¬ëŸ¼ëª…)
    header_cells = []
    for col in df.columns.astype(str).tolist():
        header_cells.append([{"type": "text", "text": {"content": str(col)[:2000]}}])

    # ì´ˆê¸° ë°ì´í„° í–‰
    first_rows_blocks = []
    row_count = 0
    for _, row in df.iterrows():
        if row_count >= max_first_batch_rows:
            break
        row_cells = []
        for col in df.columns:
            val = row[col]
            #s = "" if (val is None or (isinstance(val, float) and math.isnan(val))) else str(val)
            s = "" if pd.isna(val) else str(val)
            row_cells.append([{"type": "text", "text": {"content": s[:2000]}}])

        # ì—´ ìˆ˜ ì•ˆì „ì¥ì¹˜(íŒ¨ë”©/ì ˆë‹¨)
        if len(row_cells) < table_width:
            row_cells += [[{"type": "text", "text": {"content": ""}}]] * (table_width - len(row_cells))
        elif len(row_cells) > table_width:
            row_cells = row_cells[:table_width]

        first_rows_blocks.append(
            {"object": "block", "type": "table_row", "table_row": {"cells": row_cells}}
        )
        row_count += 1

    # 3) í…Œì´ë¸” ë¸”ë¡ ìƒì„±: table.children ì•ˆì— header + ì´ˆê¸°í–‰ í¬í•¨(ì¤‘ìš”)
    table_block = {
        "object": "block",
        "type": "table",
        "table": {
            "table_width": table_width,
            "has_column_header": has_column_header,
            "has_row_header": has_row_header,
            "children": (
                [
                    {
                        "object": "block",
                        "type": "table_row",
                        "table_row": {"cells": header_cells},
                    }
                ]
                + first_rows_blocks
            ),
        },
    }

    table_create_resp = notion.blocks.children.append(toggle_id, children=[table_block])
    table_id = table_create_resp["results"][0]["id"]

    # 4) ë‚¨ì€ í–‰ë“¤ ë°°ì¹˜ ì¶”ê°€
    total = len(df)
    start = row_count
    while start < total:
        end = min(start + batch_size, total)
        batch_children = []

        for _, row in df.iloc[start:end].iterrows():
            row_cells = []
            for col in df.columns:
                val = row[col]
                s = "" if (val is None or (isinstance(val, float) and math.isnan(val))) else str(val)
                row_cells.append([{"type": "text", "text": {"content": s[:2000]}}])

            if len(row_cells) < table_width:
                row_cells += [[{"type": "text", "text": {"content": ""}}]] * (table_width - len(row_cells))
            elif len(row_cells) > table_width:
                row_cells = row_cells[:table_width]

            batch_children.append(
                {"object": "block", "type": "table_row", "table_row": {"cells": row_cells}}
            )

        notion.blocks.children.append(table_id, children=batch_children)
        start = end

    return {"toggle_id": toggle_id, "table_id": table_id, "rows_created": total}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš© ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# from notion_client import Client
# notion = Client(auth=NOTION_TOKEN)
# resp = df_to_notion_table_under_toggle(
#     notion=notion,
#     page_id=PAGE_ID,
#     df=query_result1_dailySales,
#     toggle_title="ğŸ“Š Daily Sales (DataFrame Table)",
#     max_first_batch_rows=90,
#     batch_size=100,
# )
# print(resp)

### ì¼ìë³„ ë§¤ì¶œ
# ì¿¼ë¦¬ & ì œë¯¸ë‚˜ì´ í”„ë¡¬í”„íŠ¸

def query_run_method(service_sub: str, query):
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}} ## ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¶™ì¼ ìˆ˜ ìˆìŒ.
    print("ğŸ“§ RUN_ID=", RUN_ID, "ğŸ“§ LABEL_ID=", LABELS)

    query_result = bigquery_client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()
    return query_result

## ì¼ìë³„ ë§¤ì¶œ
def Daily_revenue_query(joyplegameid: int, **context):
    query = f"""

    select day
    , cast(sum(if(monthtype = 'ì§€ë‚œë‹¬' , pricekrw, null ))as int64) as `ì§€ë‚œë‹¬`
    , cast(sum(if(monthtype = 'ì´ë²ˆë‹¬' , pricekrw, null ))as int64) as `ì´ë²ˆë‹¬`

    from
    (select *
    , format_date('%Y-%m',  logdatekst ) as Month
    , case when logdatekst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdatekst< DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH) then 'ì§€ë‚œë‹¬'
    when logdatekst >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH) then 'ì´ë²ˆë‹¬'
    else 'etc' end as monthtype
    , format_date('%d',  logdatekst ) as day
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdateKst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    #and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )
    group by 1
    order by 1

    """
    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='daily_revenue_df', value=query_result)

    return True
    
    
#### ì „ë…„ ëŒ€ë¹„ ì›” ë§¤ì¶œ ì¶”ì´
def Daily_revenue_YOY_query(joyplegameid: int, **context):
    query = f"""

    select month
    , cast(sum(if(yeartype = 'ì‘ë…„' , pricekrw, null ))as int64) as `ì‘ë…„`
    , cast(sum(if(yeartype = 'ì˜¬í•´' , pricekrw, null ))as int64) as `ì˜¬í•´`

    from
    (select *
    , case
    when logdatekst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR), INTERVAL 1 YEAR)
    and logdatekst< DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR) then 'ì‘ë…„'

    when logdatekst >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR)
    and logdatekst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR) then 'ì˜¬í•´'
    else 'etc' end as yeartype
    , format_date('%m',  logdatekst ) as month
    , format_date('%Y',  logdatekst ) as year

    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdateKst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR), INTERVAL 1 YEAR)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR)
    )
    group by 1
    order by 1

    """
    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='Daily_revenue_YOY_df', value=query_result)

    return True


## í˜„ì¬ ë§¤ì¶œê³¼ ëª©í‘œ ë§¤ì¶œ
def Daily_revenue_target_revenue_query(joyplegameid: int, gameidx: str, **context):
    query = f"""
    ### 1> ì´ë²ˆë‹¬ ì¼ìë³„ ë§¤ì¶œ ì‹¤ì¸¡ì¹˜
    with thismonthRev as (
    select day ## ì¼ì
    , lastDay ## ì´ë²ˆë‹¬ ë§ˆì§€ë§‰ë‚  (ex - 30)
    , sum(pricekrw) as rev ## ë§¤ì¶œì•¡
    from
    (select *
    , cast(format_date('%d',  logdatekst ) as int64) as day ## ì¼ì
    , EXTRACT(DAY FROM LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)) as lastDay
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    )
    group by day,lastDay
    order by day
    ),

    ### 2> ëª©í‘œë§¤ì¶œ í…Œì´ë¸”
    salesGoal as (
    select
        CAST(REPLACE(sales, ',', '') AS INT64) as salesGoalMonthly # ì‰¼í‘œ í¬í•¨í•œ string í˜•íƒœë¡œ ì ì¬ë˜ì–´ìˆì–´ì„œ int64 í˜•íƒœë¡œ ì „ì²˜ë¦¬
    , CAST(REPLACE(sales, ',', '') AS INT64)/cast(num_of_days as int64) as salesGoalDaily # ì¼í‰ê·  ëª©í‘œë§¤ì¶œ
    from `data-science-division-216308.gameInsightFramework.slgMonthlyGoal`
    where idx = {gameidx}
    and month = FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    ),

    ### 3> í•©ì¹˜ê¸°
    thismonthRev2 as (


    select
        a.day, a.lastDay, a.rev
    , b.salesGoalDaily
    , c.maxDay ## ì´ë²ˆë‹¬ ë©°ì¹ ê¹Œì§€ ê¸°ê°„ ì°¼ëŠ”ì§€
    #, case when maxDay >5 then rev ## 5ì¼ì¹˜ ì´ìƒì˜ ë§¤ì¶œì´ ìˆìœ¼ë©´ ê·¸ëƒ¥ ì¼í• ê³„ì‚°
    #       when maxDay<=5 and day=1 then salesGoalDaily ## 5ì¼ì¹˜ ì´í•˜ì˜ ë§¤ì¶œë§Œ ìˆë‹¤ë©´ , 1ì¼ì ë§¤ì¶œì„ ë³´ì •ì¹˜ ì ìš©
    #       else rev end as rev2
    , a.rev as rev2
    from
    ## ì¼ìë³„ ë§¤ì¶œ ì‹¤ì¸¡
    (select * from thismonthRev) as a

    ## ëª©í‘œë§¤ì¶œ (ì›”ë³„, ì¼ë³„)
    cross join
    (select *
    from salesGoal
    ) as b

    ## í˜„ì¬ ë©°ì¹ ê¹Œì§€ ë§¤ì¶œ ìˆëŠ”ì§€ -> 5ì¼ ì´ì „ì¸ì§€ ì´í›„ì¸ì§€ í™•ì¸ìš©ë„
    cross join
    (select cast(max(day) as int64) as maxDay from thismonthRev) as c

    )

    #select * from thismonthRev_and_revGoal order by day

    ### 4> ì „ì²˜ë¦¬
    select cast(current_sales as int64) as current_sales, b.salesGoalMonthly
    from
    (select (rev/maxDay)*lastDay as current_sales
    from
    (select sum(rev2) as rev, max(maxDay) as maxDay, max(lastDay) as lastDay
    from thismonthRev2)
    ) as a
    cross join
    (select * from salesGoal) as b

    """

    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='Daily_revenue_target_revenue_df', value=query_result)

    return True


## ì „ë…„ ëŒ€ë¹„ ì›” ë§¤ì¶œ ì¶”ì´ ìˆ˜ì • - ë‹¹ì›”ì€ ì¼í• ê³„ì‚° ë§¤ì¶œ
def merge_daily_revenue(joyplegameid: int, gameidx: str, **context):

    s_total = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_query',
        key='daily_revenue_df'
    )
    val_total = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_YOY_query',
        key='Daily_revenue_YOY_df'
    )

    val = val_total.iat[0, 0]
    s = s_total.iloc[:, 2]
    try:
        idx = s.dropna().index[-1]                 # ë§ˆì§€ë§‰ non-null ë¼ë²¨ ì¸ë±ìŠ¤
        s_total.loc[idx, s_total.columns[2]] = val
    except IndexError:
        pass  # ëª¨ë‘ nullì¸ ê²½ìš°

    return s_total


## í”„ë¡¬í”„íŠ¸ 
### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸
def daily_revenue_gemini(joyplegameid: int, service_sub: str, **context):
    query_result1_dailySales = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_query',
        key='daily_revenue_df'
    )
    query_result1_monthlySales = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_YOY_query',
        key='Daily_revenue_YOY_df'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response1_salesComment = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¹ì›” ë§¤ì¶œì€ ì¼í• ê³„ì‚°ì‹œ {f"{int((query_result1_monthlySales.iat[0,0])):,}"}ì´ê³  ëª©í‘œëŠ” {f"{int((query_result1_monthlySales.iat[0,1])):,}"}ì´ì•¼.
    ë‹¹ì›” ë§¤ì¶œì€ ì¼í• ê³„ì‚°ì‹œ ~~ì´ê³  ëª©í‘œë§¤ì¶œì€ ~~ ìœ¼ë¡œ, ëª©í‘œëŒ€ë¹„ ì–¼ë§ˆ ë‹¬ì„±í–ˆë‹¤ì˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.
    ê·¸ë¦¬ê³  ì¶”ê°€ë¡œ ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•´ ì•„ì£¼ ê°„ë‹¨íˆ ì½”ë©˜íŠ¸ë¥¼ í•´ì¤˜.
    ~ìŠµë‹ˆë‹¤ ì²´ë¡œ ì•Œë ¤ì¤˜

    ê·¸ë¦¬ê³  ì „ë…„ë™ì›”ëŒ€ë¹„ì–´ë–¤ì§€ 3ì¤„ì´ë‚´ë¡œ ê°„ë‹¨íˆ ì•Œë ¤ì¤˜.

    ì•ìœ¼ë¡œ ì–´ë–»ê²Œ í•´ì•¼ê² ë‹¤ëŠ” ì‚¬ê²¬ì€ ì“°ì§€ë§ˆ.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    <ì¼ìë³„ ì´ ë§¤ì¶œ>
    {query_result1_dailySales}

    < ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ>
    {query_result1_monthlySales}
    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=LABELS
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response1_salesComment.text

# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

# ## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

## ê·¸ë˜í”„ ê·¸ë¦¬ê¸° : arg ê°’ìœ¼ë¡œ ê²Œì„ ì½”ë“œ
def daily_revenue_graph_draw(joyplegameid: int, gameidx: str, **context):

    df_daily = context['task_instance'].xcom_pull(
        task_ids='daily_revenue_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='daily_revenue_df'
    )
    
    x  = df_daily.iloc[:, 0]
    y1 = pd.to_numeric(df_daily.iloc[:, 1], errors='coerce')
    y2 = pd.to_numeric(df_daily.iloc[:, 2], errors='coerce')

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(x, y2, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            label=df_daily.columns[2])
    ax.plot(x, y1, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            linestyle='--', label=df_daily.columns[1])  # ê²¹ì³ì„œ í‘œì‹œ

    # ì˜µì…˜
    plt.title("ì¼ìë³„ ë§¤ì¶œ")
    #plt.xlabel(query_result1_dailySales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result1_dailySales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(df_daily[df_daily.columns[0]][::2], rotation=45)

    # ë²”ë¡€ í‘œì‹œ - ê·¸ë˜í”„ë‘ ì•ˆê²¹ì¹˜ê²Œ
    plt.legend(
        bbox_to_anchor=(1.05, 1),   # ê·¸ë˜í”„ ì˜¤ë¥¸ìª½ ë°”ê¹¥ (x=1.05, y=1)
        loc='upper left',           # ì•µì»¤ ê¸°ì¤€ ìœ„ì¹˜
        borderaxespad=0.             # ì¶•ê³¼ ê°„ê²©
    )

    # yì¶• 0ë¶€í„° ì‹œì‘ (ì•ˆí•˜ë©´ ëˆˆê¸ˆ ìµœì†Œê°’ ìì¢… ì¡°ì •)
    plt.ylim(0, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§

    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„
    #plt.grid(axis='x', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    #plt.xlabel("ë‚ ì§œ")
    #plt.ylabel("ë§¤ì¶œ")

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    ####################################### ì´ë¯¸ì§€ íŒŒì¼ì„ ì €ì¥í•  pathê°€ í•„ìš”í•¨ #####################
    filepath1_dailySales = "graph1_dailySales.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


## ì›”ê°„ ë§¤ì¶œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
def daily_revenue_YOY_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result1_monthlySales = context['task_instance'].xcom_pull(
        task_ids='Daily_revenue_YOY_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='Daily_revenue_YOY_df'
    )

    x  = query_result1_monthlySales.iloc[:, 0]
    y1 = pd.to_numeric(query_result1_monthlySales.iloc[:, 1], errors='coerce')
    y2 = pd.to_numeric(query_result1_monthlySales.iloc[:, 2], errors='coerce')

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(x, y2, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            label=query_result1_monthlySales.columns[2])
    ax.plot(x, y1, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            linestyle='--', label=query_result1_monthlySales.columns[1])  # ê²¹ì³ì„œ í‘œì‹œ

    # ì˜µì…˜
    plt.title("ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ")
    #plt.xlabel(query_result1_monthlySales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result1_monthlySales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(query_result1_monthlySales[query_result1_monthlySales.columns[0]][::1], rotation=45)

    # ë²”ë¡€ í‘œì‹œ - ê·¸ë˜í”„ë‘ ì•ˆê²¹ì¹˜ê²Œ
    plt.legend(
        bbox_to_anchor=(1.05, 1),   # ê·¸ë˜í”„ ì˜¤ë¥¸ìª½ ë°”ê¹¥ (x=1.05, y=1)
        loc='upper left',           # ì•µì»¤ ê¸°ì¤€ ìœ„ì¹˜
        borderaxespad=0.             # ì¶•ê³¼ ê°„ê²©
    )

    # y ì¶• ì¡°ì • (20ì–µë¶€í„°)
    plt.ylim(2000000000, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§

    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„
    #plt.grid(axis='x', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    #plt.xlabel("month")
    #plt.ylabel("ë§¤ì¶œ")

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    filePath1_monthlySales = "graph1_monthlySales.png"
    plt.savefig(filePath1_monthlySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filePath1_monthlySales}')
    blob.upload_from_filename(filePath1_monthlySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filePath1_monthlySales)

    return f'{gameidx}/{filePath1_monthlySales}'



# 1) íŒŒì¼ ê²½ë¡œ
def merge_daily_graph(joyplegameid: int, gameidx: str):
    p1 = daily_revenue_graph_draw(joyplegameid, gameidx)
    p2 = daily_revenue_YOY_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph1_dailySales_monthlySales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


def daily_revenue_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )
    query_result1_dailySales=context['task_instance'].xcom_pull(
        task_ids='daily_revenue_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='daily_revenue_df'
    )

    query_result1_monthlySales=context['task_instance'].xcom_pull(
        task_ids='Daily_revenue_YOY_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='Daily_revenue_YOY_df'
    )

    notion.blocks.children.append(
        PAGE_INFO["id"],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "\n\n1. ì¼ìë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    gcs_path = f'{gameidx}/graph1_dailySales_monthlySales.png'
    blob = bucket.blob(gcs_path)
    image_bytes = blob.download_as_bytes()
    filename = 'graph1_dailySales_monthlySales.png'

    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    # ì¼ìë³„ ë§¤ì¶œ
    # ê·¸ë˜í”„ëŠ” íŒŒì¼ ì €ì¥í›„ ì˜¬ë¦¬ëŠ” êµ¬ì¡°ë°–ì— ë˜ì§€ì•Šì•„ì„œ
    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": filename,
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    upload_url = file_upload[upload_url]

    # 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
    headers_upload = {
        "Content-Type": "image/png"
    }
    requests.put(upload_url, headers=headers_upload, data=image_bytes)

    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨
    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, BytesIO(image_bytes), "image/png")}
    headers_send = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION
    }
    send_resp = requests.post(send_url, headers=headers_send, files=files)
    send_resp.raise_for_status()

    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{file_upload_id}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO["id"],
        df=query_result1_dailySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¼ìë³„ ë§¤ì¶œ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO["id"],
        df=query_result1_monthlySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    response1_salesComment = daily_revenue_gemini(joyplegameid=joyplegameid)

    ## ì œë¯¸ë‚˜ì´
    blocks = md_to_notion_blocks(response1_salesComment)
    notion.blocks.children.append(
        block_id=PAGE_INFO["id"],
        children=blocks
    )

    return True




# 2. ìì²´ê²°ì œ ë§¤ì¶œ
def inhouse_sales_query(joyplegameid: int, **context):
    query = f"""
    select logdatekst, cast(sum(pgpricekrw) as int64) as rev
    from
    (SELECT t1.*,
        t2.PGRole,
        t2.PlatformDeviceTypeName,
        t2.PGName,
        t2.PGBuyCount,
        t2.PGPriceKRW
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V` AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = {joyplegameid}
    and pgrole = 'ìì²´ê²°ì œ'
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )
    group by 1 order by 1

    """
    query_result = query_run_method('2_inhouse_sales', query)

    context['task_instance'].xcom_push(key='inhouse_sales_df', value=query_result)

    return True

### 2> 24ë…„ë¶€í„° ì›”ë³„ ìì²´ê²°ì œ ë§¤ì¶œ
def inhouse_sales_before24_query(joyplegameid: int, **context):
    query = f"""
    select a.month
    , cast(a.rev_all as int64) as rev_all
    , cast(b.rev as int64) as rev_self
    , safe_divide(b.rev,a.rev_all) as self_per
    from
    (select month, sum(pricekrw) as rev_all
    from
    (select *,format_date('%Y-%m', logdatekst ) as month
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    group by 1
    ) as a

    left join
    (select month, cast(sum(pgpricekrw) as int64) as rev
    from
    (SELECT t1.*,
        t2.PGRole,
        t2.PlatformDeviceTypeName,
        t2.PGName,
        t2.PGBuyCount,
        t2.PGPriceKRW,
        format_date('%Y-%m', logdatekst ) as month
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V` AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = {joyplegameid}
    and pgrole = 'ìì²´ê²°ì œ'
    and logdatekst>='2024-01-01'
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    group by 1) as b
    on a.month = b.month
    order by month
    """

    query_result = query_run_method('2_inhouse_sales', query)
    context['task_instance'].xcom_push(key='inhouse_sales_before24_df', value=query_result)

    return True

## ì œë¯¸ë‚˜ì´ í”„ë¡¬í”„íŠ¸ 
def inhouses_revenue_gemini(joyplegameid: int, **context):
    
    inhouse_sales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_query',
        key='inhouse_sales_df'
    )
    inhouse_sales_before24 = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )

    prompt_2 = f"""

    1. ì•„ë˜ëŠ” ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œê³¼ ê³¼ê±°ë¶€í„° ì¥ê¸°ì ì¸ ìì²´ê²°ì œ ë§¤ì¶œì´ì•¼.ê°„ë‹¨í•˜ê²Œ í•´ì„í•´ì¤˜.
    2. ìì²´ê²°ì œì— ëŒ€í•œ ì •ì˜ë¥¼ ì“¸ í•„ìš”ëŠ” ì—†ì–´.
    3. ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ê³ , ì¥ê¸°ì ì¸ ìì²´ê²°ì œ íŠ¸ë Œë“œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜.
    4. ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œì™€ ì¥ê¸°ì ì¸ ìì²´ê²°ì œ íŠ¸ë Œë“œ ë‹¨ë½ì„ ë‚˜ëˆ ì¤˜. ì˜ˆ) <ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œ> , <ì¥ê¸°ì  ìì²´ê²°ì œ íŠ¸ë Œë“œ>
    5. ë‚´ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ë§ˆ.
    6. ë³€ìˆ˜ëª…ì— ëŒ€í•´ì„  ì–¸ê¸‰í•˜ì§€ë§ˆ.
    7. ìì²´ê²°ì œë€, êµ¬ê¸€ì´ë‚˜ ì• í”Œì˜ ë§ˆì¼“ ìˆ˜ìˆ˜ë£Œë¥¼ ì ˆê°í•˜ê¸° ìœ„í•´ ìˆ˜ìˆ˜ë£Œê°€ ë‚®ì€ ê²°ì œ í”Œë«í¼ì—ì„œ ê²°ì œí•˜ëŠ” ê²ƒì„ ë§í•´.
    8. ë‹¤ìŒì€ ìì²´ê²°ì œì— ëŒ€í•œ ë¶„ì„ì…ë‹ˆë‹¤ ì´ëŸ° ì‚¬ì „ì— ë§ í•˜ì§€ë§ê³  ê·¸ëƒ¥ ë¶„ì„ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜
    9. ì‚¬ê²¬ì„ ì“°ì§€ë§ê³  ê·¸ëƒ¥ í˜„ì¬ ìƒí™©ì— ëŒ€í•´ íŒ©íŠ¸ë§Œ ì•Œë ¤ì¤˜.


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.

    <ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ>
    {inhouse_sales}

    < ì¥ê¸°ì  ìì²´ê²°ì œ ë§¤ì¶œ>
    {inhouse_sales_before24}

    """

    response2_selfPaymentSales = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt_2,
        config=types.GenerateContentConfig(

            # ì˜ì–´ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì˜ ì´í•´í•  ìˆ˜ ìˆìŒ.
            system_instruction=SYSTEM_INSTRUCTION,
            #tools=[rag_retrieval_tool_test],
            temperature=0.5,
            labels=labels
            # max_output_tokens=2048
        )
    )

    return response2_selfPaymentSales.text

## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

def inhouse_revenue_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result2_dailySelfPaymentSales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_query',
        key='inhouse_sales_df'
    )
    
    # Seaborn ì„  ê·¸ë˜í”„
    sns.lineplot(
        x= query_result2_dailySelfPaymentSales.columns[0],
        y=query_result2_dailySelfPaymentSales.columns[1],
        data=query_result2_dailySelfPaymentSales,
        marker="o"
        )

    # ì˜µì…˜
    plt.title("ì´ë²ˆë‹¬ ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ")
    #plt.xlabel(query_result2_dailySelfPaymentSales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result2_dailySelfPaymentSales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(query_result2_dailySelfPaymentSales[query_result2_dailySelfPaymentSales.columns[0]][::1], rotation=45)

    # yì¶• 0ë¶€í„° ì‹œì‘
    plt.ylim(0, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§
    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    # x,yì¶• ì œê±°
    plt.xlabel(None)
    plt.ylabel(None)

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨

    ####################################### ì´ë¯¸ì§€ íŒŒì¼ì„ ì €ì¥í•  pathê°€ í•„ìš”í•¨ #####################
    filepath1_inhouseSales = "graph1_dailySelfPaymentSales.png"
    plt.savefig(filepath1_inhouseSales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_inhouseSales}')
    blob.upload_from_filename(filepath1_inhouseSales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_inhouseSales)

    return f'{gameidx}/{filepath1_inhouseSales}'



## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

def inhouse_revenue_monthly_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result2_monthlySelfPaymentSales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )

    # Figure & Axes ìƒì„±
    fig, ax1 = plt.subplots(figsize=(10,5))

    # ì˜µì…˜
    plt.title("ì›”ë³„ ìì²´ê²°ì œ ë§¤ì¶œ & ìì²´ê²°ì œ ë§¤ì¶œ ë¹„ì¤‘ (24ë…„1ì›”~) ")

    # ì²« ë²ˆì§¸ yì¶• (ì™¼ìª½, ë§‰ëŒ€ê·¸ë˜í”„)
    ax1.bar(query_result2_monthlySelfPaymentSales["month"],
            query_result2_monthlySelfPaymentSales["rev_self"],
            color="#5B9BD5",
            #label="Sales"
            )
    #ax1.set_ylabel("Sales", color="black")
    ax1.tick_params(axis="y", labelcolor="black")

    # ë‘ ë²ˆì§¸ yì¶• (ì˜¤ë¥¸ìª½, ì„ ê·¸ë˜í”„)
    ax2 = ax1.twinx()
    ax2.plot(query_result2_monthlySelfPaymentSales["month"],
            query_result2_monthlySelfPaymentSales["self_per"],
            color="#ED7D31",
            marker="o",
            #label="Users"
            )
    #ax2.set_ylabel("Users", color="black")
    ax2.tick_params(axis="y", labelcolor="black")

    # ğŸ‘‰ ì„  ìœ„ì— ë°ì´í„° ë ˆì´ë¸” í‘œì‹œ
    for x, y in zip(query_result2_monthlySelfPaymentSales["month"],
                    query_result2_monthlySelfPaymentSales["self_per"]):
        ax2.annotate(f"{y:.0%}",  # 0.23 â†’ "23%"
                    xy=(x, y),
                    xytext=(0, 5),  # ì‚´ì§ ìœ„ë¡œ
                    textcoords="offset points",
                    ha="center", color="black"
                    )


    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    ax1.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))

    # í¼ì„¼íŠ¸ í¬ë§· ìë™ ì ìš© (self_perê°€ 0~1ì´ë©´ 1.0, 0~100ì´ë©´ 100)
    maxv = float(query_result2_monthlySelfPaymentSales["self_per"].max())
    ax2.yaxis.set_major_formatter(PercentFormatter(1.0 if maxv <= 1.5 else 100))

    # xì¶• ëˆˆê¸ˆ(ê°’) ì„¸ë¡œ íšŒì „ â€” ì¶• ê°ì²´ì— ì§ì ‘ ì ìš©
    for tick in ax1.get_xticklabels():
        tick.set_rotation(90)
        tick.set_ha("center")  # ë˜ëŠ” "right"ë¡œ ë°”ê¿”ë„ ë¨

    # ì œëª© & ê²©ì
    ax1.grid(axis="y", linestyle="--", alpha=0.7)

    plt.tight_layout()

    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    filepath1_inhouseMonthlySales = "graph1_monthlySelfPaymentSales.png"
    plt.savefig(filepath1_inhouseMonthlySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_inhouseMonthlySales}')
    blob.upload_from_filename(filepath1_inhouseMonthlySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_inhouseMonthlySales)

    return f'{gameidx}/{filepath1_inhouseMonthlySales}'


## ê·¸ë˜í”„ í•©ì¹˜ê¸°
### ìì²´ê²°ì œ ì¼ìë³„ + ìì²´ê²°ì œ ì›”ë³„

### Rê·¸ë£¹ë³„ ë§¤ì¶œê·¸ë˜í”„ì™€ PU ê·¸ë˜í”„ í•©ì¹˜ê¸°

def merge_inhouse_graph(joyplegameid: int, gameidx: str):
    # 1) íŒŒì¼ ê²½ë¡œ
    p1 = inhouse_revenue_graph_draw(joyplegameid, gameidx)
    p2 = inhouse_revenue_monthly_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph2_selfPaymentSales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path



def inhouse_revenue_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )
    query_result1_inhouseSales = context['task_instance'].xcom_pull(
        task_ids = 'inhouse_sales_query',
        key='inhouse_sales_df'
    )
    query_result1_inhouseMonthlySales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )
    
    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "2. ìì²´ê²°ì œ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    gcs_path = f'{gameidx}/graph2_selfPaymentSales.png'
    blob = bucket.blob(gcs_path)
    image_bytes = blob.download_as_bytes()
    filename = 'graph2_selfPaymentSales.png'

    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": filename,
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    upload_url = file_upload[upload_url]

    # 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
    headers_upload = {
        "Content-Type": "image/png"
    }
    requests.put(upload_url, headers=headers_upload, data=image_bytes)

    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, BytesIO(image_bytes), "image/png")}
    headers_send = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION
    }
    send_resp = requests.post(send_url, headers=headers_send, files=files)
    send_resp.raise_for_status()

    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{file_upload_id}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ## (3) ë¡œë°ì´í„°
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result1_inhouseSales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result1_inhouseMonthlySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¥ê¸° ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ## (4) ì œë¯¸ë‚˜ì´ í•´ì„
    gemini_text = inhouses_revenue_gemini(joyplegameid)
    blocks = md_to_notion_blocks(gemini_text)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True




## ì´ë²ˆë‹¬ ê°€ì… ìœ ì €ì˜ êµ­ê°€ë³„ ë§¤ì¶œ
def cohort_by_country_revenue(joyplegameid: int, **context):
    query = f"""
    with countryRev as (
    select country2 as country, rev_rank2 as rev_rank, sum(rev) as rev
    from
    (select country, rev
    , case when rev_rank <= 9 then country  when rev_rank > 9 then 'etc' end as country2 # rev ê¸°ì¤€ 10ìœ„ë¶€í„°ëŠ” etc ë¡œ í‘œê¸°
    , case when rev_rank <= 9 then rev_rank when rev_rank > 9 then 10 end as rev_rank2
    from
    (select country, rev, row_number() OVER (ORDER BY rev desc ) AS rev_rank # rev ìˆœì„œëŒ€ë¡œ ë­í¬
    from
    (select countrycode as country, cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1)
    )
    )
    group by 1,2
    order by rev_rank # etc êµ­ê°€ê°€ ë§¨ ë’¤ë¡œ ê°€ì•¼í•¨
    ),

    allRev as  (
    select cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.rev,b.rev) as rev_percent
    from
    (select * from countryRev ) as a
    cross join
    (select * from allRev) as b
    order by rev_rank

    """
    query_result=query_run_method('3_global_ua', query)

    context['task_instance'].xcom_push(key='cohort_by_country_revenue_df', value=query_result)
    
    return True

## ì´ë²ˆë‹¬ êµ­ê°€ë³„ COST
def cohort_by_country_cost(joyplegameid: int, **context):
    query = f"""
    with countryCost as (
    select country2 as country, cost_rank2 as cost_rank, sum(cost) as cost
    from
    (select country, cost
    , case when cost_rank <= 9 then country when cost_rank > 9 then 'etc' end as country2 # cost ìˆœì„œëŒ€ë¡œ 10ìœ„ ë¶€í„°ëŠ” etc ë¡œ
    , case when cost_rank <= 9 then cost_rank else 10 end as cost_rank2
    from
    (select country, cost, row_number() OVER (ORDER BY cost desc ) AS cost_rank # cost ìˆœì„œë¡œ rank
    from
    (select countrycode as country, cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by countrycode))
    )
    group by 1,2
    order by cost_rank # etc êµ­ê°€ê°€ ë§¨ ë’¤ë¡œ ê°€ì•¼í•¨
    ),

    allCost as (
    select cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.cost,b.cost) as cost_percent
    from
    (select * from countryCost) as a
    cross join
    (select * from allCost) as b
    order by cost_rank


    """
    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='cohort_by_country_cost_df', value=query_result)
    
    return True


## êµ­ê°€ë³„ rev, cost í”„ë¡¬í”„íŠ¸
### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸
def cohort_by_gemini(joyplegameid: int, **context):
    
    cohort_country_revenue = context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_revenue',
        key='cohort_by_country_revenue_df'
    )
    cohort_country_cost = context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_cost',
        key='cohort_by_country_cost_df'
    )

    #client = genai.Client(api_key="AIzaSyAVv2B6DM6w9jd1MxiP3PbzAEMkl97SCGY")
    response3_revAndCostByCountry = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""
    ì´ë²ˆë‹¬ì— ì–´ë–¤ êµ­ê°€ì— ë§ˆì¼€íŒ…í–ˆê³ , ì–´ë–¤ êµ­ê°€ì—ì„œ ì‹ ê·œìœ ì €ì˜ ë§¤ì¶œì´ ë‚˜ì™”ëŠ”ì§€ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì¤„ê²Œ.
    ê°„ë‹¨í•˜ê²Œ í˜„í™© ìš”ì•½í•´ì¤˜.

    ë‹¹ì›” ë§ˆì¼€íŒ…ë¹„ìš©ì€ ì–¼ë§ˆì´ë©° ë‹¹ì›” ì‹ ê·œìœ ì € ë§¤ì¶œì€ ì–¼ë§ˆì…ë‹ˆë‹¤ë¥¼ ë¨¼ì € í•œì¤„ë¡œ ì„œë‘ì— ì–¸ê¸‰í•´ì¤˜.
    ì´ë²ˆë‹¬ COSTë§ì´ ì“´ êµ­ê°€ë“¤ ê°ê° COST ë¹„ì¤‘ì´ ëª‡% ì¸ì§€ í•œì¤„ì— ì¨ì£¼ê³ ,
    ì‹ ê·œìœ ì € ë§¤ì¶œ ë†’ì€ êµ­ê°€ë“¤ ê°ê° ëª‡% ë§¤ì¶œ ë¹„ì¤‘ì¸ì§€ í•œì¤„ì— ì¨ì¤˜ ì•Œë ¤ì¤˜.
    ê·¸ë¦¬ê³  ì£¼ìš” êµ­ê°€ë“¤ì— ëŒ€í•´ì„œ COST ë¹„ì¤‘ê³¼ ë§¤ì¶œë¹„ì¤‘ì„ ë¹„êµí•´ì„œ íŠ¹ì´í•œì ì´ ìˆëŠ”ê²ƒë§Œ ì•Œë ¤ì¤˜.

    ë§¤ì¶œê³¼ COST ì˜ ì•¡ìˆ˜ ì ˆëŒ“ê°’ì„ ë¹„êµí•˜ì§€ ë§ê³  ë¹„ì¤‘ì„ ë¹„êµí•´ì¤˜.
    etc ëŠ” ê¸°íƒ€ êµ­ê°€ë“¤ ì´ í•© í•œ ê°’ì´ë¼ì„œ etc ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ì•„ì¤˜.
    ë§ˆì¼€íŒ… íš¨ìœ¨ê°œì„ ì´ í•„ìš”í•˜ë‹¤ëŠ”ë§ì€ í•˜ì§€ë§ì•„ì¤˜.

    <ì›í•˜ëŠ” ì„œì‹>
    1. ìš”ì•½í•´ì£¼ê² ë‹¤ ë§ í•˜ì§€ë§ê³  ìš”ì•½í•œ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ì ì–´ì£¼ë©´ ë¼.
    2. ìŠµë‹ˆë‹¤. ì²´ë¡œ ì¨ì¤˜
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.

    <ë°ì´í„° ì„¤ëª…>
    ë§¤ì¶œì´ë‘ ë§ˆì¼€íŒ… ë¹„ìš©ì´ë‘ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ 9ê°œ êµ­ê°€ì™€ ê·¸ ì´í›„ 10ë²ˆì§¸ êµ­ê°€ë¶€í„°ëŠ” ì „ë¶€ etc êµ­ê°€ë¡œ ì²˜ë¦¬í–ˆì–´.
    etc ëŠ” êµ­ê°€ê°€ ì•„ë‹ˆë¼ ë‚˜ë¨¸ì§€ êµ­ê°€ ì´í•©ì´ì•¼.

    <ì´ë²ˆë‹¬ ê°€ì…ìœ ì €ì˜ êµ­ê°€ë³„ ë§¤ì¶œ>
    {cohort_country_revenue}

    <ì´ë²ˆë‹¬ êµ­ê°€ë³„ ë§ˆì¼€íŒ… ë¹„ìš©>
    {cohort_country_cost}

    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=labels
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response3_revAndCostByCountry.text


# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

## OSë³„ cost
def os_cost(joyplegameid: int, **context):
    query = f"""
    with osCost as (
    select os, cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by os
    ),

    allCost as (
    select cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.cost,b.cost) as cost_percent
    from
    (select * from osCost) as a
    cross join
    (select * from allCost) as b
    """

    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='os_cost_df', value=query_result)

    return True

## OSë³„ ë§¤ì¶œ
def os_rev(joyplegameid: int, **context):
    query = f"""
    with osRev as (

    select os, rev from (
    select os, cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1)
    where rev>0
    ),

    allRev as (
    select cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.rev,b.rev) as rev_percent
    from
    (select * from osRev) as a
    cross join
    (select * from allRev) as b
    """
    ## 129.93MB
    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='os_rev_df', value=query_result)

    return True


### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸

#client = genai.Client(api_key="AIzaSyAVv2B6DM6w9jd1MxiP3PbzAEMkl97SCGY")
def os_by_gemini(joyplegameid: int, **context):
    
    os_rev_df= context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )
    os_cost_df= context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    response3_revAndCostByOs = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""

    ì´ë²ˆë‹¬ì— IOS ì— ëª‡ % ë§ˆì¼€íŒ… ë¹„ìš© ì‚¬ìš©í–ˆìœ¼ë©° IOS ì˜ ë§¤ì¶œë¹„ì¤‘ì€ ëª‡% ì…ë‹ˆë‹¤.
    ì˜ í˜•ì‹ìœ¼ë¡œ ì•Œë ¤ì¤˜.


    <ì›í•˜ëŠ” ì„œì‹>
    1. ìš”ì•½í•´ì£¼ê² ë‹¤ ë§ í•˜ì§€ë§ê³  ìš”ì•½í•œ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ì ì–´ì£¼ë©´ ë¼.
    2. ìŠµë‹ˆë‹¤. ì²´ë¡œ ì¨ì¤˜
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    <ë°ì´í„° ì„¤ëª…>


    <ì´ë²ˆë‹¬ ê°€ì…ìœ ì €ì˜ OSë³„ ë§¤ì¶œ>
    {os_rev_df}

    <ì´ë²ˆë‹¬ OSë³„ ë§ˆì¼€íŒ… ë¹„ìš©>
    {os_cost_df}
    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=labels
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response3_revAndCostByOs.text

# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

### ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
## êµ­ê°€ë³„ ë§¤ì¶œ

def by_country_revenue_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result3_revByCountry = context['task_instance'].xcom_pull(
        task_ids = 'cohort_by_country_revenue',
        key='cohort_by_country_revenue_df'
    )

    sizes = query_result3_revByCountry["rev"].to_numpy()
    labels = query_result3_revByCountry["country"].to_numpy()
    total  = sizes.sum()

    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )
    ax.set_title("êµ­ê°€ë³„ ë§¤ì¶œ ë¹„ì¤‘", pad=24)
    #plt.title("êµ­ê°€ë³„ ë§¤ì¶œ ë¹„ì¤‘")
    plt.tight_layout()

    filepath1_dailySales = "graph3_revByCountry.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'



def by_country_cost_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result3_costByCountry = context['task_instance'].xcom_pull(
        task_ids = 'cohort_by_country_cost',
        key='cohort_by_country_cost_df'
    )

    ### êµ­ê°€ë³„ Cost
    sizes = query_result3_costByCountry["cost"].to_numpy()
    labels = query_result3_costByCountry["country"].to_numpy()
    total  = sizes.sum()


    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )
    ax.set_title("êµ­ê°€ë³„ COST ë¹„ì¤‘", pad=24)
    plt.tight_layout()


    filepath1_dailySales = "graph3_costByCountry.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


def merge_contry_graph(joyplegameid: int, gameidx: str):
    p1=by_country_revenue_graph_draw(joyplegameid, gameidx)
    p2=by_country_cost_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph3_revAndCostByCountry.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path

### OS ë³„ ë§¤ì¶œ
def os_rev_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result3_revByOs = context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    sizes = query_result3_revByOs["rev"].to_numpy()
    labels = query_result3_revByOs["os"].to_numpy()
    total  = sizes.sum()


    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )

    ax.set_title("OSë³„ ë§¤ì¶œ ë¹„ì¤‘", pad=24)
    plt.tight_layout()

    filepath1_dailySales = "graph3_revByOs.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'
    

### os ë³„ Cost
def os_cost_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result3_costByOs = context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )

    sizes = query_result3_costByOs["cost"].to_numpy()
    labels = query_result3_costByOs["os"].to_numpy()
    total  = sizes.sum()
    
    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )

    ax.set_title("OSë³„ COST ë¹„ì¤‘", pad=24)
    plt.tight_layout()

    filepath1_dailySales = "graph3_costByOs.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


def merge_os_graph(joyplegameid: int, gameidx: str):
    p1 = os_rev_graph_draw(joyplegameid, gameidx)
    p2 = os_cost_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph3_revAndCostByOs.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


#### ë…¸ì…˜ì— ì—…ë¡œë“œ

def country_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "3. ê¸€ë¡œë²Œ ëª¨ê° ì§€í‘œ " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "(1) êµ­ê°€ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ë§¤ì¶œë¹„ì¤‘" }}]
                },
            }
        ],
    )

    query_result3_revByCountry=context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_revenue',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='cohort_by_country_revenue_df'
    )
    query_result3_costByCountry=context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_cost',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='cohort_by_country_cost_df'
    )

    filePath3_revAndCostByCountry = merge_contry_graph(joyplegameid, gameidx)
    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    ## IAP+ìœ ê°€ì ¬
    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": os.path.basename(filePath3_revAndCostByCountry),
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨

    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(filePath3_revAndCostByCountry, "rb") as f:
        files = {"file": (os.path.basename(filePath3_revAndCostByCountry), f, "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
            # Content-Typeì€ filesë¡œ ìë™ ì„¤ì •ë¨
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()


    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ### ë¡œë°ì´í„° ì œê³µ
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_revByCountry,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_costByCountry,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ COST  ",
        max_first_batch_rows=90,
        batch_size=100,
    )


    ### êµ­ê°€ë³„ cost rev ì½”ë©˜íŠ¸
    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„

    text = cohort_by_gemini(joyplegameid)
    blocks = md_to_notion_blocks(text)
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


    ## ë¶€ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "(2) OSë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ë§¤ì¶œë¹„ì¤‘" }}]
                },
            }
        ],
    )




## osë³„ cost, rev ê·¸ë˜í”„
########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
## IAP+ìœ ê°€ì ¬
def country_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    query_result3_costByOs= context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )
    query_result3_revByOs= context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    filePath3_revAndCostByOs = merge_os_graph(joyplegameid, gameidx)

    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": os.path.basename(filePath3_revAndCostByOs),
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨

    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(filePath3_revAndCostByOs, "rb") as f:
        files = {"file": (os.path.basename(filePath3_revAndCostByOs), f, "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
            # Content-Typeì€ filesë¡œ ìë™ ì„¤ì •ë¨
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()


    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ### ë¡œë°ì´í„° ì œê³µ
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_revByOs,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¥ê¸° ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_costByOs,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - OSë³„ COST ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ## osë³„ cost, rev ì½”ë©˜íŠ¸
    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„
    blocks = md_to_notion_blocks(os_by_gemini(joyplegameid))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


# ìµœê·¼ 30ì¼ ê¸°ì¤€ êµ­ê°€ê·¸ë£¹ë³„ X ê²°ì œì²˜ë³„ ë§¤ì¶œ ì¿¼ë¦¬
# ì§€ë¦¬ì  ì£¼ìš” êµ­ê°€ë³„ë¡œ ê·¸ë£¹í™”
# í•œêµ­
# ë¯¸êµ­
# ì¼ë³¸
# ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„: ì¤‘êµ­, ëŒ€ë§Œ, í™ì½©, ì‹±ê°€í¬ë¥´, íƒœêµ­, ë² íŠ¸ë‚¨, ë§ë ˆì´ì‹œì•„, í•„ë¦¬í•€, ì¸ë„ë„¤ì‹œì•„, ì¸ë„, í˜¸ì£¼, ë‰´ì§ˆëœë“œ
# ì¤‘ë™: ì•„ëì—ë¯¸ë¦¬íŠ¸, ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„, í„°í‚¤, ì´ë€, ì´ìŠ¤ë¼ì—˜, ì¹´íƒ€ë¥´, ì¿ ì›¨ì´íŠ¸, ì˜¤ë§Œ, ë°”ë ˆì¸, ìš”ë¥´ë‹¨
# ì„œìœ ëŸ½: ì˜êµ­, í”„ë‘ìŠ¤, ë…ì¼, ì´íƒˆë¦¬ì•„, ìŠ¤í˜ì¸, ë„¤ëœë€ë“œ, ë²¨ê¸°ì—, ìŠ¤ìœ„ìŠ¤, ì˜¤ìŠ¤íŠ¸ë¦¬ì•„, ì•„ì¼ëœë“œ, í¬ë¥´íˆ¬ê°ˆ
# ë™ìœ ëŸ½: í´ë€ë“œ, ì²´ì½”, í—ê°€ë¦¬, ë£¨ë§ˆë‹ˆì•„, ìŠ¬ë¡œë°”í‚¤ì•„, ëŸ¬ì‹œì•„, ìš°í¬ë¼ì´ë‚˜, ë¶ˆê°€ë¦¬ì•„, ìŠ¬ë¡œë² ë‹ˆì•„, í¬ë¡œì•„í‹°ì•„
# ì•„ë©”ë¦¬ì¹´: ìºë‚˜ë‹¤, ë©•ì‹œì½”, ë¸Œë¼ì§ˆ, ì•„ë¥´í—¨í‹°ë‚˜, ì¹ ë ˆ, ì½œë¡¬ë¹„ì•„, í˜ë£¨
# ê¸°íƒ€: ê·¸ ì™¸ êµ­ê°€

def country_group_rev(joyplegameid: int, gameidx: str, **context):
    query = f"""
    with chk as (
    SELECT
    perf.LogDateKST,
    perf,AuthAccountName,
    perf.CountryGroup,
        pg.PGRole,
        pg.PlatformDeviceTypeName,
        pg.PGName,
        pg.PGBuyCount,
        pg.PGPriceKRW
    FROM
    (
        select * except(CountryGroup)
            , CASE
    WHEN CountryCode = 'KR' THEN 'í•œêµ­'
    WHEN CountryCode = 'JP' THEN 'ì¼ë³¸'
    WHEN CountryCode = 'US' THEN 'ë¯¸êµ­'
    WHEN CountryCode IN ('CN', 'TW', 'HK', 'SG', 'TH', 'VN', 'MY', 'PH', 'ID', 'IN', 'AU', 'NZ') THEN 'ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„'
    WHEN CountryCode IN ('AE', 'SA', 'TR', 'IR', 'IL', 'QA', 'KW', 'OM', 'BH', 'JO') THEN 'ì¤‘ë™'
    WHEN CountryCode IN ('GB', 'FR', 'DE', 'IT', 'ES', 'NL', 'BE', 'CH', 'AT', 'IE', 'PT') THEN 'ì„œìœ ëŸ½'
    WHEN CountryCode IN ('PL', 'CZ', 'HU', 'RO', 'SK', 'RU', 'UA', 'BG', 'SI', 'HR') THEN 'ë™ìœ ëŸ½'
    WHEN CountryCode IN ('CA', 'MX', 'BR', 'AR', 'CL', 'CO', 'PE') THEN 'ì•„ë©”ë¦¬ì¹´'
    ELSE 'ê¸°íƒ€'
    END AS CountryGroup
        from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
        where joyplegameid = {joyplegameid}
                and logdateKst >= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 14 DAY)
                and logdatekst <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    ) AS perf,
    UNNEST(perf.PaymentDetailArrayStruct) AS pg
    )

    select CountryGroup, LogDateKST, PGName, sum(PGPriceKRW) as Sales
    from chk
    group by CountryGroup, LogDateKST, PGName
    order by case when CountryGroup = 'í•œêµ­' then 1
                when CountryGroup = 'ë¯¸êµ­' then 2
                when CountryGroup = 'ì¼ë³¸' then 3
                when CountryGroup = 'ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„' then 4
                when CountryGroup = 'ì„œìœ ëŸ½' then 5
                when CountryGroup = 'ë™ìœ ëŸ½' then 6
                when CountryGroup = 'ì•„ë©”ë¦¬ì¹´' then 7
                when CountryGroup = 'ì¤‘ë™' then 8
                when CountryGroup = 'ê¸°íƒ€' then 9
            end
            , LogDateKST, PGName
    """


    query_result = query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='country_group_rev', value=query_result)

    return True

def country_group_to_df(**context):

    query_result = context['task_instance'].xcom_pull(
        task_ids='country_group_rev',
        key='country_group_rev'
    )

    grouped_dfs = {
        country: group_df.pivot_table(
            index="LogDateKST",
            columns="PGName",
            values="Sales",
            aggfunc="sum",
            fill_value=0
        )
        for country, group_df in query_result.groupby("CountryGroup")
    }


    # ë¡œë°ì´í„° ì œê³µìš© ë°ì´í„°í”„ë ˆì„
    grouped_dfs_union = query_result.pivot_table(
        index=["CountryGroup", "LogDateKST"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="PGName",
        values="Sales",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    num_cols = grouped_dfs_union.select_dtypes(include="number").columns
    grouped_dfs_union[num_cols] = grouped_dfs_union[num_cols].astype(int)

    return grouped_dfs, grouped_dfs_union



def country_group_to_df_gemini(joyplegameid: int, service_sub: str, **context):

    query_result = context['task_instance'].xcom_pull(
        task_ids='country_group_rev',
        key='country_group_rev'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response_GeoPGSales = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents=f"""
    ì§€ë‚œ 2ì£¼ê°„ êµ­ê°€ê·¸ë£¹ë³„ë¡œ ì¼ê°„ ê²°ì œì²˜ë³„ ë§¤ì¶œ ë°ì´í„°ì•¼.\n{query_result.to_csv(index=False)}
    ê° êµ­ê°€ê·¸ë£¹ì—ì„œ ê²°ì œì²˜ë³„ë¡œ ì¼ê°„ ë§¤ì¶œíë¦„ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€, ì–´ë–¤ ê²°ì œì²˜ì˜ ë§¤ì¶œì´ ì–¸ì œ ê¸‰ì¦í–ˆëŠ”ì§€ë¥¼ ìš”ì•½í•´ì„œ 6ì¤„ ì´ë‚´ë¡œ ì•Œë ¤ì¤˜.
    ë§¤ì¶œ ê¸‰ì¦ì˜ ì›ì¸ì„ íŒŒì•…í•˜ì§€ëŠ” ë§ì•„ì¤˜.
    #, ##, ###, ####ì€ ì‚¬ìš©í•˜ì§€ ë§ì•„ì¤˜.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.3
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    return response_GeoPGSales.text



def country_group_df_draw(joyplegameid: int, gameidx: str, **context):
    
    gcs_paths = []
    grouped_dfs, _ = country_group_to_df(**context)

    # âœ… ëª¨ë“  ê·¸ë£¹ë³„ë¡œ ê·¸ë˜í”„ ìƒì„±
    for country, df in grouped_dfs.items():
        # index(LogDateKST)ê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if not pd.api.types.is_datetime64_any_dtype(df.index):
            df.index = pd.to_datetime(df.index, errors="coerce")

        # ìˆ«ìí˜• ë³€í™˜ (Salesê°’)
        df = df.map(
            lambda x: pd.to_numeric(str(x).replace(",", "").replace("-", "0"), errors="coerce")
        )

        # âœ… ì¸ë±ìŠ¤ë¥¼ xì¶•ìœ¼ë¡œ ì‚¬ìš©
        x = df.index

        fig, ax = plt.subplots(figsize=(10, 5))

        # ê²°ì œìˆ˜ë‹¨ë³„ ì„ ê·¸ë˜í”„
        for col in df.columns:
            ax.plot(
                x, df[col],
                marker='o', markersize=3, linewidth=1,
                label=col
            )

        plt.title(f"{country} - ì¼ìë³„ ê²°ì œìˆ˜ë‹¨ ë§¤ì¶œ ì¶”ì´")
        plt.xlabel("ë‚ ì§œ")
        plt.ylabel("ë§¤ì¶œì•¡")

        # yì¶• ì²œ ë‹¨ìœ„ í¬ë§·
        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

        # xì¶• ë¼ë²¨ íšŒì „
        plt.xticks(x, rotation=45)

        # ë²”ë¡€, ë³´ì¡°ì„ , ì €ì¥
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()

        filepath1_dailySales = f"graph_{country}.png"
        plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
        plt.close()

        blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
        blob.upload_from_filename(filepath1_dailySales)

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(filepath1_dailySales)

        # gcs ê²½ë¡œ ì¶”ê°€
        gcs_paths.append(f'{gameidx}/{filepath1_dailySales}')

    return gcs_paths
    

def merge_images_by_three_gcs(
    bucket,
    gcs_image_paths: List[str],
    output_dir: str,
    gameidx: str,
    gap: int = 0,
    bg_color: Tuple[int, int, int, int] = (255, 255, 255, 0),
    cleanup_temp: bool = True
) -> List[str]:

    def pad_to_height(img: Image.Image, h: int, bg: Tuple = bg_color) -> Image.Image:
        """ì´ë¯¸ì§€ì˜ ë†’ì´ë¥¼ ë§ì¶°ì¤Œ (ì„¸ë¡œ íŒ¨ë”© ì¶”ê°€)"""
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    print("ì²˜ë¦¬í•  GCS ì´ë¯¸ì§€ ëª©ë¡:")
    for i, path in enumerate(gcs_image_paths, 1):
        print(f"  {i}. {path}")
    print()
    
    uploaded_paths = []
    temp_files = []
    
    # 3ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
    for batch_num, i in enumerate(range(0, len(gcs_image_paths), 3), 1):
        imgs = []
        names = []
        
        # ìµœëŒ€ 3ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ
        for j in range(3):
            if i + j < len(gcs_image_paths):
                gcs_path = gcs_image_paths[i + j]
                
                try:
                    # GCSì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    blob = bucket.blob(gcs_path)
                    image_bytes = blob.download_as_bytes()
                    img = Image.open(BytesIO(image_bytes)).convert("RGBA")
                    imgs.append(img)
                    
                    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° ë° "graph_" ì œê±°
                    name = os.path.splitext(os.path.basename(gcs_path))[0]
                    if name.startswith("graph_"):
                        name = name.replace("graph_", "", 1)
                    names.append(name)
                    
                except Exception as e:
                    print(f"ê²½ê³ : GCSì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {gcs_path}")
                    print(f"  ì—ëŸ¬: {str(e)}")
                    continue
        
        if not imgs:
            print(f"ë°°ì¹˜ {batch_num}: ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # ì„¸ë¡œ ë§ì¶”ê¸°
        target_h = max(img.height for img in imgs)
        imgs_padded = [pad_to_height(img, target_h) for img in imgs]
        
        # ê°€ë¡œë¡œ í•©ì¹˜ê¸°
        total_width = sum(img.width for img in imgs_padded) + gap * (len(imgs_padded) - 1)
        out = Image.new("RGBA", (total_width, target_h), bg_color)
        
        x_offset = 0
        for img in imgs_padded:
            out.paste(img, (x_offset, 0), img)
            x_offset += img.width + gap
        
        # íŒŒì¼ëª… êµ¬ì„±
        merged_filename = "graph_" + " ë° ".join(names) + ".png"
        temp_filepath = f"/tmp/{merged_filename}"
        
        # ë¡œì»¬ì— ì„ì‹œ ì €ì¥
        out.save(temp_filepath)
        temp_files.append(temp_filepath)
        
        # GCS ê²½ë¡œ êµ¬ì„±
        gcs_upload_path = f"{gameidx}/{output_dir}/{merged_filename}"
        
        # GCSì— ì—…ë¡œë“œ
        upload_blob = bucket.blob(gcs_upload_path)
        upload_blob.upload_from_filename(temp_filepath)
        
        print(f"âœ“ ë°°ì¹˜ {batch_num} ì €ì¥ë¨: gs://{bucket.name}/{gcs_upload_path}")
        uploaded_paths.append(gcs_upload_path)
    
    # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if cleanup_temp:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        print(f"\nâœ“ {len(temp_files)}ê°œì˜ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    
    print(f"\nì´ {len(uploaded_paths)}ê°œì˜ í•©ì³ì§„ ì´ë¯¸ì§€ê°€ GCSì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    return uploaded_paths


def merge_country_group_df_draw(joyplegameid: int, gameidx: str, **context):
    """
    Airflow DAGì—ì„œ ì‚¬ìš©í•  wrapper í•¨ìˆ˜
    """
    from google.cloud import storage
    
    # GCS í´ë¼ì´ì–¸íŠ¸ ë° ë²„í‚· ì´ˆê¸°í™”
    client = storage.Client()
    bucket = client.bucket("game-framework1")  # ë²„í‚·ëª… ìˆ˜ì • í•„ìš”
    
    # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ë¦¬ìŠ¤íŠ¸)
    img_gcs_list = country_group_df_draw(joyplegameid, gameidx, **context)
    
    # í•©ì¹˜ê¸° ì²˜ë¦¬
    merged_paths = merge_images_by_three_gcs(
        bucket=bucket,
        gcs_image_paths=img_gcs_list,
        output_dir="merged",  # GCS ë‚´ ì¶œë ¥ ë””ë ‰í† ë¦¬
        gameidx=gameidx,
        gap=0,
        bg_color=(255, 255, 255, 0),
        cleanup_temp=True
    )
    
    return merged_paths


def country_group_data_upload_to_notion(joyplegameid: int, gameidx: str, bucket_name: str = "game-framework1", merged_image_dir: str= "merged", **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "êµ­ê°€ë³„ X ê²°ì œì²˜ë³„ ì§€í‘œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ğŸŒ êµ­ê°€ê·¸ë£¹ ë¶„ë¥˜ ê¸°ì¤€\n1. í•œêµ­\n2. ë¯¸êµ­\n3. ì¼ë³¸\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "4. ì„œìœ ëŸ½: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì˜êµ­, í”„ë‘ìŠ¤, ë…ì¼, ì´íƒˆë¦¬ì•„, ìŠ¤í˜ì¸, ë„¤ëœë€ë“œ, ë²¨ê¸°ì—, ìŠ¤ìœ„ìŠ¤, ì˜¤ìŠ¤íŠ¸ë¦¬ì•„, ì•„ì¼ëœë“œ, í¬ë¥´íˆ¬ê°ˆ\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "5. ë™ìœ ëŸ½: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "í´ë€ë“œ, ì²´ì½”, í—ê°€ë¦¬, ë£¨ë§ˆë‹ˆì•„, ìŠ¬ë¡œë°”í‚¤ì•„, ëŸ¬ì‹œì•„, ìš°í¬ë¼ì´ë‚˜, ë¶ˆê°€ë¦¬ì•„, ìŠ¬ë¡œë² ë‹ˆì•„, í¬ë¡œì•„í‹°ì•„\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "6. ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì¤‘êµ­, ëŒ€ë§Œ, í™ì½©, ì‹±ê°€í¬ë¥´, íƒœêµ­, ë² íŠ¸ë‚¨, ë§ë ˆì´ì‹œì•„, í•„ë¦¬í•€, ì¸ë„ë„¤ì‹œì•„, ì¸ë„, í˜¸ì£¼, ë‰´ì§ˆëœë“œ\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "7. ì•„ë©”ë¦¬ì¹´: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ìºë‚˜ë‹¤, ë©•ì‹œì½”, ë¸Œë¼ì§ˆ, ì•„ë¥´í—¨í‹°ë‚˜, ì¹ ë ˆ, ì½œë¡¬ë¹„ì•„, í˜ë£¨\n"}},
                                            {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "8. ì¤‘ë™: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì•„ëì—ë¯¸ë¦¬íŠ¸, ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„, í„°í‚¤, ì´ë€, ì´ìŠ¤ë¼ì—˜, ì¹´íƒ€ë¥´, ì¿ ì›¨ì´íŠ¸, ì˜¤ë§Œ, ë°”ë ˆì¸, ìš”ë¥´ë‹¨\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "9. ê¸°íƒ€: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ê·¸ ì™¸ êµ­ê°€ë“¤"}},
                        ]
                },
            }
        ],
    )

    # GCS í´ë¼ì´ì–¸íŠ¸ ë° ë²„í‚· ì´ˆê¸°í™”
    gcs_client = storage.Client()
    bucket = gcs_client.bucket(bucket_name)
    
    # GCSì—ì„œ í•©ì³ì§„ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ
    gcs_image_paths = []
    blobs = gcs_client.list_blobs(
        bucket_name,
        prefix=f"{gameidx}/{merged_image_dir}/"
    )

    for blob in blobs:
        # "ë°"ì´ í¬í•¨ëœ PNG íŒŒì¼ë§Œ í•„í„°ë§
        if blob.name.lower().endswith(".png") and "ë°" in blob.name:
            gcs_image_paths.append(blob.name)
    
    # íŒŒì¼ëª… ì—­ìˆœ ì •ë ¬
    gcs_image_paths.sort(reverse=True)
    
    print(f"ì—…ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜: {len(gcs_image_paths)}ê°œ")
    print("ì´ë¯¸ì§€ ëª©ë¡:")
    for path in gcs_image_paths:
        print(f"  - {path}")
    

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    # GCSì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Notion ì—…ë¡œë“œ
    for gcs_path in gcs_image_paths:
        filename = gcs_path.split('/')[-1]
        print(f"\nì—…ë¡œë“œ ì¤‘: {filename}")
        
        try:
            # GCSì—ì„œ ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ
            blob = bucket.blob(gcs_path)
            image_bytes = blob.download_as_bytes()
            
            # íŒŒì¼ ì—…ë¡œë“œ ê°ì²´ ìƒì„±
            create_url = "https://api.notion.com/v1/file_uploads"
            payload = {
                "filename": filename,
                "content_type": "image/png"
            }
            resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
            resp.raise_for_status()
            file_upload = resp.json()
            file_upload_id = file_upload["id"]
            
            # íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡
            send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
            files = {"file": (filename, BytesIO(image_bytes), "image/png")}
            headers_send = {
                "Authorization": f"Bearer {NOTION_TOKEN}",
                "Notion-Version": NOTION_VERSION
            }
            send_resp = requests.post(send_url, headers=headers_send, files=files)
            send_resp.raise_for_status()
            
            # Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ ì²¨ë¶€
            append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
            append_payload = {
                "children": [
                    {
                        "object": "block",
                        "type": "image",
                        "image": {
                            "type": "file_upload",
                            "file_upload": {"id": file_upload_id},
                        }
                    }
                ]
            }
            
            append_resp = requests.patch(
                append_url, headers=headers_json, data=json.dumps(append_payload)
            )
            append_resp.raise_for_status()
            
            print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion API ì—ëŸ¬: {filename}")
            print(f"  ì—ëŸ¬: {str(e)}")
            continue
        except Exception as e:
            print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ì—…ë¡œë“œ ì—ëŸ¬: {filename}")
            print(f"  ì—ëŸ¬: {str(e)}")
            continue
    
    print("\nğŸ‰ ëª¨ë“  ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ!")

    _, grouped_dfs_union =country_group_to_df(**context)

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=grouped_dfs_union,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ X ê²°ì œì²˜ë³„ ì§€í‘œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(country_group_to_df_gemini(joyplegameid, "3_global_ua"))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True


def rev_group_rev_pu(joyplegameid: int, **context):
    query = f"""
    select logdatekst,Week
    , cast(sum(if(rgroup_final = 'R0', pricekrw, null)) as int64)as R0_Sales
    , cast(sum(if(rgroup_final = 'R1', pricekrw, null)) as int64) as R1_Sales
    , cast(sum(if(rgroup_final = 'R2', pricekrw, null)) as int64) as R2_Sales
    , cast(sum(if(rgroup_final = 'R3', pricekrw, null)) as int64) as R3_Sales
    , cast(sum(if(rgroup_final = 'R4', pricekrw, null)) as int64) as R4_Sales
    , cast(sum(if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ', pricekrw, null)) as int64) as `ì „ì›” ë¬´ê³¼ê¸ˆ_Sales`
    , cast(sum(if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì', pricekrw, null)) as int64) as `ë‹¹ì›”ê°€ì…ì_Sales`
    , cast(sum(pricekrw) as int64) as `ì „ì²´ìœ ì €_Sales`

    , count(distinct if(rgroup_final = 'R0' and pricekrw>0 , authaccountname, null)) as R0_PU
    , count(distinct if(rgroup_final = 'R1' and pricekrw>0 , authaccountname, null)) as R1_PU
    , count(distinct if(rgroup_final = 'R2' and pricekrw>0 , authaccountname, null)) as R2_PU
    , count(distinct if(rgroup_final = 'R3' and pricekrw>0 , authaccountname, null)) as R3_PU
    , count(distinct if(rgroup_final = 'R4' and pricekrw>0 , authaccountname, null)) as R4_PU
    , count(distinct if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ' and pricekrw>0 , authaccountname, null)) as `ì „ì›” ë¬´ê³¼ê¸ˆ_PU`
    , count(distinct if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì' and pricekrw>0 , authaccountname, null)) as `ë‹¹ì›”ê°€ì…ì_PU`
    , count(distinct if(pricekrw>0, authaccountname, null)) as `ì „ì²´ìœ ì €_PU`
    from
    (select *, concat(cast(cast(DATE_TRUNC(logdatekst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdatekst,week(Wednesday)) as date), interval 6 day) as string)) as Week
    from `data-science-division-216308.gameInsightFramework.paymentGroup`
    where logdatekst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH))
    and joypleGameID = {joyplegameid}
    group by 1,2
    order by 1
    """

    query_result =query_run_method('4_detail_sales', query)
    context['task_instance'].xcom_push(key='rev_group_rev_pu', value=query_result)

    return True


def rev_group_rev_pu_gemini(joyplegameid: int, service_sub: str, **context):
    rev_group_rev_pu_data = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_RgroupSales = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    1. ì´ë²ˆì£¼ ë§¤ì¶œê³¼ PU íŠ¸ë Œë“œì— ëŒ€í•´, ì§€ë‚œì£¼ì™€ ì§€ì§€ë‚œì£¼ì™€ ë¹„êµí•´ì„œ íŠ¹ë³„í•œ ì ì´ ìˆë‹¤ë©´ ì•Œë ¤ì¤˜.
    ì´í•©ì´ë‚˜ í‰ê· ì ì¸ê²ƒ ë§ê³ ë„ íŠ¸ë Œë“œ ë³€í™”ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜
    2. ì¡´ëŒ“ë§ë¡œ ì¨ì¤˜. 10ì¤„ ë‚´ë¡œ ì¨ì¤˜
    3. ë¶„ì„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ í˜¹ì€ ë¶„ì„í•´ë³´ì•˜ìŠµë‹ˆë‹¤ ë“±ì˜ ë§ì„ ì“°ì§€ë§ê³  ë°”ë¡œ ë¶„ì„í•œ ë‚´ìš©ì— ëŒ€í•´ ì•Œë ¤ì¤˜.
    ì œê³µí•´ì£¼ì‹  ë°ì´í„°ëŠ” ì´ëŸ° ë§ ì“°ì§€ë§ì•„ì¤˜ ë°”ë¡œ ë¶„ì„ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜
    4. ë¹„êµí• ë•Œ, ì´ë²ˆì£¼ê°€ ë‹¤ ì§€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì£¼ì°¨ë„ ë™ì¼í•œ ì¼ìˆ˜ë¥¼ ê°€ì§€ê³  ë¹„êµí•´ì£¼ê³ , ì–´ë–»ê²Œ ë™ì¼ê¸°ê°„ ë¹„êµë˜ì—ˆëŠ”ì§€ë„ ëª…ì‹œí•´ì¤˜.
    5. ë§¤ì¶œì€ ì¼ìë³„ ì´í•©ìœ¼ë¡œ ë¹„êµí•´ë„ ë˜ì§€ë§Œ, PU ëŠ” ê·¸ë‚ ì˜ PU ì´ê¸° ë•Œë¬¸ì— ì´í•©ë³´ë‹¤ëŠ” íŠ¸ë Œë“œë¡œ ë¹„êµí•´ì¤˜.
    6. ë¹„êµí•  ë•ŒëŠ” R0, R1,R2 ê·¸ë£¹ì„ ìœ„ì£¼ë¡œ ë§í•´ì¤˜
    7. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    8. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.e.g : 5100ë§Œì› , 1ì–µ 2ì²œë§Œì›
    9. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    10. í–¥í›„ ì–´ë–»ê²Œ í•´ì•¼ëœë‹¤ëŠ” ë§ì€ í•˜ì§€ ë§ì•„ì¤˜.
    11. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    í•µì‹¬ë‚´ìš©ì€ R0,R1,R2 ì˜ ë§¤ì¶œì´ ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ì§€ê°€ í•„ìš”í•´

    <ì¼ìë³„ ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œì•¡>
    {rev_group_rev_pu_data}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_RgroupSales.text


def iap_gem_ruby(joyplegameid:int, databaseschema: str='GW', **context):
    query = f"""
    select logdate_kst,week
    , cast(sum(if(cat_package2='ì „íˆ¬ê¸°', sales_buygem, 0)) as int64) as `ì „íˆ¬ê¸°`
    , cast(sum(if(cat_package2='ì¢…í•©', sales_buygem, 0)) as int64) as `ì¢…í•©`
    , cast(sum(if(cat_package2='ìì›', sales_buygem, 0)) as int64) as `ìì›`
    , cast(sum(if(cat_package2='í•­ê³µëª¨í•¨', sales_buygem, 0)) as int64) as `í•­ê³µëª¨í•¨`
    , cast(sum(if(cat_package2='ì˜ì›…', sales_buygem, 0)) as int64) as `ì˜ì›…`
    , cast(sum(if(cat_package2='êµ°í•¨', sales_buygem, 0)) as int64) as `êµ°í•¨`
    , cast(sum(if(cat_package2='ë°°í‹€íŒ¨ìŠ¤', sales_buygem, 0)) as int64) as `ë°°í‹€íŒ¨ìŠ¤`
    , cast(sum(if(cat_package2='ì—°êµ¬', sales_buygem, 0)) as int64) as `ì—°êµ¬`
    , cast(sum(if(cat_package2='ì¥ë¹„', sales_buygem, 0)) as int64) as `ì¥ë¹„`
    , cast(sum(if(cat_package2 not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„'), sales_buygem, null)) as int64) as `ê¸°íƒ€`
    from
    (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (
    ### IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        )
    union all
    ### GEM
        (
        select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        )
    union all
    ### RUBY
        (
        select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')
        )
    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1,2 order by 1
    """

    query_result =query_run_method('4_detail_sales', query)
    context['task_instance'].xcom_push(key='iap_gem_ruby', value=query_result)

    return True


def iap_gem_ruby_history(gameidx: str, **context):
    query = f"""
    select *
    from (
        select distinct updateDate `ì—…ë°ì´íŠ¸ì¼`
                        , case when category is null then 'ê¸°íƒ€'
                                when category = 'ì´ë²¤íŠ¸ (ìš´ì˜íˆ´)' then 'ì´ë²¤íŠ¸'
                                else category end as `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜`
                        , title as `ì—…ë°ì´íŠ¸ ë‚´ìš©`
        from `data-science-division-216308.gameInsightFramework.{gameidx}_history`
        where date(updateDate)>= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 32 DAY)
        and date(updateDate)<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        and (title is not null or title != '' or title != ' ')
        )
    where `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜` not in ('ê¸°íƒ€', 'ì ê²€ ê¸°ë³¸ ì •ë³´', 'LQA', 'ë²„ê·¸ ìˆ˜ì • ë° ì‚¬ìš©ì„± ê°œì„ ', 'BM_ìƒì ')
    order by `ì—…ë°ì´íŠ¸ì¼`desc
    """

    query_result =query_run_method('4_detail_sales', query)
    # 1ì£¼ì „ ìˆ˜ìš”ì¼ë¶€í„° ì–´ì œì¼ìê¹Œì§€ì˜ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ (ì¿¼ë¦¬ì—ì„œ ì „ì²˜ë¦¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”í›„ ìˆ˜ì • í•„ìš”)

    # ë¬¸ìì—´ ì»¬ëŸ¼ -> datetimeìœ¼ë¡œ íŒŒì‹± (ì‹¤íŒ¨í•œ ê°’ì€ NaT)
    s = pd.to_datetime(query_result['ì—…ë°ì´íŠ¸ì¼'], errors='coerce')

    # í•œêµ­ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜/ì „ì¼
    today = pd.Timestamp.now(tz='Asia/Seoul').normalize().date()
    yesterday = today - pd.Timedelta(days=1)

    # ì˜¤ëŠ˜ ê¸°ì¤€ 'ì§ì „ ìˆ˜ìš”ì¼'(ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ ì˜¤ëŠ˜ ì œì™¸) ê³„ì‚°
    # ì›”=0, í™”=1, ìˆ˜=2, ... ì¼=6
    w = today.weekday()
    delta_to_last_wed = (w - 2) % 7
    if delta_to_last_wed == 0:  # ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ 7ì¼ ì „ì„ 'ì§ì „ ìˆ˜ìš”ì¼'ë¡œ
        delta_to_last_wed = 7
    last_wed = today - pd.Timedelta(days=delta_to_last_wed)

    # ğŸ‘‰ "1ì£¼ ì „ ìˆ˜ìš”ì¼"ì„ ì‹œì‘ì¼ë¡œ
    start_date = last_wed

    # êµ¬ê°„: [1ì£¼ ì „ ìˆ˜ìš”ì¼, ì „ì¼] (ì–‘ë í¬í•¨)
    mask = (s.dt.date >= start_date) & (s.dt.date <= yesterday)
    query_result4_ingameHistory = query_result.loc[mask].copy()

    context['task_instance'].xcom_push(key='iap_gem_ruby_history', value=query_result4_ingameHistory)

    return True


def iap_gem_ruby_gemini(service_sub: str, **context):
    
    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
    )

    query_result4_ingameHistory = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_salesByPackage = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{query_result4_salesByPackage.to_csv(index=False)}

    ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {query_result4_ingameHistory.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage.text


def iap_df(joyplegameid: int, databaseschema: str='GW', **context):
    # IAP
    query = f"""
    WITH base AS (
    SELECT *
    FROM `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    WHERE logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(pricekrw) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (
            SELECT cat_package, sum(rev) AS peak_rev
            FROM daily
            GROUP BY 1
        )
    ORDER BY peak_rev DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
        d.logdate_kst,
        IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4

    """

    query_result =query_run_method('4_detail_sales', query)
    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ Pivot

    query_result4_salesByPackage_IAP = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='iap_df', value=query_result4_salesByPackage_IAP)

    return True


def gem_df(joyplegameid: int, **context):
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='gem' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS usegem
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(usegem) AS peak_usegem
        FROM daily
        GROUP BY 1)
    ORDER BY peak_usegem DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.usegem) AS usegem
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByPackage_GEM = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="usegem",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='gem_df', value=query_result4_salesByPackage_GEM)

    return True


def ruby_df(joyplegameid: int, **context):
    
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='ruby' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS useruby
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(useruby) AS peak_useruby
        FROM daily
        GROUP BY 1)
    ORDER BY peak_useruby DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.useruby) AS useruby
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByPackage_RUBY = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="useruby",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='ruby_df', value=query_result4_salesByPackage_RUBY)

    return True


def iap_df_gemini(service_sub: str, **context):

    iap_df = context['task_instance'].xcom_pull(
        task_ids = 'iap_df',
        key='iap_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_IAP = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{iap_df.to_csv(index=False)}

    ì´ë²ˆì£¼ IAP ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_IAP.text


def gem_df_gemini(service_sub: str, **context):
    gem_df = context['task_instance'].xcom_pull(
        task_ids = 'gem_df',
        key='gem_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_GEM = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ì ¬ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{gem_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ì ¬ ì†Œëª¨ëŸ‰ì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ì ¬ì†Œë¹„ëŸ‰ì„ ì²«ë²ˆì§¸ ìë¦¬ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ì ¬ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ì ¬ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_GEM.text


def ruby_df_gemini(service_sub: str, **context):
    ruby_df = context['task_instance'].xcom_pull(
        task_ids = 'ruby_df',
        key='ruby_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_RUBY = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ë£¨ë¹„ ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{ruby_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë£¨ë¹„ ì†Œëª¨ëŸ‰ ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
    ë‹¨ìœ„ëŠ” ì›ì´ ì•„ë‹ˆë¼ ë£¨ë¹„ë¡œ í‘œê¸°í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ í•œìë¦¬ ìˆ˜ ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë£¨ë¹„ ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_RUBY.text


def weekly_iapcategory_rev(joyplegameid: int, gameidx: str, databaseschema: str, **context):
    
    query = f"""
    with base as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (

        ## IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
        )
    )

    , daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package2, SUM(sales_buygem) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package2
    FROM
        (SELECT cat_package2, sum(rev) AS peak_rev
        FROM daily
        GROUP BY 1)
    ORDER BY peak_rev DESC, cat_package2 ASC
    LIMIT 15
    )

    SELECT d.logdate_kst,
        IF(d.cat_package2 IN (SELECT cat_package2 FROM top_cat), d.cat_package2, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2
    ORDER BY 1,2


    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByCategory = query_result.pivot_table(
        index=["logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    exclude = {'logdate_kst', 'ê¸°íƒ€'}
    cols = [c for c in query_result4_salesByCategory.columns if c not in exclude]

    # ì‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì•ˆì „ ì²˜ë¦¬
    def sq(c: str) -> str:
        return "'" + c.replace("'", "''") + "'"

    query_result4_salesByCategory_Cols = ", ".join(sq(c) for c in cols)

    context['task_instance'].xcom_push(key='weekly_iapcategory_rev', value=query_result4_salesByCategory)
    context['task_instance'].xcom_push(key='weekly_iapcategory_rev_cols', value=query_result4_salesByCategory_Cols)

    return True


def ruby_df_gemini(service_sub: str, **context):
    weekly_iapcategory_rev = context['task_instance'].xcom_pull(
        task_ids = 'weekly_iapcategory_rev',
        key='weekly_iapcategory_rev'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_salesByCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ì§€ë‚œ ì—…ë°ì´íŠ¸ì¼ë¶€í„° ì „ì¼ìê¹Œì§€ì˜ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ë‹¤ìŒì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•´ì„œ ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ê³ , ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í–ˆëŠ”ì§€ë¥¼ í™•ì¸í•´ì¤˜.\n{weekly_iapcategory_rev.to_csv(index=False)}
    ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ëŠ”ì§€ëŠ” ìƒìœ„ 3ê°œë§Œ ì•Œë ¤ì£¼ê³ , ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í•œ ì¹´í…Œê³ ë¦¬ì—ì„œëŠ” ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸í•˜ê³  ì•Œë ¤ì¤˜.
    ì œì–¸ì€ í•˜ì§€ ë§ì•„ì¤˜.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    response4_CategoryListUp = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {response4_salesByCategory.text}\nìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ì—ì„œ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜.
    ('ê³¨ë“œ', 'ì€í™”', 'ì²­ì‚¬ì§„', 'ì „ìˆ  êµë³¸', 'ìì›')
    """,
        config=types.GenerateContentConfig(
            temperature=0
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # evalë¡œ ë¬¸ìì—´ì„ ì‹¤ì œ tupleë¡œ ë³€í™˜
        CategoryListUp_2 = eval(CategoryListUp.group(0))
        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_2])
    # SQL order by ì‹œ, ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•œ ì½”ë“œ
    case_when_str = "\n".join(
        [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_2)]
    )

    return CategoryListUp_SQL, case_when_str, response4_salesByCategory, response4_CategoryListUp



def top3_items_by_category(joyplegameid: int, gameidx: str, databaseschema:str,  service_sub: str, **context):

    weekly_iapcategory_rev_cols = context['task_instance'].xcom_pull(
        task_ids = 'weekly_iapcategory_rev',
        key='weekly_iapcategory_rev_cols'
    )

    CategoryListUp_SQL, case_when_str, _, _ = ruby_df_gemini(service_sub)
    
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
        "run_id": RUN_ID,
        f"datascience_division_service_sub" : {service_sub}}


    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 3 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 3 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ({weekly_iapcategory_rev_cols}) then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
            )
            )
    where cat_package2 in ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    where `ìƒí’ˆ ì´ë¦„` != 'ê·¸ ì™¸ ìƒí’ˆë“¤'
    group by `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc

    """

    query_result=query_run_method('4_detail_sales', query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")

    context['task_instance'].xcom_push(key='top3_items_by_category', value=query_result)

    return True



def top3_items_by_category_gemini(service_sub: str, **context):

    query_result4_salesByPackage_ListedCategory = context['task_instance'].xcom_pull(
        task_ids = 'top3_items_by_category',
        key='top3_items_by_category'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    _, _, response4_salesByCategory = ruby_df_gemini(service_sub)

    response4_salesByPackage_ListedCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {query_result4_salesByPackage_ListedCategory.to_csv(index=False)}
    ìœ„ì˜ ë°ì´í„°ëŠ” ë§¤ì¶œ ìƒìœ„ 3ìœ„ ì¹´í…Œê³ ë¦¬ ë° ë§¤ì¶œ ë³€ë™ì´ ë†’ì•˜ë˜ ì¹´í…Œê³ ë¦¬ë“¤ì˜ ìƒí’ˆë³„ ë§¤ì¶œ ë°ì´í„°ì•¼.
    \n{response4_salesByCategory.text}
    ê·¸ë¦¬ê³  ìœ„ì˜ ë°ì´í„°ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ë³€í™” ìš”ì•½í•œ ë‚´ìš©ì´ì•¼.
    ë‘ ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œ,
    ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ê° ì¼ìë³„ë¡œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€(ëª¨ë“  ë‚ ì§œë¥¼ ì°¸ì¡°í•´ì¤˜),
    ê·¸ ì™¸ ì¹´í…Œê³ ë¦¬ë“¤ì€ ë§¤ì¶œ ë³€ë™ì´ í° ë‚ ì§œì—ë§Œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€ ë¶„ì„í•´ì¤˜.
    ì œì‹œëœ ë°ì´í„°ë§Œìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ì•„ì¤˜.
    ì£¼ì–´ì§„ ë°ì´í„°ê°€ í•˜ë£¨ë§Œ ìˆë‹¤ëŠ” ìœ ì˜ì‚¬í•­ì€ ë§í•˜ì§€ë§ˆ.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.1
            ,labels=LABELS
            # max_output_tokens=9000
        )
    )

    response4_WeeklySales_Draft1 = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
        ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì§€í‘œì— ëŒ€í•œ ìš”ì•½ê¸€ì´ì•¼.\n{response4_salesByCategory.text}
        ê·¸ë¦¬ê³  ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¶œì— ê¸°ì—¬í•œ ìƒí’ˆì— ëŒ€í•œ ì •ë³´ê¸€ì´ì•¼.\n{response4_salesByPackage_ListedCategory.text}
        ë‘ ê¸€ì„ ì¢…í•©í•´ì„œ, ê¸°ê°„ë™ì•ˆ ê²Œì„ì˜ ë§¤ì¶œ ë³€í™”ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.
        ë¦¬í¬íŠ¸ ì‘ì„±ì‹œ ë°˜ë“œì‹œ ì•„ë˜ì˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.

        ì„œë‘ì—ëŠ” 'ê¸ˆì£¼ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸ ë¦¬í¬íŠ¸ (ë°ì´í„° ê¸°ê°„)'ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. Bold ì²˜ë¦¬í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬(ë§¤ì¶œ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì¸ ê²½ìš°, ìˆœìœ„ ì–¸ê¸‰. e.g. ì „íˆ¬ê¸° (ë§¤ì¶œ 1ìœ„). Bold ì²˜ë¦¬í•´ì¤˜.)
        * ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”. ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. í° ë³€í™”ê°€ ìˆê±°ë‚˜ ë§¤ì¶œì´ ë†’ì•˜ë˜ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        * í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ, ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. ì£¼ìš” ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        'ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”'ì™€ 'í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ'ì€ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ 6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

        'ì ¬'ê³¼ 'ë£¨ë¹„'ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì•¼. ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì·¨ê¸‰í•˜ì§€ë§ˆ.
        ìƒí’ˆëª…ì— ìˆëŠ” ë‚ ì§œë¡œ ìƒí’ˆëª…ì˜ ì¶œì‹œëœ ë‚ ì§œë¥¼ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì´ ë§¤ì¶œì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì œì™¸í•œ ì´ë²¤íŠ¸ ë° í”„ë¡œëª¨ì…˜ì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.5
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    response4_WeeklySales_Report = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ë‹¤ìŒì€ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ì•¼.\n{iap_gem_ruby_history.to_csv(index=False)}
    ë‹¤ìŒì˜ ì§€ë‚œ ì—…ë°ì´íŠ¸ ì´í›„ ë§¤ì¶œ ë¶„ì„ ë¦¬í¬íŠ¸ì—ì„œ, ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì´ ìˆëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë‚˜ ìƒí’ˆì´ ìˆë‹¤ë©´ í•´ë‹¹ ì—…ë°ì´íŠ¸ ë‚´ìš©ê³¼ ì—°ê´€ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ì–¸ê¸‰í•´ì¤˜.\n{response4_WeeklySales_Draft1.text}
    ì£¼ì–´ì§„ ë¶„ì„ ë¦¬í¬íŠ¸ì˜ í˜•ì‹ì— ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì—…ë°ì´íŠ¸ ê´€ë ¨ ë‚´ìš©ë§Œ ì–¸ê¸‰ì„ ì¶”ê°€í•˜ëŠ” ì‹ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜. e.g.*   **ì—…ë°ì´íŠ¸ ì—°ê´€ì„±:**
    ìƒí’ˆëª…ì˜ ë‚ ì§œê°€ ë“¤ì–´ê°€ ìˆë‹¤ëŠ” ì‚¬ì‹¤ë§Œìœ¼ë¡œ ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì„±ì´ ìˆë‹¤ê³  ì¶”ë¡ í•˜ì§€ ë§ˆ.
    ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ë§ì€ í•˜ì§€ë§ˆ.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    return response4_WeeklySales_Report.text


def rgroup_top3_pu(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""
        with raw as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
        , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

        , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
        , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
        , DATE_SUB(
                    date_add(current_date('Asia/Seoul'),interval -1 day),
                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                    ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
        from
        (
        ### IAP
        (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , cast(price_sheet as int64) as price_sheet
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
        union all
        ### GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , cast(price_sheet as int64)*(1500/40) as price_sheet
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend')
        union all
        ### RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')

        )
        where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        ),

        sales_raw as ( ## 5331039
        select *  , format_date('%Y-%m',  logdatekst ) as month
        from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
        where joyplegameid = {joyplegameid}
        and logdatekst>='2025-01-01'
        ),


        monthly_rev as (
        select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
        from
        (select *
        , format_date('%Y-%m-01',  logdatekst ) as logmonth
        , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
        from sales_raw
        where logdatekst>='2025-01-01')
        group by 1,2,3
        ),

        r_group as (
        select *
        , case
        when rev>=10000000 then 'R0'
        when rev>=1000000  then 'R1'
        when rev>=100000   then 'R2'
        when rev>=10000    then 'R3'
        when rev>=1        then 'R4'
        # when rev=0         then 'nonPU'
        else 'ETC' end as rgroup
        from monthly_rev
        where rev>0
        ),

        raw2 as (
        select a.*,month_key
        , case
        when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
        when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
        else b.rgroup end as rgroup_final

        from
        (select * , format_date('%Y-%m',  week_start ) as month2
        from raw) as a

        left join
        (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
        from r_group
        ) as b

        on a.authaccountname = b.authaccountname
        and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
        ),

        raw3 as (

        select *
        , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
        , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
        from
        (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
        , count(distinct authaccountname) as PU
        , cast(sum(sales_buygem) as int64) as sales
        from raw2
        where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
        and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
        group by 1,2,3,4,5,6)
        )

        select *
        from raw3
        where rgroup_final is not null
        and pu_rank in (1,2,3)
        order by rgroup_final, pu_rank
    """

    query_result = query_run_method('4_detail_sales', query)

    context['task_instance'].xcom_push(key='rgroup_top3_pu', value=query_result)

    return True


def rgroup_top3_rev(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""

    with raw as (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    , DATE_SUB(
                date_add(current_date('Asia/Seoul'),interval -1 day),
                INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
    from
    (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , cast(price_sheet as int64) as price_sheet
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
    union all
    ### GEM
    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , cast(price_sheet as int64)*(1500/40) as price_sheet
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend')
    union all
    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend')

    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    sales_raw as ( ## 5331039
    select *  , format_date('%Y-%m',  logdatekst ) as month
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2025-01-01'
    ),


    monthly_rev as (
    select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
    from
    (select *
    , format_date('%Y-%m-01',  logdatekst ) as logmonth
    , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    from sales_raw
    where logdatekst>='2025-01-01')
    group by 1,2,3
    ),

    r_group as (
    select *
    , case
    when rev>=10000000 then 'R0'
    when rev>=1000000  then 'R1'
    when rev>=100000   then 'R2'
    when rev>=10000    then 'R3'
    when rev>=1        then 'R4'
    # when rev=0         then 'nonPU'
    else 'ETC' end as rgroup
    from monthly_rev
    where rev>0
    ),

    raw2 as (
    select a.*,month_key
    , case
    when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
    when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
    else b.rgroup end as rgroup_final

    from
    (select * , format_date('%Y-%m',  week_start ) as month2
    from raw) as a

    left join
    (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
    from r_group
    ) as b

    on a.authaccountname = b.authaccountname
    and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
    ),

    raw3 as (

    select *
    , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
    , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
    from
    (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
    , count(distinct authaccountname) as PU
    , cast(sum(sales_buygem) as int64) as sales
    from raw2
    where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
    and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
    group by 1,2,3,4,5,6)
    )

    select *
    from raw3
    where rgroup_final is not null
    and sales_rank in (1,2,3)
    order by rgroup_final, sales_rank
    """

    query_result = query_run_method('4_detail_sales', query)

    context['task_instance'].xcom_push(key='rgroup_top3_rev', value=query_result)

    return True


def rgroup_top3_gemini(service_sub: str, **context):
    query_result4_thisWeekSalesTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_pu',
        key='rgroup_top3_pu'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}


    response4_thisWeekRgroup = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    ì´ë²ˆì£¼ Rê·¸ë£¹ë³„ PU top3 , ë§¤ì¶œ top3 ìƒí’ˆë“¤ ì •ë³´ë¥¼ ì¤„ê²Œ
    ìƒìœ„ ê³¼ê¸ˆê·¸ë£¹ê³¼ í•˜ìœ„ê³¼ê¸ˆê·¸ë£¹ ê°„ì˜ ì°¨ì´ì—ëŒ€í•´ì„œë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜

    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ


    < Rê·¸ë£¹ë³„ PU top3 ìƒí’ˆ>
    {query_result4_thisWeekPUTop3}


    < Rê·¸ë£¹ë³„ ë§¤ì¶œ top3 ìƒí’ˆ>
    {query_result4_thisWeekSalesTop3}



    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_thisWeekRgroup.text



def category_for_bigquery_sql(service_sub:str, **context):

    _, _, _, response4_CategoryListUp = ruby_df_gemini(service_sub, **context)

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # ë¬¸ìì—´ì„ ì‹¤ì œ tuple/listë¡œ ë³€í™˜ (ì•ˆì „)
        CategoryListUp_2 = eval(CategoryListUp.group(0))

        # ì• 3ê°œë§Œ ì¶”ì¶œ
        CategoryListUp_Top3 = list(CategoryListUp_2)[:3]

        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜: 'ë°°í‹€íŒ¨ìŠ¤', 'êµ°í•¨', 'ì „íˆ¬ê¸°'
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_Top3])

        # SQL ORDER BYìš© CASE WHEN ... THEN ...
        case_when_str = "\n".join(
            [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_Top3)]
        )
    return CategoryListUp_SQL, case_when_str, CategoryListUp_Top3


def top3_items_rev(joyplegameid:int, gameidx:str, databaseschema:str, service_sub:str, **context):
    
    CategoryListUp_SQL, case_when_str, _ = category_for_bigquery_sql(service_sub=service_sub)

    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 5 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 5 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , case when CountryCode = 'KR' then '1.KR' when CountryCode = 'US' then '2.US' else '3.ETC' end as CountryCat
            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„') then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
                (
                    select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst, CountryCode
                        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
                        , cast(price_sheet as int64) as price_sheet
                        , pricekrw as sales_usegem, pricekrw as sales_buygem
                from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
                where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind
                        , cast(price_sheet as int64)*(1500/40) as price_sheet
                        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='gem' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
                        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='ruby' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )
            )
            )
    where cat_package2 in  ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    group by `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc
    """
    query_result = query_run_method('4_detail_sales', query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")
    
    context['task_instance'].xcom_push(key='top3_items_rev', value=query_result)


    cats = [re.sub(r"^[\"'â€™â€˜`]+|[\"'â€™â€˜`]+$", "", t.strip())
        for t in CategoryListUp_SQL.split(",") if t.strip()]

    # 2) ìˆœì„œëŒ€ë¡œ í•„í„°ë§í•´ì„œ ìƒˆ DF ìƒì„±
    category_col = "ìƒí’ˆ ì¹´í…Œê³ ë¦¬"

    dfs = {}  # ì‚¬ì „ìœ¼ë¡œ ë³´ê´€: {"query_result4_salesByPackage_forGraph_1": df1, ...}
    for i, c in enumerate(cats, start=1):
        key = f"query_result4_salesByPackage_forCategoryGraph_{i}"
        dfs[key] = query_result[
            query_result[category_col] == c
        ].copy()

    return dfs


def rgroup_rev_draw(gameidx: str, **context):
    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )
    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales.iloc[:, [0,2,3,4,5,6,7,8]]

    ##
    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales2_salesGraph.rename(
        columns = {"R0_Sales" : "R0",
                "R1_Sales" : "R1",
                "R2_Sales" : "R2",
                "R3_Sales" : "R3",
                "R4_Sales" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_Sales" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_Sales" : "ë‹¹ì›”ê°€ì…ì"}
    )


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_salesGraph["logdatekst"]
    y = query_result4_RgroupSales2_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" Rê·¸ë£¹ë³„ ë§¤ì¶œ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_salesGraph = "graph4_RgroupSales_salesGraph.png"
    
    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    plt.savefig(file_path4_RgroupSales_salesGraph, dpi=160, bbox_inches='tight') # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_salesGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_salesGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_salesGraph)

    return f'{gameidx}/{file_path4_RgroupSales_salesGraph}'


def rgroup_pu_draw(gameidx: str, **context):
    
    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales.iloc[:, [0,10,11,12,13,14,15,16]]

    ##
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales2_puGraph.rename(
        columns = {"R0_PU" : "R0",
                "R1_PU" : "R1",
                "R2_PU" : "R2",
                "R3_PU" : "R3",
                "R4_PU" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_PU" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_PU" : "ë‹¹ì›”ê°€ì…ì"}
    )

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_puGraph["logdatekst"]
    y = query_result4_RgroupSales2_puGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_puGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" (Rê·¸ë£¹ë³„ PU ìˆ˜ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_puGraph = "graph4_RgroupSales_puGraph.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_RgroupSales_puGraph, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_puGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_puGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_puGraph)

    return f'{gameidx}/{file_path4_RgroupSales_puGraph}'


def merge_rgroup_graph(gameidx: str):
    p1 = rgroup_rev_draw(gameidx)
    p2 = rgroup_pu_draw(gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA")


    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph4_RgroupSales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path



def iap_gem_ruby_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
        )

    query_result4_salesByPackage_salesGraph = query_result4_salesByPackage.iloc[:, [0,2,3,4,5,6,7,8,9,10,11]]

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP+ìœ ê°€ì ¬+ìœ ê°€ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage = "graph4_salesByPackage.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage}')
    blob.upload_from_filename(file_path4_salesByPackage)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage)

    return f'{gameidx}/{file_path4_salesByPackage}'



def iap_gem_ruby_IAP_graph_draw(gameidx:str, **context):
    
    query_result4_salesByPackage_IAP = context['task_instance'].xcom_pull(
    task_ids = 'iap_df',
    key='iap_df'
    )

    # ... (ìœ„ ë°ì´í„° ì¤€ë¹„Â·í°íŠ¸ ë¶€ë¶„ ë™ì¼)
    query_result4_salesByPackage_IAP_salesGraph = query_result4_salesByPackage_IAP.iloc[:, (query_result4_salesByPackage_IAP.columns != 'month') & (query_result4_salesByPackage_IAP.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_IAP_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_IAP_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_IAP_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_IAP = "graph4_salesByPackage_IAP.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_IAP, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_IAP}')
    blob.upload_from_filename(file_path4_salesByPackage_IAP)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_IAP)

    return f'{gameidx}/{file_path4_salesByPackage_IAP}'


def iap_gem_ruby_GEM_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage_GEM = context['task_instance'].xcom_pull(
    task_ids = 'gem_df',
    key='gem_df'
    )
    
    # ... (ìœ„ ë°ì´í„° ì¤€ë¹„Â·í°íŠ¸ ë¶€ë¶„ ë™ì¼)
    query_result4_salesByPackage_GEM_salesGraph = query_result4_salesByPackage_GEM.iloc[:, (query_result4_salesByPackage_GEM.columns != 'month') & (query_result4_salesByPackage_GEM.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_GEM_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_GEM_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_GEM_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ì ¬) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_GEM = "graph4_salesByPackage_GEM.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_GEM, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_GEM}')
    blob.upload_from_filename(file_path4_salesByPackage_GEM)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_GEM)

    return f'{gameidx}/{file_path4_salesByPackage_GEM}'
    


def iap_gem_ruby_RUBY_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage_RUBY = context['task_instance'].xcom_pull(
    task_ids = 'ruby_df',
    key='ruby_df'
    )

    query_result4_salesByPackage_RUBY_salesGraph = query_result4_salesByPackage_RUBY.iloc[:, (query_result4_salesByPackage_RUBY.columns != 'month') & (query_result4_salesByPackage_RUBY.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_RUBY_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_RUBY_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_RUBY_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_RUBY = "graph4_salesByPackage_RUBY.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_RUBY, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_RUBY}')
    blob.upload_from_filename(file_path4_salesByPackage_RUBY)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_RUBY)

    return f'{gameidx}/{file_path4_salesByPackage_RUBY}'


### 1ìœ„
def top1_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category1 = "graph4_salesByPackage_Category1.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category1, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category1}')
    blob.upload_from_filename(salesByPackage_Category1)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category1)

    return f'{gameidx}/{salesByPackage_Category1}'


### 2ìœ„
def top2_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category2 = "graph4_salesByPackage_Category2.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category2, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category2}')
    blob.upload_from_filename(salesByPackage_Category2)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category2)

    return f'{gameidx}/{salesByPackage_Category2}'



### 3ìœ„
def top3_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category3 = "graph4_salesByPackage_Category3.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category3, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category3}')
    blob.upload_from_filename(salesByPackage_Category3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category3)

    return f'{gameidx}/{salesByPackage_Category3}'


def rgroup_pu_top3_graph_draw(gameidx:str, **context):

    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_pu',
        key='rgroup_top3_pu'
    )

    df = query_result4_thisWeekPUTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekPUTop3 = "graph4_thisWeekPUTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekPUTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekPUTop3}')
    blob.upload_from_filename(file_path4_thisWeekPUTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekPUTop3)

    return f'{gameidx}/{file_path4_thisWeekPUTop3}'


def rgroup_rev_top3_graph_draw(gameidx:str, **context):

    query_result4_thisWeekRevTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    df = query_result4_thisWeekRevTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekSalesTop3 = "graph4_thisWeekRevTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekSalesTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekSalesTop3}')
    blob.upload_from_filename(file_path4_thisWeekSalesTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekSalesTop3)

    return f'{gameidx}/{file_path4_thisWeekSalesTop3}'


def rgroup_rev_upload_notion(joyplegameid: int, gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "4. ì´ë²ˆì£¼ ìƒì„¸ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(1) Rê·¸ë£¹ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": " ** ì „ì›” ê³¼ê¸ˆì•¡ ê¸°ì¤€ Rê·¸ë£¹ ì…ë‹ˆë‹¤. \n ** ì£¼ì°¨ë³„ ê¸°ì¤€ì€ ìˆ˜ìš”ì¼~í™”ìš”ì¼ ì…ë‹ˆë‹¤. " }}]
                },
            }
        ],
    )

    try:
        gcs_path = f'{gameidx}/graph4_RgroupSales.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_RgroupSales.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_RgroupSales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„

    blocks = md_to_notion_blocks(rev_group_rev_pu_gemini(joyplegameid, service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(2) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ë°ì´í„° ê¸°ì¤€ : IAP êµ¬ë§¤ - IAP ì ¬êµ¬ë§¤ - IAP ë£¨ë¹„êµ¬ë§¤ + ìœ ê°€ì ¬ ì‚¬ìš©ë‚´ì—­ + ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ì ¬ê³¼ ë£¨ë¹„ë¡œ ì–´ë–¤ ìƒí’ˆì„ êµ¬ë§¤í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´, IAPë¡œ ì ¬ê³¼ ë£¨ë¹„ë¥¼ êµ¬ë§¤í•œ ê²ƒì€ ì œê±°í•œ í›„ ìœ ê°€ì ¬/ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ì„ ë§¤ì¶œë¡œ ì§‘ê³„í•˜ì˜€ìŠµë‹ˆë‹¤." }}]
                },
            }
        ],
    )



def iap_gem_ruby_upload_notion(joyplegameid: int, gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )


    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
        )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_salesByPackage,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
        )
    
    blocks = md_to_notion_blocks(iap_gem_ruby_gemini(service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    # í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ì¤‘ê°„ì— ê·¸ë˜í”„ ì‚½ì…ì„ ìœ„í•œ ê²°ê³¼ í…ìŠ¤íŠ¸ 5ë¶„í• 

    text = top3_items_by_category_gemini(service_sub)

    # ì¤„ ë‹¨ìœ„ ë¶„ë¦¬
    lines = [line.strip() for line in text.split("\n") if line.strip()]

    # ë¸”ë¡ ë‹¨ìœ„ ë¶„ë¦¬
    blocks_raw = []
    current_block = []

    for line in lines:
        if line.startswith("**") and line.endswith("**"):
            # ìƒˆë¡œìš´ ë¸”ë¡ ì‹œì‘ â†’ ê¸°ì¡´ ë¸”ë¡ ì €ì¥
            if current_block:
                blocks_raw.append(current_block)
            current_block = [line]
        else:
            current_block.append(line)

    # ë§ˆì§€ë§‰ ë¸”ë¡ ì €ì¥
    if current_block:
        blocks_raw.append(current_block)

    # ì´ì œ blocks_raw = [[í—¤ë”, ë‚´ìš©...], [í—¤ë”, ë‚´ìš©...], ...]

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    blocks_bucket = {"blocks_1": [], "blocks_2": [], "blocks_3": [], "blocks_4": [], "blocks_5": []}

    found_first = False
    for block in blocks_raw:
        header = block[0]

        # ë§¤ì¶œ 1ìœ„ ì „ê¹Œì§€ëŠ” blocks_1
        if not found_first:
            if "(ë§¤ì¶œ 1ìœ„)" in header:
                found_first = True
                blocks_bucket["blocks_2"] = block
            else:
                blocks_bucket["blocks_1"].extend(block)
            continue

        # ì´í›„ ë§¤ì¶œ 2ìœ„, 3ìœ„, ë‚˜ë¨¸ì§€ êµ¬ë¶„
        if "(ë§¤ì¶œ 2ìœ„)" in header:
            blocks_bucket["blocks_3"] = block
        elif "(ë§¤ì¶œ 3ìœ„)" in header:
            blocks_bucket["blocks_4"] = block
        else:
            blocks_bucket["blocks_5"].extend(block)

    for k, v in blocks_bucket.items():
        if isinstance(v, list):
            blocks_bucket[k] = "\n".join(v)  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜

    blocks = md_to_notion_blocks(blocks_bucket["blocks_1"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 1ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category1.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category1.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_2"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 2ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category2.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category2.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_3"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 3ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_4"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    blocks = md_to_notion_blocks(blocks_bucket["blocks_5"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True


def iap_toggle_add(gameidx: str, service_sub:str, **context):
    
    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

        # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_IAP.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_IAP.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_IAP = context['task_instance'].xcom_pull(
        task_ids='iap_df',
        key='iap_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_IAP,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(iap_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True

def gem_toggle_add(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_GEM.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_GEM.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_GEM = context['task_instance'].xcom_pull(
        task_ids='gem_df',
        key='gem_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_GEM,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(gem_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def ruby_toggle_add(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_RUBY.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_RUBY.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_RUBY = context['task_instance'].xcom_pull(
        task_ids='ruby_df',
        key='ruby_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_RUBY,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(ruby_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def rgroup_top3_upload_notion(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(3) ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œ/PU ìƒìœ„ 3ê°œ ìƒí’ˆ \n" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_thisWeekPUTop3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekPUTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    ### íŒŒì¼ ì—…ë¡œë“œ ê°ì²´ 
    try:
        gcs_path = f'{gameidx}/graph4_thisWeekRevTop3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekRevTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    query_result4_thisWeekSalesTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekPUTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(PU)",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekSalesTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(ë§¤ì¶œ) ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    
    blocks = md_to_notion_blocks(rgroup_top3_gemini(send_resp, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    