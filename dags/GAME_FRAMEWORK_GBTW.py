import time
import pandas as pd
from google.cloud import bigquery
from google import genai
from google.genai import types
from goole.cloud import storage
from vertexai import rag
import vertexai
from google.genai import Client
from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore

# ì¸ì¦ê´€ë ¨
import google.auth
from google.auth.transport.requests import Request
import logging

# ê·¸ë˜í”„ ê´€ë ¨ íŒ¨í‚¤ì§€
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter, StrMethodFormatter, PercentFormatter, MultipleLocator
import matplotlib as mpl
import matplotlib.font_manager as fm
from matplotlib import cm
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont # 2ê°€ì§€ íŒŒì¼ í•©ì¹˜ê¸°
import matplotlib.dates as mdates
import nest_asyncio
from jinja2 import Template
from playwright.async_api import async_playwright
import asyncio
import IPython.display as IPd
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List, Tuple
from matplotlib import rcParams
from matplotlib.patches import Rectangle
from game_framework_util import *

# ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€
import numpy as np
import re
import os 
import math
import time
import pandas as pd
from notion_client import Client
import requests
import json
from datetime import datetime, timezone, timedelta
from adjustText import adjust_text
from airflow.models import Variable
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
from pathlib import Path

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)


# ë³€ìˆ˜ ìƒì„±
t0 = time.time()
PROJECT_ID = "data-science-division-216308"
LOCATION = "us-central1"
MODEL_NAME = "gemini-2.5-flash"

NOTION_TOKEN=get_var("MS_TEAM_NOTION_TOKEN") # MSíŒ€ API í‚¤
NOTION_VERSION=get_var("NOTION_API_VERSION")
DATABASE_ID=get_var("GAMEFRAMEWORK_GBTW_NOTION_DB_ID")
CREDENTIALS_JSON = get_var('GOOGLE_CREDENTIAL_JSON')

cred_dict = json.loads(CREDENTIALS_JSON)
credentials, project_id = google.auth.default(
    scopes=["https://www.googleapis.com/auth/cloud-platform"]
)
credentials.refresh(Request())


# í´ë¼ì´ì–¸íŠ¸ ëª¨ìŒ
genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)
bigquery_client = bigquery.Client(project=PROJECT_ID, credentials=credentials)# location=LOCATION ## us-central1 ë¡œ í•  ê²½ìš° í—ˆë¸Œ ì¡°íšŒë¶ˆê°€ëŠ¥
notion = Client(auth=NOTION_TOKEN)
gcs_client = storage.Client.from_service_account_info(cred_dict)
bucket = gcs_client.bucket('game-framework1')


#### ì œë¯¸ë‚˜ì´ ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ 
SYSTEM_INSTRUCTION = [
                "You're a Game Data Analyst.",
                "Your task is to analyze the metrics of a given mobile game and identify the causes of any changes.",
                "Your answers must be in Korean.",
                "The unit of amount in the Sales or Revenue, Cost Data is Korean Won.",
                "You must answer in Notion's Markdown format, but do not use title syntax.",
            ]


## í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ //////////// task í•¨ìˆ˜
def make_gameframework_notion_page(gameidx: str, **context):

    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    # íƒ€ì„ì¡´ ì§€ì •
    kst = ZoneInfo("Asia/Seoul")
    # ì˜¤ëŠ˜
    today_kst = datetime.now(kst).date()
    # ì–´ì œ
    yesterday_kst = today_kst - timedelta(days=1)

    # ì´ë²ˆ ë‹¬ 1ì¼ (ì–´ì œ ë‚ ì§œ ê¸°ì¤€)
    first_day = yesterday_kst.replace(day=1)

    # íƒ€ì´í‹€ ë¬¸ìì—´ ë§Œë“¤ê¸°
    title = f"{yesterday_kst.strftime('%y')}ë…„ {yesterday_kst.month}ì›” ë§¤ì¶œí˜„í™©( ~ {yesterday_kst})"
    print(f"{title} : {gameidx}")

    # í˜ì´ì§€ ìƒì„± ìš”ì²­ ë°”ë””
    data = {
        "parent": {"database_id": DATABASE_ID},
        "properties": {
            "ì´ë¦„": {
                "title": [
                    {"text": {"content": title }}
                ]
            },
            "ë“±ë¡ ë‚ ì§œ": {
                "date": {"start": today_kst.isoformat() }
            },
            "í”„ë¡œì íŠ¸": {
                "multi_select": [
                    {"name": {gameidx}}   # ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜
                ]
            },
            "ë¦¬í¬íŠ¸ ì¢…ë¥˜": {
                "multi_select": [
                    {"name": "ê²Œì„ë¶„ì„"}   # ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜
                ]
            },
            "ì‘ì„±ì": {
                "people": [
                    {"id": "ce95f16a-6b6b-447d-a996-a9c5f0cc0113"},  # Notion user_id
                    {"id": "662575bc-731c-481c-afc7-13b2fdf5482a"}  # Notion user_id
                ]
            }
        }
    }

    res = requests.post(url, headers=headers, json=data)

    if res.status_code == 200:
        page_info = res.json() # âœ… í˜ì´ì§€ ID page_info["id"]
        print(f"âœ… í˜ì´ì§€ ìƒì„± ì„±ê³µ âœ… í˜ì´ì§€ ID : {page_info["id"]}")
    else:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {res.status_code} >> {res.text}")

    notion.blocks.children.append(
        block_id=page_info["id"] ,
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {
                            "type": "text",
                            "text": {"content": " â—¾ ëª©ì°¨"},
                            "annotations": {"bold": True}
                        }
                    ]
                }
            }
        ]
    )

    # ëª©ì°¨ ë¸”ë¡ ì¶”ê°€
    notion.blocks.children.append(
        block_id=page_info["id"] ,
        children=[
            {
                "object": "block",
                "type": "table_of_contents",
                "table_of_contents": {
                    "color": "default"  # "gray", "brown", "orange", "yellow", "green", "blue", "purple", "pink", "red" ê°€ëŠ¥
                }
            }
        ],
    )
    
    if res.status_code == 200:
        page_info = res.json()
        print(f"âœ… í˜ì´ì§€ ìƒì„± ì„±ê³µ âœ… í˜ì´ì§€ ID : {page_info['id']}")
    else:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {res.status_code} >> {res.text}")

    context['task_instance'].xcom_push(key='page_info', value=page_info)

    return page_info 



################# notion í˜ì´ì§€ ìƒì„± í•¨ìˆ˜ ì‹¤í–‰ ############################

# ğŸ‘‰ Markdown ë‚´ **êµµê²Œ** ì²˜ë¦¬ ë³€í™˜
def parse_rich_text(md_text):
    """
    '**êµµê²Œ**' â†’ Notion rich_text [{"text": {...}, "annotations": {"bold": True}}]
    """
    parts = re.split(r"(\*\*.*?\*\*)", md_text)  # **...** ê¸°ì¤€ split
    rich_text = []
    for part in parts:
        if part.startswith("**") and part.endswith("**"):
            rich_text.append({
                "type": "text",
                "text": {"content": part[2:-2]},
                "annotations": {"bold": True}
            })
        else:
            if part:
                rich_text.append({
                    "type": "text",
                    "text": {"content": part}
                })
    return rich_text



# ğŸ‘‰ Markdownì„ Notion Blocksë¡œ ë³€í™˜
def md_to_notion_blocks(md_text, blank_blocks=3):
    blocks = []
    lines = md_text.splitlines()
    stack = [blocks]  # í˜„ì¬ ê³„ì¸µ ì¶”ì 

    def detect_indent_unit(lines):
        indents = []
        for line in lines:
            if line.lstrip().startswith(("* ", "- ", "+ ")):  # ë¦¬ìŠ¤íŠ¸ ë¬¸ë²• ê°ì§€
                indent = len(line) - len(line.lstrip())
                if indent > 0:
                    indents.append(indent)
        return min(indents) if indents else 4  # fallback = 4ì¹¸
    indent_unit = detect_indent_unit(lines)

    i = 0
    while i < len(lines):
        line = lines[i].rstrip()
        if not line:
            i += 1
            continue

        # Heading ì²˜ë¦¬
        if line.startswith("# "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": parse_rich_text(line[2:])}
            })
        elif line.startswith("## "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": parse_rich_text(line[3:])}
            })
        elif line.startswith("### "):
            stack = [blocks]
            stack[-1].append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": parse_rich_text(line[4:])}
            })

        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        elif line.lstrip().startswith("* "):
            indent = len(line) - len(line.lstrip())  # ë“¤ì—¬ì“°ê¸° ë ˆë²¨
            content = line.strip()[2:].strip()

            block = {
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": parse_rich_text(content),
                    "children": []
                }
            }

            # indent ê¸°ë°˜ ê³„ì¸µ ì²˜ë¦¬
            level = indent // indent_unit + 1
            while len(stack) > level:
                stack.pop()
            stack[-1].append(block)
            stack.append(block["bulleted_list_item"]["children"])
        else:
            stack = [blocks]
            # ì¼ë°˜ ë¬¸ë‹¨
            stack[-1].append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": parse_rich_text(line.strip())}
            })

        i += 1

    # âœ… ë§ˆì§€ë§‰ì— ë¹ˆ ë¸”ë¡ ì¶”ê°€ (ê°œìˆ˜ëŠ” íŒŒë¼ë¯¸í„° blank_blocksë¡œ ì œì–´)
    for _ in range(blank_blocks):
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": []}
        })

    return blocks


def df_to_notion_table_under_toggle(
    notion: Client,
    page_id: str,
    df: pd.DataFrame,
    toggle_title: str = "ğŸ“Š Data Table",
    max_first_batch_rows: int = 90,
    batch_size: int = 100,
    has_column_header: bool = True,
    has_row_header: bool = False,
    ):
    """
    Notion í˜ì´ì§€ì— í† ê¸€ì„ ë§Œë“¤ê³ , ê·¸ ì•„ë˜ì— Pandas DataFrameì„ í‘œ(Table)ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    - ìµœì´ˆ ìƒì„± ì‹œ í…Œì´ë¸”ì˜ header + ì´ˆê¸° í–‰ë“¤ì„ table.children ì•ˆì— í¬í•¨
    - ì´í›„ ë‚¨ì€ í–‰ë“¤ì€ table_rowë¡œ ë°°ì¹˜ append

    Parameters
    ----------
    notion : notion_client.Client
        Notion SDK í´ë¼ì´ì–¸íŠ¸ (Client(auth=...) ë¡œ ìƒì„±)
    page_id : str
        í…Œì´ë¸”ì„ ì¶”ê°€í•  í˜ì´ì§€(í˜¹ì€ ë¸”ë¡) ID
    df : pandas.DataFrame
        ì—…ë¡œë“œí•  ë°ì´í„°í”„ë ˆì„
    toggle_title : str
        í† ê¸€ íƒ€ì´í‹€
    max_first_batch_rows : int
        í…Œì´ë¸” ìµœì´ˆ ìƒì„± ì‹œ í¬í•¨í•  ì´ˆê¸° í–‰ ê°œìˆ˜(Too many children ë°©ì§€ìš©)
    batch_size : int
        ì´í›„ ë°°ì¹˜ append ì‹œ ë¬¶ìŒ í¬ê¸°
    has_column_header : bool
        Notion í…Œì´ë¸” ì˜µì…˜ - ì»¬ëŸ¼ í—¤ë” ì‚¬ìš© ì—¬ë¶€
    has_row_header : bool
        Notion í…Œì´ë¸” ì˜µì…˜ - í–‰ í—¤ë” ì‚¬ìš© ì—¬ë¶€

    Returns
    -------
    dict : {"toggle_id": str, "table_id": str, "rows_created": int}
    """
    # 1) í† ê¸€ ìƒì„±
    toggle_resp = notion.blocks.children.append(
        page_id,
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {
                            "type": "text",
                            "text": {"content": toggle_title[:2000]},
                            "annotations": {
                                "bold": True,
                                "italic": False,
                                "underline": False,
                                "strikethrough": False,
                                "code": False,
                                "color": "blue"   # â† ìƒ‰ìƒ ì§€ì •
                            },
                        }
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    # 2) í—¤ë”/ì´ˆê¸°í–‰ ì¤€ë¹„
    table_width = len(df.columns)

    # í—¤ë”(ì»¬ëŸ¼ëª…)
    header_cells = []
    for col in df.columns.astype(str).tolist():
        header_cells.append([{"type": "text", "text": {"content": str(col)[:2000]}}])

    # ì´ˆê¸° ë°ì´í„° í–‰
    first_rows_blocks = []
    row_count = 0
    for _, row in df.iterrows():
        if row_count >= max_first_batch_rows:
            break
        row_cells = []
        for col in df.columns:
            val = row[col]
            #s = "" if (val is None or (isinstance(val, float) and math.isnan(val))) else str(val)
            s = "" if pd.isna(val) else str(val)
            row_cells.append([{"type": "text", "text": {"content": s[:2000]}}])

        # ì—´ ìˆ˜ ì•ˆì „ì¥ì¹˜(íŒ¨ë”©/ì ˆë‹¨)
        if len(row_cells) < table_width:
            row_cells += [[{"type": "text", "text": {"content": ""}}]] * (table_width - len(row_cells))
        elif len(row_cells) > table_width:
            row_cells = row_cells[:table_width]

        first_rows_blocks.append(
            {"object": "block", "type": "table_row", "table_row": {"cells": row_cells}}
        )
        row_count += 1

    # 3) í…Œì´ë¸” ë¸”ë¡ ìƒì„±: table.children ì•ˆì— header + ì´ˆê¸°í–‰ í¬í•¨(ì¤‘ìš”)
    table_block = {
        "object": "block",
        "type": "table",
        "table": {
            "table_width": table_width,
            "has_column_header": has_column_header,
            "has_row_header": has_row_header,
            "children": (
                [
                    {
                        "object": "block",
                        "type": "table_row",
                        "table_row": {"cells": header_cells},
                    }
                ]
                + first_rows_blocks
            ),
        },
    }

    table_create_resp = notion.blocks.children.append(toggle_id, children=[table_block])
    table_id = table_create_resp["results"][0]["id"]

    # 4) ë‚¨ì€ í–‰ë“¤ ë°°ì¹˜ ì¶”ê°€
    total = len(df)
    start = row_count
    while start < total:
        end = min(start + batch_size, total)
        batch_children = []

        for _, row in df.iloc[start:end].iterrows():
            row_cells = []
            for col in df.columns:
                val = row[col]
                s = "" if (val is None or (isinstance(val, float) and math.isnan(val))) else str(val)
                row_cells.append([{"type": "text", "text": {"content": s[:2000]}}])

            if len(row_cells) < table_width:
                row_cells += [[{"type": "text", "text": {"content": ""}}]] * (table_width - len(row_cells))
            elif len(row_cells) > table_width:
                row_cells = row_cells[:table_width]

            batch_children.append(
                {"object": "block", "type": "table_row", "table_row": {"cells": row_cells}}
            )

        notion.blocks.children.append(table_id, children=batch_children)
        start = end

    return {"toggle_id": toggle_id, "table_id": table_id, "rows_created": total}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš© ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# from notion_client import Client
# notion = Client(auth=NOTION_TOKEN)
# resp = df_to_notion_table_under_toggle(
#     notion=notion,
#     page_id=PAGE_ID,
#     df=query_result1_dailySales,
#     toggle_title="ğŸ“Š Daily Sales (DataFrame Table)",
#     max_first_batch_rows=90,
#     batch_size=100,
# )
# print(resp)

### ì¼ìë³„ ë§¤ì¶œ
# ì¿¼ë¦¬ & ì œë¯¸ë‚˜ì´ í”„ë¡¬í”„íŠ¸

def query_run_method(service_sub: str, query):
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}} ## ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¶™ì¼ ìˆ˜ ìˆìŒ.
    print("ğŸ“§ RUN_ID=", RUN_ID, "ğŸ“§ LABEL_ID=", LABELS)

    query_result = bigquery_client.query(query, job_config=bigquery.QueryJobConfig(labels=LABELS)).to_dataframe()
    return query_result

## ì¼ìë³„ ë§¤ì¶œ
def Daily_revenue_query(joyplegameid: int, **context):
    query = f"""

    select day
    , cast(sum(if(monthtype = 'ì§€ë‚œë‹¬' , pricekrw, null ))as int64) as `ì§€ë‚œë‹¬`
    , cast(sum(if(monthtype = 'ì´ë²ˆë‹¬' , pricekrw, null ))as int64) as `ì´ë²ˆë‹¬`

    from
    (select *
    , format_date('%Y-%m',  logdatekst ) as Month
    , case when logdatekst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdatekst< DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH) then 'ì§€ë‚œë‹¬'
    when logdatekst >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH) then 'ì´ë²ˆë‹¬'
    else 'etc' end as monthtype
    , format_date('%d',  logdatekst ) as day
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdateKst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    #and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )
    group by 1
    order by 1

    """
    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='daily_revenue_df', value=query_result)

    return True
    
    
#### ì „ë…„ ëŒ€ë¹„ ì›” ë§¤ì¶œ ì¶”ì´
def Daily_revenue_YOY_query(joyplegameid: int, **context):
    query = f"""

    select month
    , cast(sum(if(yeartype = 'ì‘ë…„' , pricekrw, null ))as int64) as `ì‘ë…„`
    , cast(sum(if(yeartype = 'ì˜¬í•´' , pricekrw, null ))as int64) as `ì˜¬í•´`

    from
    (select *
    , case
    when logdatekst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR), INTERVAL 1 YEAR)
    and logdatekst< DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR) then 'ì‘ë…„'

    when logdatekst >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR)
    and logdatekst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR) then 'ì˜¬í•´'
    else 'etc' end as yeartype
    , format_date('%m',  logdatekst ) as month
    , format_date('%Y',  logdatekst ) as year

    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdateKst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR), INTERVAL 1 YEAR)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), YEAR)
    )
    group by 1
    order by 1

    """
    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='Daily_revenue_YOY_df', value=query_result)

    return True


## í˜„ì¬ ë§¤ì¶œê³¼ ëª©í‘œ ë§¤ì¶œ
def Daily_revenue_target_revenue_query(joyplegameid: int, gameidx: str, **context):
    query = f"""
    ### 1> ì´ë²ˆë‹¬ ì¼ìë³„ ë§¤ì¶œ ì‹¤ì¸¡ì¹˜
    with thismonthRev as (
    select day ## ì¼ì
    , lastDay ## ì´ë²ˆë‹¬ ë§ˆì§€ë§‰ë‚  (ex - 30)
    , sum(pricekrw) as rev ## ë§¤ì¶œì•¡
    from
    (select *
    , cast(format_date('%d',  logdatekst ) as int64) as day ## ì¼ì
    , EXTRACT(DAY FROM LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)) as lastDay
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    )
    group by day,lastDay
    order by day
    ),

    ### 2> ëª©í‘œë§¤ì¶œ í…Œì´ë¸”
    salesGoal as (
    select
        CAST(REPLACE(sales, ',', '') AS INT64) as salesGoalMonthly # ì‰¼í‘œ í¬í•¨í•œ string í˜•íƒœë¡œ ì ì¬ë˜ì–´ìˆì–´ì„œ int64 í˜•íƒœë¡œ ì „ì²˜ë¦¬
    , CAST(REPLACE(sales, ',', '') AS INT64)/cast(num_of_days as int64) as salesGoalDaily # ì¼í‰ê·  ëª©í‘œë§¤ì¶œ
    from `data-science-division-216308.gameInsightFramework.slgMonthlyGoal`
    where idx = {gameidx}
    and month = FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    ),

    ### 3> í•©ì¹˜ê¸°
    thismonthRev2 as (


    select
        a.day, a.lastDay, a.rev
    , b.salesGoalDaily
    , c.maxDay ## ì´ë²ˆë‹¬ ë©°ì¹ ê¹Œì§€ ê¸°ê°„ ì°¼ëŠ”ì§€
    #, case when maxDay >5 then rev ## 5ì¼ì¹˜ ì´ìƒì˜ ë§¤ì¶œì´ ìˆìœ¼ë©´ ê·¸ëƒ¥ ì¼í• ê³„ì‚°
    #       when maxDay<=5 and day=1 then salesGoalDaily ## 5ì¼ì¹˜ ì´í•˜ì˜ ë§¤ì¶œë§Œ ìˆë‹¤ë©´ , 1ì¼ì ë§¤ì¶œì„ ë³´ì •ì¹˜ ì ìš©
    #       else rev end as rev2
    , a.rev as rev2
    from
    ## ì¼ìë³„ ë§¤ì¶œ ì‹¤ì¸¡
    (select * from thismonthRev) as a

    ## ëª©í‘œë§¤ì¶œ (ì›”ë³„, ì¼ë³„)
    cross join
    (select *
    from salesGoal
    ) as b

    ## í˜„ì¬ ë©°ì¹ ê¹Œì§€ ë§¤ì¶œ ìˆëŠ”ì§€ -> 5ì¼ ì´ì „ì¸ì§€ ì´í›„ì¸ì§€ í™•ì¸ìš©ë„
    cross join
    (select cast(max(day) as int64) as maxDay from thismonthRev) as c

    )

    #select * from thismonthRev_and_revGoal order by day

    ### 4> ì „ì²˜ë¦¬
    select cast(current_sales as int64) as current_sales, b.salesGoalMonthly
    from
    (select (rev/maxDay)*lastDay as current_sales
    from
    (select sum(rev2) as rev, max(maxDay) as maxDay, max(lastDay) as lastDay
    from thismonthRev2)
    ) as a
    cross join
    (select * from salesGoal) as b

    """

    query_result = query_run_method('1_daily_sales', query)
    context['task_instance'].xcom_push(key='Daily_revenue_target_revenue_df', value=query_result)

    return True


## ì „ë…„ ëŒ€ë¹„ ì›” ë§¤ì¶œ ì¶”ì´ ìˆ˜ì • - ë‹¹ì›”ì€ ì¼í• ê³„ì‚° ë§¤ì¶œ
def merge_daily_revenue(joyplegameid: int, gameidx: str, **context):

    s_total = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_query',
        key='daily_revenue_df'
    )
    val_total = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_YOY_query',
        key='Daily_revenue_YOY_df'
    )

    val = val_total.iat[0, 0]
    s = s_total.iloc[:, 2]
    try:
        idx = s.dropna().index[-1]                 # ë§ˆì§€ë§‰ non-null ë¼ë²¨ ì¸ë±ìŠ¤
        s_total.loc[idx, s_total.columns[2]] = val
    except IndexError:
        pass  # ëª¨ë‘ nullì¸ ê²½ìš°

    return s_total


## í”„ë¡¬í”„íŠ¸ 
### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸
def daily_revenue_gemini(joyplegameid: int, service_sub: str, **context):

    query_result1_dailySales = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_query',
        key='daily_revenue_df'
    )

    query_result1_monthlySales = context['task_instance'].xcom_pull(
        task_ids = 'Daily_revenue_YOY_query',
        key='Daily_revenue_YOY_df'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response1_salesComment = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¹ì›” ë§¤ì¶œì€ ì¼í• ê³„ì‚°ì‹œ {f"{int((query_result1_monthlySales.iat[0,0])):,}"}ì´ê³  ëª©í‘œëŠ” {f"{int((query_result1_monthlySales.iat[0,1])):,}"}ì´ì•¼.
    ë‹¹ì›” ë§¤ì¶œì€ ì¼í• ê³„ì‚°ì‹œ ~~ì´ê³  ëª©í‘œë§¤ì¶œì€ ~~ ìœ¼ë¡œ, ëª©í‘œëŒ€ë¹„ ì–¼ë§ˆ ë‹¬ì„±í–ˆë‹¤ì˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.
    ê·¸ë¦¬ê³  ì¶”ê°€ë¡œ ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•´ ì•„ì£¼ ê°„ë‹¨íˆ ì½”ë©˜íŠ¸ë¥¼ í•´ì¤˜.
    ~ìŠµë‹ˆë‹¤ ì²´ë¡œ ì•Œë ¤ì¤˜

    ê·¸ë¦¬ê³  ì „ë…„ë™ì›”ëŒ€ë¹„ì–´ë–¤ì§€ 3ì¤„ì´ë‚´ë¡œ ê°„ë‹¨íˆ ì•Œë ¤ì¤˜.

    ì•ìœ¼ë¡œ ì–´ë–»ê²Œ í•´ì•¼ê² ë‹¤ëŠ” ì‚¬ê²¬ì€ ì“°ì§€ë§ˆ.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    <ì¼ìë³„ ì´ ë§¤ì¶œ>
    {query_result1_dailySales}

    < ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ>
    {query_result1_monthlySales}
    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=LABELS
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response1_salesComment.text

# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

# ## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

## ê·¸ë˜í”„ ê·¸ë¦¬ê¸° : arg ê°’ìœ¼ë¡œ ê²Œì„ ì½”ë“œ
def daily_revenue_graph_draw(joyplegameid: int, gameidx: str, daily_revenue_query_path:str, **context):

    df_daily = load_df_from_gcs(bucket, daily_revenue_query_path.split('/')[-1])
    
    x  = df_daily.iloc[:, 0]
    y1 = pd.to_numeric(df_daily.iloc[:, 1], errors='coerce')
    y2 = pd.to_numeric(df_daily.iloc[:, 2], errors='coerce')

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(x, y2, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            label=df_daily.columns[2])
    ax.plot(x, y1, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            linestyle='--', label=df_daily.columns[1])  # ê²¹ì³ì„œ í‘œì‹œ

    # ì˜µì…˜
    plt.title("ì¼ìë³„ ë§¤ì¶œ")
    #plt.xlabel(query_result1_dailySales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result1_dailySales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(df_daily[df_daily.columns[0]][::2], rotation=45)

    # ë²”ë¡€ í‘œì‹œ - ê·¸ë˜í”„ë‘ ì•ˆê²¹ì¹˜ê²Œ
    plt.legend(
        bbox_to_anchor=(1.05, 1),   # ê·¸ë˜í”„ ì˜¤ë¥¸ìª½ ë°”ê¹¥ (x=1.05, y=1)
        loc='upper left',           # ì•µì»¤ ê¸°ì¤€ ìœ„ì¹˜
        borderaxespad=0.             # ì¶•ê³¼ ê°„ê²©
    )

    # yì¶• 0ë¶€í„° ì‹œì‘ (ì•ˆí•˜ë©´ ëˆˆê¸ˆ ìµœì†Œê°’ ìì¢… ì¡°ì •)
    plt.ylim(0, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§

    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„
    #plt.grid(axis='x', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    #plt.xlabel("ë‚ ì§œ")
    #plt.ylabel("ë§¤ì¶œ")

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    ####################################### ì´ë¯¸ì§€ íŒŒì¼ì„ ì €ì¥í•  pathê°€ í•„ìš”í•¨ #####################
    filepath1_dailySales = "graph1_dailySales.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


## ì›”ê°„ ë§¤ì¶œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
def daily_revenue_YOY_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result1_monthlySales = context['task_instance'].xcom_pull(
        task_ids='Daily_revenue_YOY_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='Daily_revenue_YOY_df'
    )

    x  = query_result1_monthlySales.iloc[:, 0]
    y1 = pd.to_numeric(query_result1_monthlySales.iloc[:, 1], errors='coerce')
    y2 = pd.to_numeric(query_result1_monthlySales.iloc[:, 2], errors='coerce')

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(x, y2, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            label=query_result1_monthlySales.columns[2])
    ax.plot(x, y1, marker='o',
            markersize=3, linewidth=1, # ë§ˆì»¤ í¬ê¸° ì‘ê²Œ
            linestyle='--', label=query_result1_monthlySales.columns[1])  # ê²¹ì³ì„œ í‘œì‹œ

    # ì˜µì…˜
    plt.title("ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ")
    #plt.xlabel(query_result1_monthlySales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result1_monthlySales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(query_result1_monthlySales[query_result1_monthlySales.columns[0]][::1], rotation=45)

    # ë²”ë¡€ í‘œì‹œ - ê·¸ë˜í”„ë‘ ì•ˆê²¹ì¹˜ê²Œ
    plt.legend(
        bbox_to_anchor=(1.05, 1),   # ê·¸ë˜í”„ ì˜¤ë¥¸ìª½ ë°”ê¹¥ (x=1.05, y=1)
        loc='upper left',           # ì•µì»¤ ê¸°ì¤€ ìœ„ì¹˜
        borderaxespad=0.             # ì¶•ê³¼ ê°„ê²©
    )

    # y ì¶• ì¡°ì • (20ì–µë¶€í„°)
    plt.ylim(2000000000, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§

    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„
    #plt.grid(axis='x', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    #plt.xlabel("month")
    #plt.ylabel("ë§¤ì¶œ")

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    filePath1_monthlySales = "graph1_monthlySales.png"
    plt.savefig(filePath1_monthlySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filePath1_monthlySales}')
    blob.upload_from_filename(filePath1_monthlySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filePath1_monthlySales)

    return f'{gameidx}/{filePath1_monthlySales}'



# 1) íŒŒì¼ ê²½ë¡œ
def merge_daily_graph(joyplegameid: int, gameidx: str):
    p1 = daily_revenue_graph_draw(joyplegameid, gameidx)
    p2 = daily_revenue_YOY_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph1_dailySales_monthlySales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


def daily_revenue_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )
    query_result1_dailySales=context['task_instance'].xcom_pull(
        task_ids='daily_revenue_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='daily_revenue_df'
    )

    query_result1_monthlySales=context['task_instance'].xcom_pull(
        task_ids='Daily_revenue_YOY_query',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='Daily_revenue_YOY_df'
    )

    notion.blocks.children.append(
        PAGE_INFO["id"],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "\n\n1. ì¼ìë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    gcs_path = f'{gameidx}/graph1_dailySales_monthlySales.png'
    blob = bucket.blob(gcs_path)
    image_bytes = blob.download_as_bytes()
    filename = 'graph1_dailySales_monthlySales.png'

    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    # ì¼ìë³„ ë§¤ì¶œ
    # ê·¸ë˜í”„ëŠ” íŒŒì¼ ì €ì¥í›„ ì˜¬ë¦¬ëŠ” êµ¬ì¡°ë°–ì— ë˜ì§€ì•Šì•„ì„œ
    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": filename,
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    upload_url = file_upload[upload_url]

    # 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
    headers_upload = {
        "Content-Type": "image/png"
    }
    requests.put(upload_url, headers=headers_upload, data=image_bytes)

    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨
    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, BytesIO(image_bytes), "image/png")}
    headers_send = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION
    }
    send_resp = requests.post(send_url, headers=headers_send, files=files)
    send_resp.raise_for_status()

    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{file_upload_id}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO["id"],
        df=query_result1_dailySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¼ìë³„ ë§¤ì¶œ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO["id"],
        df=query_result1_monthlySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì „ë…„ ë™ì›”ëŒ€ë¹„ ë§¤ì¶œ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    response1_salesComment = daily_revenue_gemini(joyplegameid=joyplegameid)

    ## ì œë¯¸ë‚˜ì´
    blocks = md_to_notion_blocks(response1_salesComment)
    notion.blocks.children.append(
        block_id=PAGE_INFO["id"],
        children=blocks
    )

    return True




# 2. ìì²´ê²°ì œ ë§¤ì¶œ
def inhouse_sales_query(joyplegameid: int, **context):
    query = f"""
    select logdatekst, cast(sum(pgpricekrw) as int64) as rev
    from
    (SELECT t1.*,
        t2.PGRole,
        t2.PlatformDeviceTypeName,
        t2.PGName,
        t2.PGBuyCount,
        t2.PGPriceKRW
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V` AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = {joyplegameid}
    and pgrole = 'ìì²´ê²°ì œ'
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )
    group by 1 order by 1

    """
    query_result = query_run_method('2_inhouse_sales', query)

    context['task_instance'].xcom_push(key='inhouse_sales_df', value=query_result)

    return True

### 2> 24ë…„ë¶€í„° ì›”ë³„ ìì²´ê²°ì œ ë§¤ì¶œ
def inhouse_sales_before24_query(joyplegameid: int, **context):
    query = f"""
    select a.month
    , cast(a.rev_all as int64) as rev_all
    , cast(b.rev as int64) as rev_self
    , safe_divide(b.rev,a.rev_all) as self_per
    from
    (select month, sum(pricekrw) as rev_all
    from
    (select *,format_date('%Y-%m', logdatekst ) as month
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    group by 1
    ) as a

    left join
    (select month, cast(sum(pgpricekrw) as int64) as rev
    from
    (SELECT t1.*,
        t2.PGRole,
        t2.PlatformDeviceTypeName,
        t2.PGName,
        t2.PGBuyCount,
        t2.PGPriceKRW,
        format_date('%Y-%m', logdatekst ) as month
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V` AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = {joyplegameid}
    and pgrole = 'ìì²´ê²°ì œ'
    and logdatekst>='2024-01-01'
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY))
    group by 1) as b
    on a.month = b.month
    order by month
    """

    query_result = query_run_method('2_inhouse_sales', query)
    context['task_instance'].xcom_push(key='inhouse_sales_before24_df', value=query_result)

    return True

## ì œë¯¸ë‚˜ì´ í”„ë¡¬í”„íŠ¸ 
def inhouses_revenue_gemini(joyplegameid: int, **context):
    
    inhouse_sales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_query',
        key='inhouse_sales_df'
    )
    inhouse_sales_before24 = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )

    prompt_2 = f"""

    1. ì•„ë˜ëŠ” ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œê³¼ ê³¼ê±°ë¶€í„° ì¥ê¸°ì ì¸ ìì²´ê²°ì œ ë§¤ì¶œì´ì•¼.ê°„ë‹¨í•˜ê²Œ í•´ì„í•´ì¤˜.
    2. ìì²´ê²°ì œì— ëŒ€í•œ ì •ì˜ë¥¼ ì“¸ í•„ìš”ëŠ” ì—†ì–´.
    3. ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ê³ , ì¥ê¸°ì ì¸ ìì²´ê²°ì œ íŠ¸ë Œë“œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜.
    4. ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œì™€ ì¥ê¸°ì ì¸ ìì²´ê²°ì œ íŠ¸ë Œë“œ ë‹¨ë½ì„ ë‚˜ëˆ ì¤˜. ì˜ˆ) <ì¼ìë³„ ìì²´ê²°ì œ íŠ¸ë Œë“œ> , <ì¥ê¸°ì  ìì²´ê²°ì œ íŠ¸ë Œë“œ>
    5. ë‚´ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ë§ˆ.
    6. ë³€ìˆ˜ëª…ì— ëŒ€í•´ì„  ì–¸ê¸‰í•˜ì§€ë§ˆ.
    7. ìì²´ê²°ì œë€, êµ¬ê¸€ì´ë‚˜ ì• í”Œì˜ ë§ˆì¼“ ìˆ˜ìˆ˜ë£Œë¥¼ ì ˆê°í•˜ê¸° ìœ„í•´ ìˆ˜ìˆ˜ë£Œê°€ ë‚®ì€ ê²°ì œ í”Œë«í¼ì—ì„œ ê²°ì œí•˜ëŠ” ê²ƒì„ ë§í•´.
    8. ë‹¤ìŒì€ ìì²´ê²°ì œì— ëŒ€í•œ ë¶„ì„ì…ë‹ˆë‹¤ ì´ëŸ° ì‚¬ì „ì— ë§ í•˜ì§€ë§ê³  ê·¸ëƒ¥ ë¶„ì„ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜
    9. ì‚¬ê²¬ì„ ì“°ì§€ë§ê³  ê·¸ëƒ¥ í˜„ì¬ ìƒí™©ì— ëŒ€í•´ íŒ©íŠ¸ë§Œ ì•Œë ¤ì¤˜.


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.

    <ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ>
    {inhouse_sales}

    < ì¥ê¸°ì  ìì²´ê²°ì œ ë§¤ì¶œ>
    {inhouse_sales_before24}

    """

    response2_selfPaymentSales = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt_2,
        config=types.GenerateContentConfig(

            # ì˜ì–´ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì˜ ì´í•´í•  ìˆ˜ ìˆìŒ.
            system_instruction=SYSTEM_INSTRUCTION,
            #tools=[rag_retrieval_tool_test],
            temperature=0.5,
            labels=labels
            # max_output_tokens=2048
        )
    )

    return response2_selfPaymentSales.text

## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

def inhouse_revenue_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result2_dailySelfPaymentSales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_query',
        key='inhouse_sales_df'
    )
    
    # Seaborn ì„  ê·¸ë˜í”„
    sns.lineplot(
        x= query_result2_dailySelfPaymentSales.columns[0],
        y=query_result2_dailySelfPaymentSales.columns[1],
        data=query_result2_dailySelfPaymentSales,
        marker="o"
        )

    # ì˜µì…˜
    plt.title("ì´ë²ˆë‹¬ ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ")
    #plt.xlabel(query_result2_dailySelfPaymentSales.columns[0])   # ìë™ìœ¼ë¡œ ì»¬ëŸ¼ëª… í‘œì‹œ ê°€ëŠ¥
    #plt.ylabel(query_result2_dailySelfPaymentSales.columns[1])

    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(query_result2_dailySelfPaymentSales[query_result2_dailySelfPaymentSales.columns[0]][::1], rotation=45)

    # yì¶• 0ë¶€í„° ì‹œì‘
    plt.ylim(0, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§
    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    # x,yì¶• ì œê±°
    plt.xlabel(None)
    plt.ylabel(None)

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨

    ####################################### ì´ë¯¸ì§€ íŒŒì¼ì„ ì €ì¥í•  pathê°€ í•„ìš”í•¨ #####################
    filepath1_inhouseSales = "graph1_dailySelfPaymentSales.png"
    plt.savefig(filepath1_inhouseSales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_inhouseSales}')
    blob.upload_from_filename(filepath1_inhouseSales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_inhouseSales)

    return f'{gameidx}/{filepath1_inhouseSales}'



## í•œê¸€ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì§€ì •
# font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
# if Path(font_path).exists():
#     fm.fontManager.addfont(font_path)       # ìˆ˜ë™ ë“±ë¡
#     mpl.rc('font', family='NanumGothic')    # ê¸°ë³¸ í°íŠ¸ ì§€ì •
#     mpl.rc('axes', unicode_minus=False)     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
# else:
#     print("âš ï¸ NanumGothic ì„¤ì¹˜ ì‹¤íŒ¨. ë‹¤ë¥¸ í°íŠ¸ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.")

def inhouse_revenue_monthly_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result2_monthlySelfPaymentSales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )

    # Figure & Axes ìƒì„±
    fig, ax1 = plt.subplots(figsize=(10,5))

    # ì˜µì…˜
    plt.title("ì›”ë³„ ìì²´ê²°ì œ ë§¤ì¶œ & ìì²´ê²°ì œ ë§¤ì¶œ ë¹„ì¤‘ (24ë…„1ì›”~) ")

    # ì²« ë²ˆì§¸ yì¶• (ì™¼ìª½, ë§‰ëŒ€ê·¸ë˜í”„)
    ax1.bar(query_result2_monthlySelfPaymentSales["month"],
            query_result2_monthlySelfPaymentSales["rev_self"],
            color="#5B9BD5",
            #label="Sales"
            )
    #ax1.set_ylabel("Sales", color="black")
    ax1.tick_params(axis="y", labelcolor="black")

    # ë‘ ë²ˆì§¸ yì¶• (ì˜¤ë¥¸ìª½, ì„ ê·¸ë˜í”„)
    ax2 = ax1.twinx()
    ax2.plot(query_result2_monthlySelfPaymentSales["month"],
            query_result2_monthlySelfPaymentSales["self_per"],
            color="#ED7D31",
            marker="o",
            #label="Users"
            )
    #ax2.set_ylabel("Users", color="black")
    ax2.tick_params(axis="y", labelcolor="black")

    # ğŸ‘‰ ì„  ìœ„ì— ë°ì´í„° ë ˆì´ë¸” í‘œì‹œ
    for x, y in zip(query_result2_monthlySelfPaymentSales["month"],
                    query_result2_monthlySelfPaymentSales["self_per"]):
        ax2.annotate(f"{y:.0%}",  # 0.23 â†’ "23%"
                    xy=(x, y),
                    xytext=(0, 5),  # ì‚´ì§ ìœ„ë¡œ
                    textcoords="offset points",
                    ha="center", color="black"
                    )


    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    ax1.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))

    # í¼ì„¼íŠ¸ í¬ë§· ìë™ ì ìš© (self_perê°€ 0~1ì´ë©´ 1.0, 0~100ì´ë©´ 100)
    maxv = float(query_result2_monthlySelfPaymentSales["self_per"].max())
    ax2.yaxis.set_major_formatter(PercentFormatter(1.0 if maxv <= 1.5 else 100))

    # xì¶• ëˆˆê¸ˆ(ê°’) ì„¸ë¡œ íšŒì „ â€” ì¶• ê°ì²´ì— ì§ì ‘ ì ìš©
    for tick in ax1.get_xticklabels():
        tick.set_rotation(90)
        tick.set_ha("center")  # ë˜ëŠ” "right"ë¡œ ë°”ê¿”ë„ ë¨

    # ì œëª© & ê²©ì
    ax1.grid(axis="y", linestyle="--", alpha=0.7)

    plt.tight_layout()

    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    filepath1_inhouseMonthlySales = "graph1_monthlySelfPaymentSales.png"
    plt.savefig(filepath1_inhouseMonthlySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_inhouseMonthlySales}')
    blob.upload_from_filename(filepath1_inhouseMonthlySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_inhouseMonthlySales)

    return f'{gameidx}/{filepath1_inhouseMonthlySales}'


## ê·¸ë˜í”„ í•©ì¹˜ê¸°
### ìì²´ê²°ì œ ì¼ìë³„ + ìì²´ê²°ì œ ì›”ë³„

### Rê·¸ë£¹ë³„ ë§¤ì¶œê·¸ë˜í”„ì™€ PU ê·¸ë˜í”„ í•©ì¹˜ê¸°

def merge_inhouse_graph(joyplegameid: int, gameidx: str):
    # 1) íŒŒì¼ ê²½ë¡œ
    p1 = inhouse_revenue_graph_draw(joyplegameid, gameidx)
    p2 = inhouse_revenue_monthly_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph2_selfPaymentSales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path



def inhouse_revenue_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )
    query_result1_inhouseSales = context['task_instance'].xcom_pull(
        task_ids = 'inhouse_sales_query',
        key='inhouse_sales_df'
    )
    query_result1_inhouseMonthlySales = context['task_instance'].xcom_pull(
        task_ids='inhouse_sales_before24_query',
        key='inhouse_sales_before24_df'
    )
    
    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "2. ìì²´ê²°ì œ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    gcs_path = f'{gameidx}/graph2_selfPaymentSales.png'
    blob = bucket.blob(gcs_path)
    image_bytes = blob.download_as_bytes()
    filename = 'graph2_selfPaymentSales.png'

    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": filename,
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    upload_url = file_upload[upload_url]

    # 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
    headers_upload = {
        "Content-Type": "image/png"
    }
    requests.put(upload_url, headers=headers_upload, data=image_bytes)

    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    files = {"file": (filename, BytesIO(image_bytes), "image/png")}
    headers_send = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION
    }
    send_resp = requests.post(send_url, headers=headers_send, files=files)
    send_resp.raise_for_status()

    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{file_upload_id}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ## (3) ë¡œë°ì´í„°
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result1_inhouseSales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¼ìë³„ ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result1_inhouseMonthlySales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¥ê¸° ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ## (4) ì œë¯¸ë‚˜ì´ í•´ì„
    gemini_text = inhouses_revenue_gemini(joyplegameid)
    blocks = md_to_notion_blocks(gemini_text)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True




## ì´ë²ˆë‹¬ ê°€ì… ìœ ì €ì˜ êµ­ê°€ë³„ ë§¤ì¶œ
def cohort_by_country_revenue(joyplegameid: int, **context):
    query = f"""
    with countryRev as (
    select country2 as country, rev_rank2 as rev_rank, sum(rev) as rev
    from
    (select country, rev
    , case when rev_rank <= 9 then country  when rev_rank > 9 then 'etc' end as country2 # rev ê¸°ì¤€ 10ìœ„ë¶€í„°ëŠ” etc ë¡œ í‘œê¸°
    , case when rev_rank <= 9 then rev_rank when rev_rank > 9 then 10 end as rev_rank2
    from
    (select country, rev, row_number() OVER (ORDER BY rev desc ) AS rev_rank # rev ìˆœì„œëŒ€ë¡œ ë­í¬
    from
    (select countrycode as country, cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1)
    )
    )
    group by 1,2
    order by rev_rank # etc êµ­ê°€ê°€ ë§¨ ë’¤ë¡œ ê°€ì•¼í•¨
    ),

    allRev as  (
    select cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.rev,b.rev) as rev_percent
    from
    (select * from countryRev ) as a
    cross join
    (select * from allRev) as b
    order by rev_rank

    """
    query_result=query_run_method('3_global_ua', query)

    context['task_instance'].xcom_push(key='cohort_by_country_revenue_df', value=query_result)
    
    return True

## ì´ë²ˆë‹¬ êµ­ê°€ë³„ COST
def cohort_by_country_cost(joyplegameid: int, **context):
    query = f"""
    with countryCost as (
    select country2 as country, cost_rank2 as cost_rank, sum(cost) as cost
    from
    (select country, cost
    , case when cost_rank <= 9 then country when cost_rank > 9 then 'etc' end as country2 # cost ìˆœì„œëŒ€ë¡œ 10ìœ„ ë¶€í„°ëŠ” etc ë¡œ
    , case when cost_rank <= 9 then cost_rank else 10 end as cost_rank2
    from
    (select country, cost, row_number() OVER (ORDER BY cost desc ) AS cost_rank # cost ìˆœì„œë¡œ rank
    from
    (select countrycode as country, cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by countrycode))
    )
    group by 1,2
    order by cost_rank # etc êµ­ê°€ê°€ ë§¨ ë’¤ë¡œ ê°€ì•¼í•¨
    ),

    allCost as (
    select cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.cost,b.cost) as cost_percent
    from
    (select * from countryCost) as a
    cross join
    (select * from allCost) as b
    order by cost_rank


    """
    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='cohort_by_country_cost_df', value=query_result)
    
    return True


## êµ­ê°€ë³„ rev, cost í”„ë¡¬í”„íŠ¸
### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸
def cohort_by_gemini(joyplegameid: int, **context):
    
    cohort_country_revenue = context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_revenue',
        key='cohort_by_country_revenue_df'
    )
    cohort_country_cost = context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_cost',
        key='cohort_by_country_cost_df'
    )

    #client = genai.Client(api_key="AIzaSyAVv2B6DM6w9jd1MxiP3PbzAEMkl97SCGY")
    response3_revAndCostByCountry = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""
    ì´ë²ˆë‹¬ì— ì–´ë–¤ êµ­ê°€ì— ë§ˆì¼€íŒ…í–ˆê³ , ì–´ë–¤ êµ­ê°€ì—ì„œ ì‹ ê·œìœ ì €ì˜ ë§¤ì¶œì´ ë‚˜ì™”ëŠ”ì§€ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì¤„ê²Œ.
    ê°„ë‹¨í•˜ê²Œ í˜„í™© ìš”ì•½í•´ì¤˜.

    ë‹¹ì›” ë§ˆì¼€íŒ…ë¹„ìš©ì€ ì–¼ë§ˆì´ë©° ë‹¹ì›” ì‹ ê·œìœ ì € ë§¤ì¶œì€ ì–¼ë§ˆì…ë‹ˆë‹¤ë¥¼ ë¨¼ì € í•œì¤„ë¡œ ì„œë‘ì— ì–¸ê¸‰í•´ì¤˜.
    ì´ë²ˆë‹¬ COSTë§ì´ ì“´ êµ­ê°€ë“¤ ê°ê° COST ë¹„ì¤‘ì´ ëª‡% ì¸ì§€ í•œì¤„ì— ì¨ì£¼ê³ ,
    ì‹ ê·œìœ ì € ë§¤ì¶œ ë†’ì€ êµ­ê°€ë“¤ ê°ê° ëª‡% ë§¤ì¶œ ë¹„ì¤‘ì¸ì§€ í•œì¤„ì— ì¨ì¤˜ ì•Œë ¤ì¤˜.
    ê·¸ë¦¬ê³  ì£¼ìš” êµ­ê°€ë“¤ì— ëŒ€í•´ì„œ COST ë¹„ì¤‘ê³¼ ë§¤ì¶œë¹„ì¤‘ì„ ë¹„êµí•´ì„œ íŠ¹ì´í•œì ì´ ìˆëŠ”ê²ƒë§Œ ì•Œë ¤ì¤˜.

    ë§¤ì¶œê³¼ COST ì˜ ì•¡ìˆ˜ ì ˆëŒ“ê°’ì„ ë¹„êµí•˜ì§€ ë§ê³  ë¹„ì¤‘ì„ ë¹„êµí•´ì¤˜.
    etc ëŠ” ê¸°íƒ€ êµ­ê°€ë“¤ ì´ í•© í•œ ê°’ì´ë¼ì„œ etc ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ì•„ì¤˜.
    ë§ˆì¼€íŒ… íš¨ìœ¨ê°œì„ ì´ í•„ìš”í•˜ë‹¤ëŠ”ë§ì€ í•˜ì§€ë§ì•„ì¤˜.

    <ì›í•˜ëŠ” ì„œì‹>
    1. ìš”ì•½í•´ì£¼ê² ë‹¤ ë§ í•˜ì§€ë§ê³  ìš”ì•½í•œ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ì ì–´ì£¼ë©´ ë¼.
    2. ìŠµë‹ˆë‹¤. ì²´ë¡œ ì¨ì¤˜
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.

    <ë°ì´í„° ì„¤ëª…>
    ë§¤ì¶œì´ë‘ ë§ˆì¼€íŒ… ë¹„ìš©ì´ë‘ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ 9ê°œ êµ­ê°€ì™€ ê·¸ ì´í›„ 10ë²ˆì§¸ êµ­ê°€ë¶€í„°ëŠ” ì „ë¶€ etc êµ­ê°€ë¡œ ì²˜ë¦¬í–ˆì–´.
    etc ëŠ” êµ­ê°€ê°€ ì•„ë‹ˆë¼ ë‚˜ë¨¸ì§€ êµ­ê°€ ì´í•©ì´ì•¼.

    <ì´ë²ˆë‹¬ ê°€ì…ìœ ì €ì˜ êµ­ê°€ë³„ ë§¤ì¶œ>
    {cohort_country_revenue}

    <ì´ë²ˆë‹¬ êµ­ê°€ë³„ ë§ˆì¼€íŒ… ë¹„ìš©>
    {cohort_country_cost}

    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=labels
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response3_revAndCostByCountry.text


# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

## OSë³„ cost
def os_cost(joyplegameid: int, **context):
    query = f"""
    with osCost as (
    select os, cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by os
    ),

    allCost as (
    select cast(sum(cost) as int64) cost
    from `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = {joyplegameid}
    and cmpgndate >= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and cmpgndate <=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.cost,b.cost) as cost_percent
    from
    (select * from osCost) as a
    cross join
    (select * from allCost) as b
    """

    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='os_cost_df', value=query_result)

    return True

## OSë³„ ë§¤ì¶œ
def os_rev(joyplegameid: int, **context):
    query = f"""
    with osRev as (

    select os, rev from (
    select os, cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1)
    where rev>0
    ),

    allRev as (
    select cast(sum(pricekrw) as int64) as rev
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst>= DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and authaccountregdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    )

    select a.*, safe_divide(a.rev,b.rev) as rev_percent
    from
    (select * from osRev) as a
    cross join
    (select * from allRev) as b
    """
    ## 129.93MB
    query_result =query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='os_rev_df', value=query_result)

    return True


### 4> ì¼ìë³„ ë§¤ì¶œì— ëŒ€í•œ ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸

#client = genai.Client(api_key="AIzaSyAVv2B6DM6w9jd1MxiP3PbzAEMkl97SCGY")
def os_by_gemini(joyplegameid: int, **context):
    
    os_rev_df= context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )
    os_cost_df= context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    response3_revAndCostByOs = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""

    ì´ë²ˆë‹¬ì— IOS ì— ëª‡ % ë§ˆì¼€íŒ… ë¹„ìš© ì‚¬ìš©í–ˆìœ¼ë©° IOS ì˜ ë§¤ì¶œë¹„ì¤‘ì€ ëª‡% ì…ë‹ˆë‹¤.
    ì˜ í˜•ì‹ìœ¼ë¡œ ì•Œë ¤ì¤˜.


    <ì›í•˜ëŠ” ì„œì‹>
    1. ìš”ì•½í•´ì£¼ê² ë‹¤ ë§ í•˜ì§€ë§ê³  ìš”ì•½í•œ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ì ì–´ì£¼ë©´ ë¼.
    2. ìŠµë‹ˆë‹¤. ì²´ë¡œ ì¨ì¤˜
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    <ë°ì´í„° ì„¤ëª…>


    <ì´ë²ˆë‹¬ ê°€ì…ìœ ì €ì˜ OSë³„ ë§¤ì¶œ>
    {os_rev_df}

    <ì´ë²ˆë‹¬ OSë³„ ë§ˆì¼€íŒ… ë¹„ìš©>
    {os_cost_df}
    """,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5,
            labels=labels
        )

    )
    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response3_revAndCostByOs.text

# ì½”ë©˜íŠ¸ ì •ë¦¬ ( í–¥í›„ ìš”ì•½ì— ì‚¬ìš©í•˜ê¸° ìš©ë„ )
#gemini_result.loc[len(gemini_result)] = response.text

### ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
## êµ­ê°€ë³„ ë§¤ì¶œ

def by_country_revenue_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result3_revByCountry = context['task_instance'].xcom_pull(
        task_ids = 'cohort_by_country_revenue',
        key='cohort_by_country_revenue_df'
    )

    sizes = query_result3_revByCountry["rev"].to_numpy()
    labels = query_result3_revByCountry["country"].to_numpy()
    total  = sizes.sum()

    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )
    ax.set_title("êµ­ê°€ë³„ ë§¤ì¶œ ë¹„ì¤‘", pad=24)
    #plt.title("êµ­ê°€ë³„ ë§¤ì¶œ ë¹„ì¤‘")
    plt.tight_layout()

    filepath1_dailySales = "graph3_revByCountry.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'



def by_country_cost_graph_draw(joyplegameid: int, gameidx: str, **context):
    
    query_result3_costByCountry = context['task_instance'].xcom_pull(
        task_ids = 'cohort_by_country_cost',
        key='cohort_by_country_cost_df'
    )

    ### êµ­ê°€ë³„ Cost
    sizes = query_result3_costByCountry["cost"].to_numpy()
    labels = query_result3_costByCountry["country"].to_numpy()
    total  = sizes.sum()


    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )
    ax.set_title("êµ­ê°€ë³„ COST ë¹„ì¤‘", pad=24)
    plt.tight_layout()


    filepath1_dailySales = "graph3_costByCountry.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


def merge_contry_graph(joyplegameid: int, gameidx: str):
    p1=by_country_revenue_graph_draw(joyplegameid, gameidx)
    p2=by_country_cost_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph3_revAndCostByCountry.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path

### OS ë³„ ë§¤ì¶œ
def os_rev_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result3_revByOs = context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    sizes = query_result3_revByOs["rev"].to_numpy()
    labels = query_result3_revByOs["os"].to_numpy()
    total  = sizes.sum()


    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )

    ax.set_title("OSë³„ ë§¤ì¶œ ë¹„ì¤‘", pad=24)
    plt.tight_layout()

    filepath1_dailySales = "graph3_revByOs.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'
    

### os ë³„ Cost
def os_cost_graph_draw(joyplegameid: int, gameidx: str, **context):

    query_result3_costByOs = context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )

    sizes = query_result3_costByOs["cost"].to_numpy()
    labels = query_result3_costByOs["os"].to_numpy()
    total  = sizes.sum()
    
    fig, ax = plt.subplots(figsize=(5,5))
    wedges, _ = ax.pie(sizes, labels=None, startangle=90)

    # ê° ì›¨ì§€ì˜ ì¤‘ì•™ê°(ë„), ë‚´ë¶€/ì™¸ë¶€ ì¢Œí‘œ ê³„ì‚°
    angles = [(p.theta1 + p.theta2)/2 for p in wedges]
    inside_r, outside_r = 0.6, 1.28

    # 1) ë¼ë²¨ì„ "ì´ë¦„ (x.x%)" í˜•ì‹ìœ¼ë¡œ ìš°ì„  ë‚´ë¶€ì— ë°°ì¹˜
    texts = []
    for ang, size, name in zip(angles, sizes, labels):
        percent = size / total * 100
        txt = f"{name} ({percent:.1f}%)"
        x_in = np.cos(np.deg2rad(ang)) * inside_r
        y_in = np.sin(np.deg2rad(ang)) * inside_r
        t = ax.text(x_in, y_in, txt, ha='center', va='center', fontsize=9, color="black")
        texts.append(t)

    # 2) ê²¹ì¹¨ ê°ì§€ í•¨ìˆ˜ (ë””ìŠ¤í”Œë ˆì´ ì¢Œí‘œì—ì„œ bbox ê²¹ì¹¨ í™•ì¸)
    def any_overlaps(texts, renderer):
        bboxes = [t.get_window_extent(renderer=renderer).expanded(1.05, 1.2) for t in texts]
        overlaps = set()
        for i in range(len(bboxes)):
            for j in range(i+1, len(bboxes)):
                if bboxes[i].overlaps(bboxes[j]):
                    overlaps.add(i); overlaps.add(j)
        return overlaps

    # 3) ê²¹ì¹˜ëŠ” ê²ƒë§Œ ì™¸ë¶€ë¡œ ì¬ë°°ì¹˜ + í™”ì‚´í‘œ ì—°ê²° (ì‘ì€ íŒŒì´ì¼ìˆ˜ë¡ ìš°ì„  ì´ë™)
    fig.canvas.draw()  # ë Œë”ëŸ¬ ì¤€ë¹„
    over_idx = any_overlaps(texts, fig.canvas.get_renderer())

    # ê²¹ì¹˜ëŠ” í…ìŠ¤íŠ¸ ì¤‘, ì›¨ì§€ ë©´ì (=sizes) ì‘ì€ ê²ƒë¶€í„° ë°”ê¹¥ìœ¼ë¡œ
    idx_sorted = sorted(list(over_idx), key=lambda i: sizes[i])
    for i in idx_sorted:
        ang = angles[i]
        # ì› ë°– ë¼ë²¨ ìœ„ì¹˜
        x_out = np.cos(np.deg2rad(ang)) * outside_r
        y_out = np.sin(np.deg2rad(ang)) * outside_r
        # ì› ê²½ê³„ ìª½(í™”ì‚´í‘œ ê¸°ì¤€ì )
        x_edge = np.cos(np.deg2rad(ang)) * 1.0
        y_edge = np.sin(np.deg2rad(ang)) * 1.0

        # ê¸°ì¡´ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ¨ê¸°ê³ (ë˜ëŠ” ì œê±°) ë°”ê¹¥ì— ìƒˆë¡œ ë°°ì¹˜
        txt_str = texts[i].get_text()
        texts[i].set_visible(False)

        # ì¢Œìš° ì •ë ¬ì€ ë°˜ëŒ€ìª½ìœ¼ë¡œ ë§ì¶”ë©´ ë³´ê¸° ì¢‹ìŒ
        ha = 'left' if x_out >= 0 else 'right'
        ax.annotate(
            txt_str,
            xy=(x_edge, y_edge), xycoords='data',           # í™”ì‚´í‘œ ë„ì°©ì (íŒŒì´ ê²½ê³„)
            xytext=(x_out, y_out), textcoords='data',       # í…ìŠ¤íŠ¸ ìœ„ì¹˜(ì› ë°–)
            ha=ha, va='center', fontsize=9, color='black',
            arrowprops=dict(arrowstyle='-', color='gray', shrinkA=0, shrinkB=0)
        )

    ax.set_title("OSë³„ COST ë¹„ì¤‘", pad=24)
    plt.tight_layout()

    filepath1_dailySales = "graph3_costByOs.png"
    plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
    blob.upload_from_filename(filepath1_dailySales)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(filepath1_dailySales)

    return f'{gameidx}/{filepath1_dailySales}'


def merge_os_graph(joyplegameid: int, gameidx: str):
    p1 = os_rev_graph_draw(joyplegameid, gameidx)
    p2 = os_cost_graph_draw(joyplegameid, gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1))
    im2 = Image.open(BytesIO(im2))

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph3_revAndCostByOs.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


#### ë…¸ì…˜ì— ì—…ë¡œë“œ

def country_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "3. ê¸€ë¡œë²Œ ëª¨ê° ì§€í‘œ " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "(1) êµ­ê°€ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ë§¤ì¶œë¹„ì¤‘" }}]
                },
            }
        ],
    )

    query_result3_revByCountry=context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_revenue',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='cohort_by_country_revenue_df'
    )
    query_result3_costByCountry=context['task_instance'].xcom_pull(
        task_ids='cohort_by_country_cost',  # â† ì²« ë²ˆì§¸ Taskì˜ task_id
        key='cohort_by_country_cost_df'
    )

    filePath3_revAndCostByCountry = merge_contry_graph(joyplegameid, gameidx)
    ########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
    ## IAP+ìœ ê°€ì ¬
    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": os.path.basename(filePath3_revAndCostByCountry),
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨

    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(filePath3_revAndCostByCountry, "rb") as f:
        files = {"file": (os.path.basename(filePath3_revAndCostByCountry), f, "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
            # Content-Typeì€ filesë¡œ ìë™ ì„¤ì •ë¨
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()


    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ### ë¡œë°ì´í„° ì œê³µ
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_revByCountry,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_costByCountry,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ COST  ",
        max_first_batch_rows=90,
        batch_size=100,
    )


    ### êµ­ê°€ë³„ cost rev ì½”ë©˜íŠ¸
    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„

    text = cohort_by_gemini(joyplegameid)
    blocks = md_to_notion_blocks(text)
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


    ## ë¶€ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "(2) OSë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ë§¤ì¶œë¹„ì¤‘" }}]
                },
            }
        ],
    )




## osë³„ cost, rev ê·¸ë˜í”„
########### (2) ê·¸ë˜í”„ ì—…ë¡œë“œ
## IAP+ìœ ê°€ì ¬
def country_data_upload_to_notion(joyplegameid: int, gameidx: str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    query_result3_costByOs= context['task_instance'].xcom_pull(
        task_ids='os_cost',
        key='os_cost_df'
    )
    query_result3_revByOs= context['task_instance'].xcom_pull(
        task_ids='os_rev',
        key='os_rev_df'
    )

    filePath3_revAndCostByOs = merge_os_graph(joyplegameid, gameidx)

    # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
    create_url = "https://api.notion.com/v1/file_uploads"
    payload = {
        "filename": os.path.basename(filePath3_revAndCostByOs),
        "content_type": "image/png"
    }
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
    resp.raise_for_status()
    file_upload = resp.json()
    file_upload_id = file_upload["id"]   # ì—…ë¡œë“œ ID
    # file_upload["upload_url"] ë„ ì‘ë‹µì— í¬í•¨ë¨

    # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
    send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
    with open(filePath3_revAndCostByOs, "rb") as f:
        files = {"file": (os.path.basename(filePath3_revAndCostByOs), f, "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
            # Content-Typeì€ filesë¡œ ìë™ ì„¤ì •ë¨
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()


    # 3) ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ í˜ì´ì§€ì— ì²¨ë¶€
    append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
    append_payload = {
        "children": [
            {
                "object": "block",
                "type": "image",
                "image": {
                    "type": "file_upload",
                    "file_upload": {"id": file_upload_id},
                    # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                }
            }
        ]
    }

    headers_json_patch = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    append_resp = requests.patch(append_url, headers=headers_json_patch, data=json.dumps(append_payload))
    append_resp.raise_for_status()

    ### ë¡œë°ì´í„° ì œê³µ
    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_revByOs,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì¥ê¸° ìì²´ê²°ì œ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result3_costByOs,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - OSë³„ COST ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ## osë³„ cost, rev ì½”ë©˜íŠ¸
    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„
    blocks = md_to_notion_blocks(os_by_gemini(joyplegameid))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


# ìµœê·¼ 30ì¼ ê¸°ì¤€ êµ­ê°€ê·¸ë£¹ë³„ X ê²°ì œì²˜ë³„ ë§¤ì¶œ ì¿¼ë¦¬
# ì§€ë¦¬ì  ì£¼ìš” êµ­ê°€ë³„ë¡œ ê·¸ë£¹í™”
# í•œêµ­
# ë¯¸êµ­
# ì¼ë³¸
# ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„: ì¤‘êµ­, ëŒ€ë§Œ, í™ì½©, ì‹±ê°€í¬ë¥´, íƒœêµ­, ë² íŠ¸ë‚¨, ë§ë ˆì´ì‹œì•„, í•„ë¦¬í•€, ì¸ë„ë„¤ì‹œì•„, ì¸ë„, í˜¸ì£¼, ë‰´ì§ˆëœë“œ
# ì¤‘ë™: ì•„ëì—ë¯¸ë¦¬íŠ¸, ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„, í„°í‚¤, ì´ë€, ì´ìŠ¤ë¼ì—˜, ì¹´íƒ€ë¥´, ì¿ ì›¨ì´íŠ¸, ì˜¤ë§Œ, ë°”ë ˆì¸, ìš”ë¥´ë‹¨
# ì„œìœ ëŸ½: ì˜êµ­, í”„ë‘ìŠ¤, ë…ì¼, ì´íƒˆë¦¬ì•„, ìŠ¤í˜ì¸, ë„¤ëœë€ë“œ, ë²¨ê¸°ì—, ìŠ¤ìœ„ìŠ¤, ì˜¤ìŠ¤íŠ¸ë¦¬ì•„, ì•„ì¼ëœë“œ, í¬ë¥´íˆ¬ê°ˆ
# ë™ìœ ëŸ½: í´ë€ë“œ, ì²´ì½”, í—ê°€ë¦¬, ë£¨ë§ˆë‹ˆì•„, ìŠ¬ë¡œë°”í‚¤ì•„, ëŸ¬ì‹œì•„, ìš°í¬ë¼ì´ë‚˜, ë¶ˆê°€ë¦¬ì•„, ìŠ¬ë¡œë² ë‹ˆì•„, í¬ë¡œì•„í‹°ì•„
# ì•„ë©”ë¦¬ì¹´: ìºë‚˜ë‹¤, ë©•ì‹œì½”, ë¸Œë¼ì§ˆ, ì•„ë¥´í—¨í‹°ë‚˜, ì¹ ë ˆ, ì½œë¡¬ë¹„ì•„, í˜ë£¨
# ê¸°íƒ€: ê·¸ ì™¸ êµ­ê°€

def country_group_rev(joyplegameid: int, gameidx: str, **context):
    query = f"""
    with chk as (
    SELECT
    perf.LogDateKST,
    perf,AuthAccountName,
    perf.CountryGroup,
        pg.PGRole,
        pg.PlatformDeviceTypeName,
        pg.PGName,
        pg.PGBuyCount,
        pg.PGPriceKRW
    FROM
    (
        select * except(CountryGroup)
            , CASE
    WHEN CountryCode = 'KR' THEN 'í•œêµ­'
    WHEN CountryCode = 'JP' THEN 'ì¼ë³¸'
    WHEN CountryCode = 'US' THEN 'ë¯¸êµ­'
    WHEN CountryCode IN ('CN', 'TW', 'HK', 'SG', 'TH', 'VN', 'MY', 'PH', 'ID', 'IN', 'AU', 'NZ') THEN 'ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„'
    WHEN CountryCode IN ('AE', 'SA', 'TR', 'IR', 'IL', 'QA', 'KW', 'OM', 'BH', 'JO') THEN 'ì¤‘ë™'
    WHEN CountryCode IN ('GB', 'FR', 'DE', 'IT', 'ES', 'NL', 'BE', 'CH', 'AT', 'IE', 'PT') THEN 'ì„œìœ ëŸ½'
    WHEN CountryCode IN ('PL', 'CZ', 'HU', 'RO', 'SK', 'RU', 'UA', 'BG', 'SI', 'HR') THEN 'ë™ìœ ëŸ½'
    WHEN CountryCode IN ('CA', 'MX', 'BR', 'AR', 'CL', 'CO', 'PE') THEN 'ì•„ë©”ë¦¬ì¹´'
    ELSE 'ê¸°íƒ€'
    END AS CountryGroup
        from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
        where joyplegameid = {joyplegameid}
                and logdateKst >= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 14 DAY)
                and logdatekst <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    ) AS perf,
    UNNEST(perf.PaymentDetailArrayStruct) AS pg
    )

    select CountryGroup, LogDateKST, PGName, sum(PGPriceKRW) as Sales
    from chk
    group by CountryGroup, LogDateKST, PGName
    order by case when CountryGroup = 'í•œêµ­' then 1
                when CountryGroup = 'ë¯¸êµ­' then 2
                when CountryGroup = 'ì¼ë³¸' then 3
                when CountryGroup = 'ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„' then 4
                when CountryGroup = 'ì„œìœ ëŸ½' then 5
                when CountryGroup = 'ë™ìœ ëŸ½' then 6
                when CountryGroup = 'ì•„ë©”ë¦¬ì¹´' then 7
                when CountryGroup = 'ì¤‘ë™' then 8
                when CountryGroup = 'ê¸°íƒ€' then 9
            end
            , LogDateKST, PGName
    """


    query_result = query_run_method('3_global_ua', query)
    context['task_instance'].xcom_push(key='country_group_rev', value=query_result)

    return True

def country_group_to_df(**context):

    query_result = context['task_instance'].xcom_pull(
        task_ids='country_group_rev',
        key='country_group_rev'
    )

    grouped_dfs = {
        country: group_df.pivot_table(
            index="LogDateKST",
            columns="PGName",
            values="Sales",
            aggfunc="sum",
            fill_value=0
        )
        for country, group_df in query_result.groupby("CountryGroup")
    }


    # ë¡œë°ì´í„° ì œê³µìš© ë°ì´í„°í”„ë ˆì„
    grouped_dfs_union = query_result.pivot_table(
        index=["CountryGroup", "LogDateKST"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="PGName",
        values="Sales",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    num_cols = grouped_dfs_union.select_dtypes(include="number").columns
    grouped_dfs_union[num_cols] = grouped_dfs_union[num_cols].astype(int)

    return grouped_dfs, grouped_dfs_union



def country_group_to_df_gemini(joyplegameid: int, service_sub: str, **context):

    query_result = context['task_instance'].xcom_pull(
        task_ids='country_group_rev',
        key='country_group_rev'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response_GeoPGSales = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents=f"""
    ì§€ë‚œ 2ì£¼ê°„ êµ­ê°€ê·¸ë£¹ë³„ë¡œ ì¼ê°„ ê²°ì œì²˜ë³„ ë§¤ì¶œ ë°ì´í„°ì•¼.\n{query_result.to_csv(index=False)}
    ê° êµ­ê°€ê·¸ë£¹ì—ì„œ ê²°ì œì²˜ë³„ë¡œ ì¼ê°„ ë§¤ì¶œíë¦„ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€, ì–´ë–¤ ê²°ì œì²˜ì˜ ë§¤ì¶œì´ ì–¸ì œ ê¸‰ì¦í–ˆëŠ”ì§€ë¥¼ ìš”ì•½í•´ì„œ 6ì¤„ ì´ë‚´ë¡œ ì•Œë ¤ì¤˜.
    ë§¤ì¶œ ê¸‰ì¦ì˜ ì›ì¸ì„ íŒŒì•…í•˜ì§€ëŠ” ë§ì•„ì¤˜.
    #, ##, ###, ####ì€ ì‚¬ìš©í•˜ì§€ ë§ì•„ì¤˜.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.3
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    return response_GeoPGSales.text



def country_group_df_draw(joyplegameid: int, gameidx: str, **context):
    
    gcs_paths = []
    grouped_dfs, _ = country_group_to_df(**context)

    # âœ… ëª¨ë“  ê·¸ë£¹ë³„ë¡œ ê·¸ë˜í”„ ìƒì„±
    for country, df in grouped_dfs.items():
        # index(LogDateKST)ê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if not pd.api.types.is_datetime64_any_dtype(df.index):
            df.index = pd.to_datetime(df.index, errors="coerce")

        # ìˆ«ìí˜• ë³€í™˜ (Salesê°’)
        df = df.map(
            lambda x: pd.to_numeric(str(x).replace(",", "").replace("-", "0"), errors="coerce")
        )

        # âœ… ì¸ë±ìŠ¤ë¥¼ xì¶•ìœ¼ë¡œ ì‚¬ìš©
        x = df.index

        fig, ax = plt.subplots(figsize=(10, 5))

        # ê²°ì œìˆ˜ë‹¨ë³„ ì„ ê·¸ë˜í”„
        for col in df.columns:
            ax.plot(
                x, df[col],
                marker='o', markersize=3, linewidth=1,
                label=col
            )

        plt.title(f"{country} - ì¼ìë³„ ê²°ì œìˆ˜ë‹¨ ë§¤ì¶œ ì¶”ì´")
        plt.xlabel("ë‚ ì§œ")
        plt.ylabel("ë§¤ì¶œì•¡")

        # yì¶• ì²œ ë‹¨ìœ„ í¬ë§·
        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

        # xì¶• ë¼ë²¨ íšŒì „
        plt.xticks(x, rotation=45)

        # ë²”ë¡€, ë³´ì¡°ì„ , ì €ì¥
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()

        filepath1_dailySales = f"graph_{country}.png"
        plt.savefig(filepath1_dailySales, dpi=160) # dpi : í•´ìƒë„
        plt.close()

        blob = bucket.blob(f'{gameidx}/{filepath1_dailySales}')
        blob.upload_from_filename(filepath1_dailySales)

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(filepath1_dailySales)

        # gcs ê²½ë¡œ ì¶”ê°€
        gcs_paths.append(f'{gameidx}/{filepath1_dailySales}')

    return gcs_paths
    

def merge_images_by_three_gcs(
    bucket,
    gcs_image_paths: List[str],
    output_dir: str,
    gameidx: str,
    gap: int = 0,
    bg_color: Tuple[int, int, int, int] = (255, 255, 255, 0),
    cleanup_temp: bool = True
) -> List[str]:

    def pad_to_height(img: Image.Image, h: int, bg: Tuple = bg_color) -> Image.Image:
        """ì´ë¯¸ì§€ì˜ ë†’ì´ë¥¼ ë§ì¶°ì¤Œ (ì„¸ë¡œ íŒ¨ë”© ì¶”ê°€)"""
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    print("ì²˜ë¦¬í•  GCS ì´ë¯¸ì§€ ëª©ë¡:")
    for i, path in enumerate(gcs_image_paths, 1):
        print(f"  {i}. {path}")
    print()
    
    uploaded_paths = []
    temp_files = []
    
    # 3ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
    for batch_num, i in enumerate(range(0, len(gcs_image_paths), 3), 1):
        imgs = []
        names = []
        
        # ìµœëŒ€ 3ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ
        for j in range(3):
            if i + j < len(gcs_image_paths):
                gcs_path = gcs_image_paths[i + j]
                
                try:
                    # GCSì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    blob = bucket.blob(gcs_path)
                    image_bytes = blob.download_as_bytes()
                    img = Image.open(BytesIO(image_bytes)).convert("RGBA")
                    imgs.append(img)
                    
                    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° ë° "graph_" ì œê±°
                    name = os.path.splitext(os.path.basename(gcs_path))[0]
                    if name.startswith("graph_"):
                        name = name.replace("graph_", "", 1)
                    names.append(name)
                    
                except Exception as e:
                    print(f"ê²½ê³ : GCSì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {gcs_path}")
                    print(f"  ì—ëŸ¬: {str(e)}")
                    continue
        
        if not imgs:
            print(f"ë°°ì¹˜ {batch_num}: ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # ì„¸ë¡œ ë§ì¶”ê¸°
        target_h = max(img.height for img in imgs)
        imgs_padded = [pad_to_height(img, target_h) for img in imgs]
        
        # ê°€ë¡œë¡œ í•©ì¹˜ê¸°
        total_width = sum(img.width for img in imgs_padded) + gap * (len(imgs_padded) - 1)
        out = Image.new("RGBA", (total_width, target_h), bg_color)
        
        x_offset = 0
        for img in imgs_padded:
            out.paste(img, (x_offset, 0), img)
            x_offset += img.width + gap
        
        # íŒŒì¼ëª… êµ¬ì„±
        merged_filename = "graph_" + " ë° ".join(names) + ".png"
        temp_filepath = f"/tmp/{merged_filename}"
        
        # ë¡œì»¬ì— ì„ì‹œ ì €ì¥
        out.save(temp_filepath)
        temp_files.append(temp_filepath)
        
        # GCS ê²½ë¡œ êµ¬ì„±
        gcs_upload_path = f"{gameidx}/{output_dir}/{merged_filename}"
        
        # GCSì— ì—…ë¡œë“œ
        upload_blob = bucket.blob(gcs_upload_path)
        upload_blob.upload_from_filename(temp_filepath)
        
        print(f"âœ“ ë°°ì¹˜ {batch_num} ì €ì¥ë¨: gs://{bucket.name}/{gcs_upload_path}")
        uploaded_paths.append(gcs_upload_path)
    
    # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if cleanup_temp:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        print(f"\nâœ“ {len(temp_files)}ê°œì˜ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    
    print(f"\nì´ {len(uploaded_paths)}ê°œì˜ í•©ì³ì§„ ì´ë¯¸ì§€ê°€ GCSì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    return uploaded_paths


def merge_country_group_df_draw(joyplegameid: int, gameidx: str, **context):
    """
    Airflow DAGì—ì„œ ì‚¬ìš©í•  wrapper í•¨ìˆ˜
    """
    from google.cloud import storage
    
    # GCS í´ë¼ì´ì–¸íŠ¸ ë° ë²„í‚· ì´ˆê¸°í™”
    client = storage.Client()
    bucket = client.bucket("game-framework1")  # ë²„í‚·ëª… ìˆ˜ì • í•„ìš”
    
    # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ë¦¬ìŠ¤íŠ¸)
    img_gcs_list = country_group_df_draw(joyplegameid, gameidx, **context)
    
    # í•©ì¹˜ê¸° ì²˜ë¦¬
    merged_paths = merge_images_by_three_gcs(
        bucket=bucket,
        gcs_image_paths=img_gcs_list,
        output_dir="merged",  # GCS ë‚´ ì¶œë ¥ ë””ë ‰í† ë¦¬
        gameidx=gameidx,
        gap=0,
        bg_color=(255, 255, 255, 0),
        cleanup_temp=True
    )
    
    return merged_paths


def country_group_data_upload_to_notion(joyplegameid: int, gameidx: str, bucket_name: str = "game-framework1", merged_image_dir: str= "merged", **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "êµ­ê°€ë³„ X ê²°ì œì²˜ë³„ ì§€í‘œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ğŸŒ êµ­ê°€ê·¸ë£¹ ë¶„ë¥˜ ê¸°ì¤€\n1. í•œêµ­\n2. ë¯¸êµ­\n3. ì¼ë³¸\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "4. ì„œìœ ëŸ½: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì˜êµ­, í”„ë‘ìŠ¤, ë…ì¼, ì´íƒˆë¦¬ì•„, ìŠ¤í˜ì¸, ë„¤ëœë€ë“œ, ë²¨ê¸°ì—, ìŠ¤ìœ„ìŠ¤, ì˜¤ìŠ¤íŠ¸ë¦¬ì•„, ì•„ì¼ëœë“œ, í¬ë¥´íˆ¬ê°ˆ\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "5. ë™ìœ ëŸ½: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "í´ë€ë“œ, ì²´ì½”, í—ê°€ë¦¬, ë£¨ë§ˆë‹ˆì•„, ìŠ¬ë¡œë°”í‚¤ì•„, ëŸ¬ì‹œì•„, ìš°í¬ë¼ì´ë‚˜, ë¶ˆê°€ë¦¬ì•„, ìŠ¬ë¡œë² ë‹ˆì•„, í¬ë¡œì•„í‹°ì•„\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "6. ë™ì•„ì‹œì•„ & ì˜¤ì„¸ì•„ë‹ˆì•„: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì¤‘êµ­, ëŒ€ë§Œ, í™ì½©, ì‹±ê°€í¬ë¥´, íƒœêµ­, ë² íŠ¸ë‚¨, ë§ë ˆì´ì‹œì•„, í•„ë¦¬í•€, ì¸ë„ë„¤ì‹œì•„, ì¸ë„, í˜¸ì£¼, ë‰´ì§ˆëœë“œ\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "7. ì•„ë©”ë¦¬ì¹´: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ìºë‚˜ë‹¤, ë©•ì‹œì½”, ë¸Œë¼ì§ˆ, ì•„ë¥´í—¨í‹°ë‚˜, ì¹ ë ˆ, ì½œë¡¬ë¹„ì•„, í˜ë£¨\n"}},
                                            {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "8. ì¤‘ë™: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ì•„ëì—ë¯¸ë¦¬íŠ¸, ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„, í„°í‚¤, ì´ë€, ì´ìŠ¤ë¼ì—˜, ì¹´íƒ€ë¥´, ì¿ ì›¨ì´íŠ¸, ì˜¤ë§Œ, ë°”ë ˆì¸, ìš”ë¥´ë‹¨\n"}},
                        {'annotations': {'bold': True,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "9. ê¸°íƒ€: "}},
                        {'annotations': {'bold': False,
                                                'code': False,
                                                'color': 'default',
                                                'italic': False,
                                                'strikethrough': False,
                                                'underline': False},
                        "type": "text", "text": {"content": "ê·¸ ì™¸ êµ­ê°€ë“¤"}},
                        ]
                },
            }
        ],
    )

    # GCS í´ë¼ì´ì–¸íŠ¸ ë° ë²„í‚· ì´ˆê¸°í™”
    gcs_client = storage.Client()
    bucket = gcs_client.bucket(bucket_name)
    
    # GCSì—ì„œ í•©ì³ì§„ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ
    gcs_image_paths = []
    blobs = gcs_client.list_blobs(
        bucket_name,
        prefix=f"{gameidx}/{merged_image_dir}/"
    )

    for blob in blobs:
        # "ë°"ì´ í¬í•¨ëœ PNG íŒŒì¼ë§Œ í•„í„°ë§
        if blob.name.lower().endswith(".png") and "ë°" in blob.name:
            gcs_image_paths.append(blob.name)
    
    # íŒŒì¼ëª… ì—­ìˆœ ì •ë ¬
    gcs_image_paths.sort(reverse=True)
    
    print(f"ì—…ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜: {len(gcs_image_paths)}ê°œ")
    print("ì´ë¯¸ì§€ ëª©ë¡:")
    for path in gcs_image_paths:
        print(f"  - {path}")
    

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    # GCSì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Notion ì—…ë¡œë“œ
    for gcs_path in gcs_image_paths:
        filename = gcs_path.split('/')[-1]
        print(f"\nì—…ë¡œë“œ ì¤‘: {filename}")
        
        try:
            # GCSì—ì„œ ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ
            blob = bucket.blob(gcs_path)
            image_bytes = blob.download_as_bytes()
            
            # íŒŒì¼ ì—…ë¡œë“œ ê°ì²´ ìƒì„±
            create_url = "https://api.notion.com/v1/file_uploads"
            payload = {
                "filename": filename,
                "content_type": "image/png"
            }
            resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
            resp.raise_for_status()
            file_upload = resp.json()
            file_upload_id = file_upload["id"]
            
            # íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡
            send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
            files = {"file": (filename, BytesIO(image_bytes), "image/png")}
            headers_send = {
                "Authorization": f"Bearer {NOTION_TOKEN}",
                "Notion-Version": NOTION_VERSION
            }
            send_resp = requests.post(send_url, headers=headers_send, files=files)
            send_resp.raise_for_status()
            
            # Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ ì²¨ë¶€
            append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
            append_payload = {
                "children": [
                    {
                        "object": "block",
                        "type": "image",
                        "image": {
                            "type": "file_upload",
                            "file_upload": {"id": file_upload_id},
                        }
                    }
                ]
            }
            
            append_resp = requests.patch(
                append_url, headers=headers_json, data=json.dumps(append_payload)
            )
            append_resp.raise_for_status()
            
            print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion API ì—ëŸ¬: {filename}")
            print(f"  ì—ëŸ¬: {str(e)}")
            continue
        except Exception as e:
            print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ì—…ë¡œë“œ ì—ëŸ¬: {filename}")
            print(f"  ì—ëŸ¬: {str(e)}")
            continue
    
    print("\nğŸ‰ ëª¨ë“  ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ!")

    _, grouped_dfs_union =country_group_to_df(**context)

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=grouped_dfs_union,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - êµ­ê°€ë³„ X ê²°ì œì²˜ë³„ ì§€í‘œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(country_group_to_df_gemini(joyplegameid, "3_global_ua"))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True


def rev_group_rev_pu(joyplegameid: int, **context):
    query = f"""
    select logdatekst,Week
    , cast(sum(if(rgroup_final = 'R0', pricekrw, null)) as int64)as R0_Sales
    , cast(sum(if(rgroup_final = 'R1', pricekrw, null)) as int64) as R1_Sales
    , cast(sum(if(rgroup_final = 'R2', pricekrw, null)) as int64) as R2_Sales
    , cast(sum(if(rgroup_final = 'R3', pricekrw, null)) as int64) as R3_Sales
    , cast(sum(if(rgroup_final = 'R4', pricekrw, null)) as int64) as R4_Sales
    , cast(sum(if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ', pricekrw, null)) as int64) as `ì „ì›” ë¬´ê³¼ê¸ˆ_Sales`
    , cast(sum(if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì', pricekrw, null)) as int64) as `ë‹¹ì›”ê°€ì…ì_Sales`
    , cast(sum(pricekrw) as int64) as `ì „ì²´ìœ ì €_Sales`

    , count(distinct if(rgroup_final = 'R0' and pricekrw>0 , authaccountname, null)) as R0_PU
    , count(distinct if(rgroup_final = 'R1' and pricekrw>0 , authaccountname, null)) as R1_PU
    , count(distinct if(rgroup_final = 'R2' and pricekrw>0 , authaccountname, null)) as R2_PU
    , count(distinct if(rgroup_final = 'R3' and pricekrw>0 , authaccountname, null)) as R3_PU
    , count(distinct if(rgroup_final = 'R4' and pricekrw>0 , authaccountname, null)) as R4_PU
    , count(distinct if(rgroup_final = 'ì „ì›” ë¬´ê³¼ê¸ˆ' and pricekrw>0 , authaccountname, null)) as `ì „ì›” ë¬´ê³¼ê¸ˆ_PU`
    , count(distinct if(rgroup_final = 'ë‹¹ì›”ê°€ì…ì' and pricekrw>0 , authaccountname, null)) as `ë‹¹ì›”ê°€ì…ì_PU`
    , count(distinct if(pricekrw>0, authaccountname, null)) as `ì „ì²´ìœ ì €_PU`
    from
    (select *, concat(cast(cast(DATE_TRUNC(logdatekst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdatekst,week(Wednesday)) as date), interval 6 day) as string)) as Week
    from `data-science-division-216308.gameInsightFramework.paymentGroup`
    where logdatekst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdatekst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH))
    and joypleGameID = {joyplegameid}
    group by 1,2
    order by 1
    """

    query_result =query_run_method('4_detail_sales', query)
    context['task_instance'].xcom_push(key='rev_group_rev_pu', value=query_result)

    return True


def rev_group_rev_pu_gemini(joyplegameid: int, service_sub: str, **context):
    rev_group_rev_pu_data = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_RgroupSales = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    1. ì´ë²ˆì£¼ ë§¤ì¶œê³¼ PU íŠ¸ë Œë“œì— ëŒ€í•´, ì§€ë‚œì£¼ì™€ ì§€ì§€ë‚œì£¼ì™€ ë¹„êµí•´ì„œ íŠ¹ë³„í•œ ì ì´ ìˆë‹¤ë©´ ì•Œë ¤ì¤˜.
    ì´í•©ì´ë‚˜ í‰ê· ì ì¸ê²ƒ ë§ê³ ë„ íŠ¸ë Œë“œ ë³€í™”ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜
    2. ì¡´ëŒ“ë§ë¡œ ì¨ì¤˜. 10ì¤„ ë‚´ë¡œ ì¨ì¤˜
    3. ë¶„ì„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ í˜¹ì€ ë¶„ì„í•´ë³´ì•˜ìŠµë‹ˆë‹¤ ë“±ì˜ ë§ì„ ì“°ì§€ë§ê³  ë°”ë¡œ ë¶„ì„í•œ ë‚´ìš©ì— ëŒ€í•´ ì•Œë ¤ì¤˜.
    ì œê³µí•´ì£¼ì‹  ë°ì´í„°ëŠ” ì´ëŸ° ë§ ì“°ì§€ë§ì•„ì¤˜ ë°”ë¡œ ë¶„ì„ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜
    4. ë¹„êµí• ë•Œ, ì´ë²ˆì£¼ê°€ ë‹¤ ì§€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì£¼ì°¨ë„ ë™ì¼í•œ ì¼ìˆ˜ë¥¼ ê°€ì§€ê³  ë¹„êµí•´ì£¼ê³ , ì–´ë–»ê²Œ ë™ì¼ê¸°ê°„ ë¹„êµë˜ì—ˆëŠ”ì§€ë„ ëª…ì‹œí•´ì¤˜.
    5. ë§¤ì¶œì€ ì¼ìë³„ ì´í•©ìœ¼ë¡œ ë¹„êµí•´ë„ ë˜ì§€ë§Œ, PU ëŠ” ê·¸ë‚ ì˜ PU ì´ê¸° ë•Œë¬¸ì— ì´í•©ë³´ë‹¤ëŠ” íŠ¸ë Œë“œë¡œ ë¹„êµí•´ì¤˜.
    6. ë¹„êµí•  ë•ŒëŠ” R0, R1,R2 ê·¸ë£¹ì„ ìœ„ì£¼ë¡œ ë§í•´ì¤˜
    7. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    8. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.e.g : 5100ë§Œì› , 1ì–µ 2ì²œë§Œì›
    9. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    10. í–¥í›„ ì–´ë–»ê²Œ í•´ì•¼ëœë‹¤ëŠ” ë§ì€ í•˜ì§€ ë§ì•„ì¤˜.
    11. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    í•µì‹¬ë‚´ìš©ì€ R0,R1,R2 ì˜ ë§¤ì¶œì´ ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ì§€ê°€ í•„ìš”í•´

    <ì¼ìë³„ ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œì•¡>
    {rev_group_rev_pu_data}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_RgroupSales.text


def iap_gem_ruby(joyplegameid:int, databaseschema: str='GW', **context):
    query = f"""
    select logdate_kst,week
    , cast(sum(if(cat_package2='ì „íˆ¬ê¸°', sales_buygem, 0)) as int64) as `ì „íˆ¬ê¸°`
    , cast(sum(if(cat_package2='ì¢…í•©', sales_buygem, 0)) as int64) as `ì¢…í•©`
    , cast(sum(if(cat_package2='ìì›', sales_buygem, 0)) as int64) as `ìì›`
    , cast(sum(if(cat_package2='í•­ê³µëª¨í•¨', sales_buygem, 0)) as int64) as `í•­ê³µëª¨í•¨`
    , cast(sum(if(cat_package2='ì˜ì›…', sales_buygem, 0)) as int64) as `ì˜ì›…`
    , cast(sum(if(cat_package2='êµ°í•¨', sales_buygem, 0)) as int64) as `êµ°í•¨`
    , cast(sum(if(cat_package2='ë°°í‹€íŒ¨ìŠ¤', sales_buygem, 0)) as int64) as `ë°°í‹€íŒ¨ìŠ¤`
    , cast(sum(if(cat_package2='ì—°êµ¬', sales_buygem, 0)) as int64) as `ì—°êµ¬`
    , cast(sum(if(cat_package2='ì¥ë¹„', sales_buygem, 0)) as int64) as `ì¥ë¹„`
    , cast(sum(if(cat_package2 not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„'), sales_buygem, null)) as int64) as `ê¸°íƒ€`
    from
    (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (
    ### IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        )
    union all
    ### GEM
        (
        select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        )
    union all
    ### RUBY
        (
        select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')
        )
    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1,2 order by 1
    """

    query_result =query_run_method('4_detail_sales', query)
    context['task_instance'].xcom_push(key='iap_gem_ruby', value=query_result)

    return True


def iap_gem_ruby_history(gameidx: str, **context):
    query = f"""
    select *
    from (
        select distinct updateDate `ì—…ë°ì´íŠ¸ì¼`
                        , case when category is null then 'ê¸°íƒ€'
                                when category = 'ì´ë²¤íŠ¸ (ìš´ì˜íˆ´)' then 'ì´ë²¤íŠ¸'
                                else category end as `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜`
                        , title as `ì—…ë°ì´íŠ¸ ë‚´ìš©`
        from `data-science-division-216308.gameInsightFramework.{gameidx}_history`
        where date(updateDate)>= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 32 DAY)
        and date(updateDate)<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        and (title is not null or title != '' or title != ' ')
        )
    where `ì—…ë°ì´íŠ¸ í•­ëª© ë¶„ë¥˜` not in ('ê¸°íƒ€', 'ì ê²€ ê¸°ë³¸ ì •ë³´', 'LQA', 'ë²„ê·¸ ìˆ˜ì • ë° ì‚¬ìš©ì„± ê°œì„ ', 'BM_ìƒì ')
    order by `ì—…ë°ì´íŠ¸ì¼`desc
    """

    query_result =query_run_method('4_detail_sales', query)
    # 1ì£¼ì „ ìˆ˜ìš”ì¼ë¶€í„° ì–´ì œì¼ìê¹Œì§€ì˜ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ (ì¿¼ë¦¬ì—ì„œ ì „ì²˜ë¦¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”í›„ ìˆ˜ì • í•„ìš”)

    # ë¬¸ìì—´ ì»¬ëŸ¼ -> datetimeìœ¼ë¡œ íŒŒì‹± (ì‹¤íŒ¨í•œ ê°’ì€ NaT)
    s = pd.to_datetime(query_result['ì—…ë°ì´íŠ¸ì¼'], errors='coerce')

    # í•œêµ­ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜/ì „ì¼
    today = pd.Timestamp.now(tz='Asia/Seoul').normalize().date()
    yesterday = today - pd.Timedelta(days=1)

    # ì˜¤ëŠ˜ ê¸°ì¤€ 'ì§ì „ ìˆ˜ìš”ì¼'(ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ ì˜¤ëŠ˜ ì œì™¸) ê³„ì‚°
    # ì›”=0, í™”=1, ìˆ˜=2, ... ì¼=6
    w = today.weekday()
    delta_to_last_wed = (w - 2) % 7
    if delta_to_last_wed == 0:  # ì˜¤ëŠ˜ì´ ìˆ˜ìš”ì¼ì´ë©´ 7ì¼ ì „ì„ 'ì§ì „ ìˆ˜ìš”ì¼'ë¡œ
        delta_to_last_wed = 7
    last_wed = today - pd.Timedelta(days=delta_to_last_wed)

    # ğŸ‘‰ "1ì£¼ ì „ ìˆ˜ìš”ì¼"ì„ ì‹œì‘ì¼ë¡œ
    start_date = last_wed

    # êµ¬ê°„: [1ì£¼ ì „ ìˆ˜ìš”ì¼, ì „ì¼] (ì–‘ë í¬í•¨)
    mask = (s.dt.date >= start_date) & (s.dt.date <= yesterday)
    query_result4_ingameHistory = query_result.loc[mask].copy()

    context['task_instance'].xcom_push(key='iap_gem_ruby_history', value=query_result4_ingameHistory)

    return True


def iap_gem_ruby_gemini(service_sub: str, **context):
    
    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
    )

    query_result4_ingameHistory = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_salesByPackage = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{query_result4_salesByPackage.to_csv(index=False)}

    ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {query_result4_ingameHistory.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage.text


def iap_df(joyplegameid: int, databaseschema: str='GW', **context):
    # IAP
    query = f"""
    WITH base AS (
    SELECT *
    FROM `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    WHERE logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(pricekrw) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (
            SELECT cat_package, sum(rev) AS peak_rev
            FROM daily
            GROUP BY 1
        )
    ORDER BY peak_rev DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
        d.logdate_kst,
        IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4

    """

    query_result =query_run_method('4_detail_sales', query)
    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ Pivot

    query_result4_salesByPackage_IAP = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='iap_df', value=query_result4_salesByPackage_IAP)

    return True


def gem_df(joyplegameid: int, **context):
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='gem' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS usegem
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(usegem) AS peak_usegem
        FROM daily
        GROUP BY 1)
    ORDER BY peak_usegem DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.usegem) AS usegem
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByPackage_GEM = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="usegem",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='gem_df', value=query_result4_salesByPackage_GEM)

    return True


def ruby_df(joyplegameid: int, **context):
    
    query = f"""
    WITH base AS (
    SELECT * EXCEPT(package_name, cat_shop, cat_package)
    , CASE WHEN action_category_name = 'payment' THEN package_name ELSE action_name END AS package_name
    , CASE WHEN action_category_name = 'payment' THEN cat_shop ELSE 'contents' END AS cat_shop
    , CASE WHEN action_category_name = 'payment' THEN cat_package ELSE 'contents' END AS cat_package
    FROM `data-science-division-216308.gameInsightFramework.sales_goods`
    WHERE is_tester=0
    AND joyple_game_code = {joyplegameid}
    AND goods_name='ruby' and add_or_spend = 'spend'
    AND logdate_kst >= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    AND logdate_kst <= LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package, SUM(usegem) AS useruby
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package
    FROM
        (SELECT cat_package, sum(useruby) AS peak_useruby
        FROM daily
        GROUP BY 1)
    ORDER BY peak_useruby DESC, cat_package ASC
    LIMIT 15
    )

    SELECT format_date('%Y-%m', d.logdate_kst) as month,
        concat(cast(cast(DATE_TRUNC(d.logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(d.logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week,
    d.logdate_kst,
    IF(d.cat_package IN (SELECT cat_package FROM top_cat), d.cat_package, 'ê¸°íƒ€') AS cat_package_grouped,
    SUM(d.useruby) AS useruby
    FROM daily d
    GROUP BY 1,2,3,4
    ORDER BY 1,2,3,4
    ;

    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByPackage_RUBY = query_result.pivot_table(
        index=["month", "week", "logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="useruby",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    context['task_instance'].xcom_push(key='ruby_df', value=query_result4_salesByPackage_RUBY)

    return True


def iap_df_gemini(service_sub: str, **context):

    iap_df = context['task_instance'].xcom_pull(
        task_ids = 'iap_df',
        key='iap_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_IAP = genai_client.models.generate_content(
    model=MODEL_NAME,

    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œì•¡ì´ì•¼.
    \n{iap_df.to_csv(index=False)}

    ì´ë²ˆì£¼ IAP ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_IAP.text


def gem_df_gemini(service_sub: str, **context):
    gem_df = context['task_instance'].xcom_pull(
        task_ids = 'gem_df',
        key='gem_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_GEM = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ì ¬ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{gem_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ì ¬ìœ¼ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ì ¬ ì†Œëª¨ëŸ‰ì— ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ì ¬ì†Œë¹„ëŸ‰ì„ ì²«ë²ˆì§¸ ìë¦¬ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ì ¬ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ì ¬ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_GEM.text


def ruby_df_gemini(service_sub: str, **context):
    ruby_df = context['task_instance'].xcom_pull(
        task_ids = 'ruby_df',
        key='ruby_df'
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}
    
    response4_salesByPackage_RUBY = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    ë‹¤ìŒì€ ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ë³„ ë£¨ë¹„ ì†Œëª¨ëŸ‰ì´ì•¼.
    \n{ruby_df.to_csv(index=False)}

    ì´ë²ˆì£¼ ë£¨ë¹„ë¡œ êµ¬ë§¤í•œ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë£¨ë¹„ ì†Œëª¨ëŸ‰ ëŒ€í•´ì„œ íŠ¹ë³„í•œ ì ì„ ì•„ì£¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë§í•´ì¤˜. (15ì¤„ì´ë‚´)
    ì´ë²ˆì£¼ëŠ” ë°ì´í„° "week" ì»¬ëŸ¼ì—ì„œ ê°€ì¥ ìµœê·¼ì„ ë§í•´.
    ë‹¤ìŒì˜ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ê³ , ì—°ê´€ì´ ì—†ë‹¤ë©´ ì—…ë°ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
    ì „ì£¼, ì „ì „ì£¼ì™€ ë¹„êµí•˜ë˜, ë™ì¼ê¸°ê°„ìœ¼ë¡œ ë¹„êµí•´ì¤˜. (ì´ë²ˆì£¼ ë°ì´í„°ê°€ 3ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ ì „ì£¼, ì „ì „ì£¼ë„ 3ì¼ì¹˜ë§Œ ë¹„êµ)
    6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
    ë‹¨ìœ„ëŠ” ì›ì´ ì•„ë‹ˆë¼ ë£¨ë¹„ë¡œ í‘œê¸°í•´ì¤˜.

    < ì„œë‘ì— ì“°ì¼ ë‚´ìš©>
    1. í•µì‹¬ì ì¸ ë‚´ìš© í•œì¤„ì„ ì„œë‘ì— ì¨ì£¼ê³ (Bold ì²˜ë¦¬), ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜.
    2. ì„œë‘ì— í•µì‹¬ë‚´ìš© ì“¸ë•ŒëŠ” ë¨¼ì € ì´ë²ˆì£¼ ê¸°ê°„ê³¼ ì§€ë‚œì£¼, ì§€ì§€ë‚œì£¼ ê¸°ê°„ì„ ì¨ì£¼ê³  ë™ì¼ê¸°ê°„ ë¹„êµ í–ˆë‹¤ê³ ë„ ê°™ì´ ì¨ì¤˜.
    3. ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¨ì´ê°€ ì–´ë–»ê²Œ ë‚¬ë‹¤ê³  ì„œë‘ì— ì¨ì¤˜.



    <ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬>
    {iap_gem_ruby_history.to_csv(index=False)}


    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ í•œìë¦¬ ìˆ˜ ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë£¨ë¹„ ì†Œë¹„ëŸ‰ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë£¨ë¹„ ì†Œë¹„ëŸ‰ì„ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ

    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_salesByPackage_RUBY.text


def weekly_iapcategory_rev(joyplegameid: int, gameidx: str, databaseschema: str, **context):
    
    query = f"""
    with base as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    from
    (

        ## IAP
        (
        select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

        union all

        ## RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend'
        and logdate_kst >= CASE
                    WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                    THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                    ELSE DATE_SUB(
                            CURRENT_DATE("Asia/Seoul"),
                            INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                            )
                    END
        AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
        )
    )

    , daily AS (  -- ì¼ì x ìƒí’ˆêµ° ë§¤ì¶œ
    SELECT logdate_kst, cat_package2, SUM(sales_buygem) AS rev
    FROM base
    GROUP BY 1,2
    ),

    top_cat AS (  -- ë§¤ì¶œ top15 (ë™ë¥  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê²°ì •)
    SELECT cat_package2
    FROM
        (SELECT cat_package2, sum(rev) AS peak_rev
        FROM daily
        GROUP BY 1)
    ORDER BY peak_rev DESC, cat_package2 ASC
    LIMIT 15
    )

    SELECT d.logdate_kst,
        IF(d.cat_package2 IN (SELECT cat_package2 FROM top_cat), d.cat_package2, 'ê¸°íƒ€') AS cat_package_grouped,
        SUM(d.rev) AS rev
    FROM daily d
    GROUP BY 1,2
    ORDER BY 1,2


    """

    query_result =query_run_method('4_detail_sales', query)

    query_result4_salesByCategory = query_result.pivot_table(
        index=["logdate_kst"],  # ë‘ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
        columns="cat_package_grouped",
        values="rev",
        aggfunc="sum",
        fill_value=0
    ).reset_index()

    exclude = {'logdate_kst', 'ê¸°íƒ€'}
    cols = [c for c in query_result4_salesByCategory.columns if c not in exclude]

    # ì‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì•ˆì „ ì²˜ë¦¬
    def sq(c: str) -> str:
        return "'" + c.replace("'", "''") + "'"

    query_result4_salesByCategory_Cols = ", ".join(sq(c) for c in cols)

    context['task_instance'].xcom_push(key='weekly_iapcategory_rev', value=query_result4_salesByCategory)
    context['task_instance'].xcom_push(key='weekly_iapcategory_rev_cols', value=query_result4_salesByCategory_Cols)

    return True


def ruby_df_gemini(service_sub: str, **context):
    weekly_iapcategory_rev = context['task_instance'].xcom_pull(
        task_ids = 'weekly_iapcategory_rev',
        key='weekly_iapcategory_rev'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    response4_salesByCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ì§€ë‚œ ì—…ë°ì´íŠ¸ì¼ë¶€í„° ì „ì¼ìê¹Œì§€ì˜ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ë‹¤ìŒì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•´ì„œ ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ê³ , ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í–ˆëŠ”ì§€ë¥¼ í™•ì¸í•´ì¤˜.\n{weekly_iapcategory_rev.to_csv(index=False)}
    ì–´ë–¤ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¶œì´ ë†’ê²Œ ë‚˜ì™”ëŠ”ì§€ëŠ” ìƒìœ„ 3ê°œë§Œ ì•Œë ¤ì£¼ê³ , ë§¤ì¶œì´ í¬ê²Œ ë³€í™”í•œ ì¹´í…Œê³ ë¦¬ì—ì„œëŠ” ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸í•˜ê³  ì•Œë ¤ì¤˜.
    ì œì–¸ì€ í•˜ì§€ ë§ì•„ì¤˜.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    response4_CategoryListUp = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {response4_salesByCategory.text}\nìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ì—ì„œ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜.
    ('ê³¨ë“œ', 'ì€í™”', 'ì²­ì‚¬ì§„', 'ì „ìˆ  êµë³¸', 'ìì›')
    """,
        config=types.GenerateContentConfig(
            temperature=0
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # evalë¡œ ë¬¸ìì—´ì„ ì‹¤ì œ tupleë¡œ ë³€í™˜
        CategoryListUp_2 = eval(CategoryListUp.group(0))
        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_2])
    # SQL order by ì‹œ, ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•œ ì½”ë“œ
    case_when_str = "\n".join(
        [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_2)]
    )

    return CategoryListUp_SQL, case_when_str, response4_salesByCategory, response4_CategoryListUp



def top3_items_by_category(joyplegameid: int, gameidx: str, databaseschema:str,  service_sub: str, **context):

    weekly_iapcategory_rev_cols = context['task_instance'].xcom_pull(
        task_ids = 'weekly_iapcategory_rev',
        key='weekly_iapcategory_rev_cols'
    )

    CategoryListUp_SQL, case_when_str, _, _ = ruby_df_gemini(service_sub)
    
    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
        "run_id": RUN_ID,
        f"datascience_division_service_sub" : {service_sub}}


    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 3 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 3 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ({weekly_iapcategory_rev_cols}) then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))

                union all

    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend'
                and logdate_kst >= CASE
                            WHEN EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) = 4
                            THEN DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 7 DAY)
                            ELSE DATE_SUB(
                                    CURRENT_DATE("Asia/Seoul"),
                                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM CURRENT_DATE("Asia/Seoul")) - 4 + 7, 7) DAY
                                    )
                            END
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY))
            )
            )
    where cat_package2 in ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    where `ìƒí’ˆ ì´ë¦„` != 'ê·¸ ì™¸ ìƒí’ˆë“¤'
    group by `ì¼ì`, `ìƒí’ˆê²°ì œ ì¬í™”`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc

    """

    query_result=query_run_method('4_detail_sales', query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")

    context['task_instance'].xcom_push(key='top3_items_by_category', value=query_result)

    return True



def top3_items_by_category_gemini(service_sub: str, **context):

    query_result4_salesByPackage_ListedCategory = context['task_instance'].xcom_pull(
        task_ids = 'top3_items_by_category',
        key='top3_items_by_category'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}

    _, _, response4_salesByCategory = ruby_df_gemini(service_sub)

    response4_salesByPackage_ListedCategory = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    {query_result4_salesByPackage_ListedCategory.to_csv(index=False)}
    ìœ„ì˜ ë°ì´í„°ëŠ” ë§¤ì¶œ ìƒìœ„ 3ìœ„ ì¹´í…Œê³ ë¦¬ ë° ë§¤ì¶œ ë³€ë™ì´ ë†’ì•˜ë˜ ì¹´í…Œê³ ë¦¬ë“¤ì˜ ìƒí’ˆë³„ ë§¤ì¶œ ë°ì´í„°ì•¼.
    \n{response4_salesByCategory.text}
    ê·¸ë¦¬ê³  ìœ„ì˜ ë°ì´í„°ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ë³€í™” ìš”ì•½í•œ ë‚´ìš©ì´ì•¼.
    ë‘ ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œ,
    ë§¤ì¶œ ìƒìœ„ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ê° ì¼ìë³„ë¡œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€(ëª¨ë“  ë‚ ì§œë¥¼ ì°¸ì¡°í•´ì¤˜),
    ê·¸ ì™¸ ì¹´í…Œê³ ë¦¬ë“¤ì€ ë§¤ì¶œ ë³€ë™ì´ í° ë‚ ì§œì—ë§Œ ì–´ë–¤ ìƒí’ˆë“¤ ë•Œë¬¸ì¸ì§€ ë¶„ì„í•´ì¤˜.
    ì œì‹œëœ ë°ì´í„°ë§Œìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ì„œëŠ” ì–¸ê¸‰í•˜ì§€ ë§ì•„ì¤˜.
    ì£¼ì–´ì§„ ë°ì´í„°ê°€ í•˜ë£¨ë§Œ ìˆë‹¤ëŠ” ìœ ì˜ì‚¬í•­ì€ ë§í•˜ì§€ë§ˆ.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.1
            ,labels=LABELS
            # max_output_tokens=9000
        )
    )

    response4_WeeklySales_Draft1 = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
        ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ì§€í‘œì— ëŒ€í•œ ìš”ì•½ê¸€ì´ì•¼.\n{response4_salesByCategory.text}
        ê·¸ë¦¬ê³  ë‹¤ìŒì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¶œì— ê¸°ì—¬í•œ ìƒí’ˆì— ëŒ€í•œ ì •ë³´ê¸€ì´ì•¼.\n{response4_salesByPackage_ListedCategory.text}
        ë‘ ê¸€ì„ ì¢…í•©í•´ì„œ, ê¸°ê°„ë™ì•ˆ ê²Œì„ì˜ ë§¤ì¶œ ë³€í™”ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.
        ë¦¬í¬íŠ¸ ì‘ì„±ì‹œ ë°˜ë“œì‹œ ì•„ë˜ì˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.

        ì„œë‘ì—ëŠ” 'ê¸ˆì£¼ ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸ ë¦¬í¬íŠ¸ (ë°ì´í„° ê¸°ê°„)'ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. Bold ì²˜ë¦¬í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬(ë§¤ì¶œ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì¸ ê²½ìš°, ìˆœìœ„ ì–¸ê¸‰. e.g. ì „íˆ¬ê¸° (ë§¤ì¶œ 1ìœ„). Bold ì²˜ë¦¬í•´ì¤˜.)
        * ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”. ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. í° ë³€í™”ê°€ ìˆê±°ë‚˜ ë§¤ì¶œì´ ë†’ì•˜ë˜ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        * í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ, ì¼ìì™€ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜. ë¬¸ë‹¨ ì œëª©ì€ ì‘ì„±í•˜ì§€ ë§ì•„ì¤˜. ì£¼ìš” ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í•´ì¤˜.
        'ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë³€í™”'ì™€ 'í•´ë‹¹ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ì£¼ìš” ìƒí’ˆ ë§¤ì¶œ'ì€ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
        ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ 6ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.

        'ì ¬'ê³¼ 'ë£¨ë¹„'ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì•¼. ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì·¨ê¸‰í•˜ì§€ë§ˆ.
        ìƒí’ˆëª…ì— ìˆëŠ” ë‚ ì§œë¡œ ìƒí’ˆëª…ì˜ ì¶œì‹œëœ ë‚ ì§œë¥¼ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì´ ë§¤ì¶œì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì œì™¸í•œ ì´ë²¤íŠ¸ ë° í”„ë¡œëª¨ì…˜ì€ ì–¸ê¸‰í•˜ì§€ë§ˆ.
        """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.5
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    iap_gem_ruby_history = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby_history',
        key='iap_gem_ruby_history'
    )

    response4_WeeklySales_Report = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents=f"""
    ë‹¤ìŒì€ ê²Œì„ ì—…ë°ì´íŠ¸ì¼ê³¼ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°ì´í„°ì•¼.\n{iap_gem_ruby_history.to_csv(index=False)}
    ë‹¤ìŒì˜ ì§€ë‚œ ì—…ë°ì´íŠ¸ ì´í›„ ë§¤ì¶œ ë¶„ì„ ë¦¬í¬íŠ¸ì—ì„œ, ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì´ ìˆëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë‚˜ ìƒí’ˆì´ ìˆë‹¤ë©´ í•´ë‹¹ ì—…ë°ì´íŠ¸ ë‚´ìš©ê³¼ ì—°ê´€ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ì–¸ê¸‰í•´ì¤˜.\n{response4_WeeklySales_Draft1.text}
    ì£¼ì–´ì§„ ë¶„ì„ ë¦¬í¬íŠ¸ì˜ í˜•ì‹ì— ê° ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì—…ë°ì´íŠ¸ ê´€ë ¨ ë‚´ìš©ë§Œ ì–¸ê¸‰ì„ ì¶”ê°€í•˜ëŠ” ì‹ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜. e.g.*   **ì—…ë°ì´íŠ¸ ì—°ê´€ì„±:**
    ìƒí’ˆëª…ì˜ ë‚ ì§œê°€ ë“¤ì–´ê°€ ìˆë‹¤ëŠ” ì‚¬ì‹¤ë§Œìœ¼ë¡œ ì—…ë°ì´íŠ¸ì™€ ì—°ê´€ì„±ì´ ìˆë‹¤ê³  ì¶”ë¡ í•˜ì§€ ë§ˆ.
    ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ë§ì€ í•˜ì§€ë§ˆ.
    """,
    config=types.GenerateContentConfig(
        system_instruction=SYSTEM_INSTRUCTION,
        # tools=[RAG],
        temperature=0.1
        ,labels=LABELS
        # max_output_tokens=2048
        )
    )

    return response4_WeeklySales_Report.text


def rgroup_top3_pu(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""
        with raw as (
        select *
        , format_date('%Y-%m',  logdate_kst ) as month
        , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

        , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
        , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
        , DATE_SUB(
                    date_add(current_date('Asia/Seoul'),interval -1 day),
                    INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                    ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
        from
        (
        ### IAP
        (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
        , cast(price_sheet as int64) as price_sheet
        , pricekrw as sales_usegem, pricekrw as sales_buygem
        from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
        where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
        union all
        ### GEM
        (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind
        , cast(price_sheet as int64)*(1500/40) as price_sheet
        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='gem' and add_or_spend = 'spend')
        union all
        ### RUBY
        (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
        , case when action_category_name = 'payment' then package_name else action_name end as package_name
        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
        from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
        and joyple_game_code = {joyplegameid}
        and goods_name='ruby' and add_or_spend = 'spend')

        )
        where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
        and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
        ),

        sales_raw as ( ## 5331039
        select *  , format_date('%Y-%m',  logdatekst ) as month
        from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
        where joyplegameid = {joyplegameid}
        and logdatekst>='2025-01-01'
        ),


        monthly_rev as (
        select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
        from
        (select *
        , format_date('%Y-%m-01',  logdatekst ) as logmonth
        , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
        from sales_raw
        where logdatekst>='2025-01-01')
        group by 1,2,3
        ),

        r_group as (
        select *
        , case
        when rev>=10000000 then 'R0'
        when rev>=1000000  then 'R1'
        when rev>=100000   then 'R2'
        when rev>=10000    then 'R3'
        when rev>=1        then 'R4'
        # when rev=0         then 'nonPU'
        else 'ETC' end as rgroup
        from monthly_rev
        where rev>0
        ),

        raw2 as (
        select a.*,month_key
        , case
        when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
        when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
        else b.rgroup end as rgroup_final

        from
        (select * , format_date('%Y-%m',  week_start ) as month2
        from raw) as a

        left join
        (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
        from r_group
        ) as b

        on a.authaccountname = b.authaccountname
        and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
        ),

        raw3 as (

        select *
        , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
        , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
        from
        (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
        , count(distinct authaccountname) as PU
        , cast(sum(sales_buygem) as int64) as sales
        from raw2
        where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
        and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
        group by 1,2,3,4,5,6)
        )

        select *
        from raw3
        where rgroup_final is not null
        and pu_rank in (1,2,3)
        order by rgroup_final, pu_rank
    """

    query_result = query_run_method('4_detail_sales', query)

    context['task_instance'].xcom_push(key='rgroup_top3_pu', value=query_result)

    return True


def rgroup_top3_rev(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""

    with raw as (
    select *
    , format_date('%Y-%m',  logdate_kst ) as month
    , format_date('%Y-%m',  authaccountregdatekst ) as regmonth

    , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                    cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as week
    , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤' else cat_package end as cat_package2
    , DATE_SUB(
                date_add(current_date('Asia/Seoul'),interval -1 day),
                INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                ) AS week_start ## ê°€ì¥ ìµœê·¼ ì§ì „ ìˆ˜ìš”ì¼
    from
    (
    ### IAP
    (select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst
    , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
    , cast(price_sheet as int64) as price_sheet
    , pricekrw as sales_usegem, pricekrw as sales_buygem
    from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
    where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null))
    union all
    ### GEM
    (select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind
    , cast(price_sheet as int64)*(1500/40) as price_sheet
    , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='gem' and add_or_spend = 'spend')
    union all
    ### RUBY
    (select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst
    , case when action_category_name = 'payment' then package_name else action_name end as package_name
    , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
    , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
    , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
    , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
    from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
    and joyple_game_code = {joyplegameid}
    and goods_name='ruby' and add_or_spend = 'spend')

    )
    where logdate_kst>= DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH), INTERVAL 1 MONTH)
    and logdate_kst<=LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    ),

    sales_raw as ( ## 5331039
    select *  , format_date('%Y-%m',  logdatekst ) as month
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2025-01-01'
    ),


    monthly_rev as (
    select authaccountname, logmonth, regmonth, ifnull(sum(pricekrw),0) as rev
    from
    (select *
    , format_date('%Y-%m-01',  logdatekst ) as logmonth
    , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    from sales_raw
    where logdatekst>='2025-01-01')
    group by 1,2,3
    ),

    r_group as (
    select *
    , case
    when rev>=10000000 then 'R0'
    when rev>=1000000  then 'R1'
    when rev>=100000   then 'R2'
    when rev>=10000    then 'R3'
    when rev>=1        then 'R4'
    # when rev=0         then 'nonPU'
    else 'ETC' end as rgroup
    from monthly_rev
    where rev>0
    ),

    raw2 as (
    select a.*,month_key
    , case
    when a.month2 <= a.regmonth then 'ë‹¹ì›”ê°€ì…ì'
    when b.rgroup is null then 'ì „ì›” ë¬´ê³¼ê¸ˆ'
    else b.rgroup end as rgroup_final

    from
    (select * , format_date('%Y-%m',  week_start ) as month2
    from raw) as a

    left join
    (select * , format_date('%Y-%m', date_add(date(logmonth), interval 1 month )) as month_key
    from r_group
    ) as b

    on a.authaccountname = b.authaccountname
    and a.month2 = b.month_key  ## ì£¼ì°¨ ì‹œì‘ì¼ê³¼ ì¡°ì¸
    ),

    raw3 as (

    select *
    , row_number() OVER (partition by week, rgroup_final ORDER BY PU desc, sales desc  ) AS pu_rank
    , row_number() OVER (partition by week, rgroup_final ORDER BY sales desc, PU desc ) AS sales_rank
    from
    (select week, rgroup_final, package_name, cat_shop as shop_category, cat_package2 as package_category, price_sheet
    , count(distinct authaccountname) as PU
    , cast(sum(sales_buygem) as int64) as sales
    from raw2
    where logdate_kst between week_start and DATE_ADD(week_start, INTERVAL 6 DAY) ## ì´ë²ˆì£¼ í•„í„°(ìˆ˜ìš”ì¼ë¶€í„° í™”ìš”ì¼)
    and sales_buygem>0 ## ìœ ê°€ì ¬ ì‚¬ìš©ë§Œ
    group by 1,2,3,4,5,6)
    )

    select *
    from raw3
    where rgroup_final is not null
    and sales_rank in (1,2,3)
    order by rgroup_final, sales_rank
    """

    query_result = query_run_method('4_detail_sales', query)

    context['task_instance'].xcom_push(key='rgroup_top3_rev', value=query_result)

    return True


def rgroup_top3_gemini(service_sub: str, **context):
    query_result4_thisWeekSalesTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_pu',
        key='rgroup_top3_pu'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            f"datascience_division_service_sub" : {service_sub}}


    response4_thisWeekRgroup = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ê³¼ê¸ˆê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ì „ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ì „ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ì „ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    ì „ì›” ë¬´ê³¼ê¸ˆ : ì „ì›” ë¬´ê³¼ê¸ˆ ìœ ì €
    ë‹¹ì›”ê°€ì…ì : ì´ë²ˆë‹¬ì— ê°€ì…í•œ ìœ ì €

    ì´ë²ˆì£¼ Rê·¸ë£¹ë³„ PU top3 , ë§¤ì¶œ top3 ìƒí’ˆë“¤ ì •ë³´ë¥¼ ì¤„ê²Œ
    ìƒìœ„ ê³¼ê¸ˆê·¸ë£¹ê³¼ í•˜ìœ„ê³¼ê¸ˆê·¸ë£¹ ê°„ì˜ ì°¨ì´ì—ëŒ€í•´ì„œë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜

    < ì„œì‹ ìš”êµ¬ì‚¬í•­ >
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    4. ë§¤ì¶œì•¡ ì˜ í™•ì¸í•´ì¤˜. 1ì–µì¸ë° 10ì–µì´ë¼ê³  ì“°ì§€ë§ˆ


    < Rê·¸ë£¹ë³„ PU top3 ìƒí’ˆ>
    {query_result4_thisWeekPUTop3}


    < Rê·¸ë£¹ë³„ ë§¤ì¶œ top3 ìƒí’ˆ>
    {query_result4_thisWeekSalesTop3}



    """,

    ### ì´ì „ë²„ì „ í”„ë¡¬í”„íŠ¸ ###
    # contents - f"""
    # ì´ë²ˆì£¼ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡ì—ì„œ íŠ¹ë³„í•œ ì ì„ ì•Œë ¤ì¤˜.
    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì£¼ê³  ì—†ìœ¼ë©´ ì•„ì˜ˆ íˆìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë¬´ ì–¸ê¸‰í•˜ì§€ë§ì•„ì¤˜

    # 1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    # 2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.
    # 3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    # <ì¼ìë³„ ìƒí’ˆêµ°ë³„ ë§¤ì¶œì•¡>
    # {query_result4_salesByPackage}
    # <íˆìŠ¤í† ë¦¬>
    # {query_result4_ingameHistory}
    # """

    config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response4_thisWeekRgroup.text



def category_for_bigquery_sql(service_sub:str, **context):

    _, _, _, response4_CategoryListUp = ruby_df_gemini(service_sub, **context)

    CategoryListUp = re.search(r"\(.*\)", response4_CategoryListUp.text)
    if CategoryListUp:
        # ë¬¸ìì—´ì„ ì‹¤ì œ tuple/listë¡œ ë³€í™˜ (ì•ˆì „)
        CategoryListUp_2 = eval(CategoryListUp.group(0))

        # ì• 3ê°œë§Œ ì¶”ì¶œ
        CategoryListUp_Top3 = list(CategoryListUp_2)[:3]

        # SQLìš© ë¬¸ìì—´ë¡œ ë³€í™˜: 'ë°°í‹€íŒ¨ìŠ¤', 'êµ°í•¨', 'ì „íˆ¬ê¸°'
        CategoryListUp_SQL = ", ".join([f"'{c}'" for c in CategoryListUp_Top3])

        # SQL ORDER BYìš© CASE WHEN ... THEN ...
        case_when_str = "\n".join(
            [f"WHEN '{c}' THEN {i+1}" for i, c in enumerate(CategoryListUp_Top3)]
        )
    return CategoryListUp_SQL, case_when_str, CategoryListUp_Top3


def top3_items_rev(joyplegameid:int, gameidx:str, databaseschema:str, service_sub:str, **context):
    
    CategoryListUp_SQL, case_when_str, _ = category_for_bigquery_sql(service_sub=service_sub)

    query = f"""
    with sales_data as (
    select `ì¼ì`
        , case when rnum <= 5 then `ìƒí’ˆê²°ì œ ì¬í™”` else null end as `ìƒí’ˆê²°ì œ ì¬í™”`
        ,`ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , case when rnum <= 5 then `ìƒí’ˆ` else 'ê·¸ ì™¸ ìƒí’ˆë“¤' end as `ìƒí’ˆ ì´ë¦„`
        , `ë§¤ì¶œ`
    from (
        select logdate_kst as `ì¼ì`
        , idx as `ìƒí’ˆê²°ì œ ì¬í™”`
        , cat_package2 as `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
        , package_name as `ìƒí’ˆ`
        , sum(sales_buygem) as `ë§¤ì¶œ`
        , row_number() over(partition by cat_package2, logdate_kst order by sum(sales_buygem) desc) as rnum
        from(
            select *
            , format_date('%Y-%m',  logdate_kst ) as month

            , case when CountryCode = 'KR' then '1.KR' when CountryCode = 'US' then '2.US' else '3.ETC' end as CountryCat
            , concat(cast(cast(DATE_TRUNC(logdate_kst ,week(Wednesday)) as date) as string),' ~ ',
                        cast(date_add(cast(DATE_TRUNC(logdate_kst,week(Wednesday)) as date), interval 6 day) as string)) as logweek
            , case when cat_shop = 'ë°°í‹€íŒ¨ìŠ¤' then 'ë°°í‹€íŒ¨ìŠ¤'
                when cat_package not in ('ì „íˆ¬ê¸°','ì¢…í•©','ìì›','í•­ê³µëª¨í•¨','ì˜ì›…','êµ°í•¨','ë£¨ë¹„','ë°°í‹€íŒ¨ìŠ¤','ì—°êµ¬','ì¥ë¹„') then 'ê¸°íƒ€'
                        else cat_package end as cat_package2
            from (
                (
                    select 'IAP' As idx, logdate_kst, datetime(logtime_kst) as logtime_kst, authaccountname, authaccountregdatekst, CountryCode
                        , package_name, cat_shop, cat_package, cast(package_kind as string) as package_kind
                        , cast(price_sheet as int64) as price_sheet
                        , pricekrw as sales_usegem, pricekrw as sales_buygem
                from `data-science-division-216308.{databaseschema}.Sales_iap_hub`
                where (cat_package not in ('ì ¬','ë£¨ë¹„') or cat_package is null)
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'GEM' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind
                        , cast(price_sheet as int64)*(1500/40) as price_sheet
                        , (usegem*(1500/40)) as sales_usegem, (buygem*(1500/40)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='gem' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )

                union all

                (
                select 'RUBY' as idx, logdate_kst, datetime(logtime_kst) as logtime_kst, auth_account_name, authaccountregdatekst, CountryCode
                        , case when action_category_name = 'payment' then package_name else action_name end as package_name
                        , case when action_category_name = 'payment' then cat_shop else 'contents' end as cat_shop
                        , case when action_category_name = 'payment' then cat_package else 'contents' end as cat_package
                        , package_kind, cast(price_sheet as int64)*(15000/999) as price_sheet
                        , (usegem*(15000/999)) as sales_usegem, (buygem*(15000/999)) as sales_buygem
                from `data-science-division-216308.gameInsightFramework.sales_goods`  where is_tester=0
                and joyple_game_code = {joyplegameid}
                and goods_name='ruby' and add_or_spend = 'spend'
                        and logdate_kst >= DATE_SUB(
                                        date_add(current_date('Asia/Seoul'),interval -1 day),
                                        INTERVAL MOD(EXTRACT(DAYOFWEEK FROM date_add(current_date('Asia/Seoul'),interval -1 day)) - 4 + 7, 7) DAY
                                        )
                AND logdate_kst <= DATE_SUB(CURRENT_DATE("Asia/Seoul"), INTERVAL 1 DAY)
                )
            )
            )
    where cat_package2 in  ({CategoryListUp_SQL})
        group by 1,2,3,4
        )
    )

    select `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`, sum(`ë§¤ì¶œ`) as `ë§¤ì¶œ`
    from sales_data
    group by `ì¼ì`, `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`, `ìƒí’ˆ ì´ë¦„`
    order by `ì¼ì`,
            CASE `ìƒí’ˆ ì¹´í…Œê³ ë¦¬`
            {case_when_str}
            ELSE 99
            END,
            case when `ìƒí’ˆ ì´ë¦„` = 'ê·¸ ì™¸ ìƒí’ˆë“¤' then 1 else 0 end,
            `ë§¤ì¶œ` desc
    """
    query_result = query_run_method('4_detail_sales', query)
    query_result['ë§¤ì¶œ'] = query_result['ë§¤ì¶œ'].map(lambda x: f"{int(x)}")
    
    context['task_instance'].xcom_push(key='top3_items_rev', value=query_result)


    cats = [re.sub(r"^[\"'â€™â€˜`]+|[\"'â€™â€˜`]+$", "", t.strip())
        for t in CategoryListUp_SQL.split(",") if t.strip()]

    # 2) ìˆœì„œëŒ€ë¡œ í•„í„°ë§í•´ì„œ ìƒˆ DF ìƒì„±
    category_col = "ìƒí’ˆ ì¹´í…Œê³ ë¦¬"

    dfs = {}  # ì‚¬ì „ìœ¼ë¡œ ë³´ê´€: {"query_result4_salesByPackage_forGraph_1": df1, ...}
    for i, c in enumerate(cats, start=1):
        key = f"query_result4_salesByPackage_forCategoryGraph_{i}"
        dfs[key] = query_result[
            query_result[category_col] == c
        ].copy()

    return dfs


def rgroup_rev_draw(gameidx: str, **context):
    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )
    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales.iloc[:, [0,2,3,4,5,6,7,8]]

    ##
    query_result4_RgroupSales2_salesGraph = query_result4_RgroupSales2_salesGraph.rename(
        columns = {"R0_Sales" : "R0",
                "R1_Sales" : "R1",
                "R2_Sales" : "R2",
                "R3_Sales" : "R3",
                "R4_Sales" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_Sales" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_Sales" : "ë‹¹ì›”ê°€ì…ì"}
    )


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_salesGraph["logdatekst"]
    y = query_result4_RgroupSales2_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" Rê·¸ë£¹ë³„ ë§¤ì¶œ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_salesGraph = "graph4_RgroupSales_salesGraph.png"
    
    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    plt.savefig(file_path4_RgroupSales_salesGraph, dpi=160, bbox_inches='tight') # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_salesGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_salesGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_salesGraph)

    return f'{gameidx}/{file_path4_RgroupSales_salesGraph}'


def rgroup_pu_draw(gameidx: str, **context):
    
    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    ## í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë§¤ì¶œ, PU ë‘˜ë‹¤ ìˆì–´ì„œ, ë§¤ì¶œê¹Œì§€ë§Œ í•„í„°ë§
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales.iloc[:, [0,10,11,12,13,14,15,16]]

    ##
    query_result4_RgroupSales2_puGraph = query_result4_RgroupSales2_puGraph.rename(
        columns = {"R0_PU" : "R0",
                "R1_PU" : "R1",
                "R2_PU" : "R2",
                "R3_PU" : "R3",
                "R4_PU" : "R4",
                "ì „ì›” ë¬´ê³¼ê¸ˆ_PU" : "ì „ì›” ë¬´ê³¼ê¸ˆ",
                "ë‹¹ì›”ê°€ì…ì_PU" : "ë‹¹ì›”ê°€ì…ì"}
    )

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(12, 6))

    x = query_result4_RgroupSales2_puGraph["logdatekst"]
    y = query_result4_RgroupSales2_puGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_RgroupSales2_puGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title(" (Rê·¸ë£¹ë³„ PU ìˆ˜ ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_RgroupSales_puGraph = "graph4_RgroupSales_puGraph.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_RgroupSales_puGraph, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_RgroupSales_puGraph}')
    blob.upload_from_filename(file_path4_RgroupSales_puGraph)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_RgroupSales_puGraph)

    return f'{gameidx}/{file_path4_RgroupSales_puGraph}'


def merge_rgroup_graph(gameidx: str):
    p1 = rgroup_rev_draw(gameidx)
    p2 = rgroup_pu_draw(gameidx)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA")


    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph4_RgroupSales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path



def iap_gem_ruby_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
        )

    query_result4_salesByPackage_salesGraph = query_result4_salesByPackage.iloc[:, [0,2,3,4,5,6,7,8,9,10,11]]

    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP+ìœ ê°€ì ¬+ìœ ê°€ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage = "graph4_salesByPackage.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage}')
    blob.upload_from_filename(file_path4_salesByPackage)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage)

    return f'{gameidx}/{file_path4_salesByPackage}'



def iap_gem_ruby_IAP_graph_draw(gameidx:str, **context):
    
    query_result4_salesByPackage_IAP = context['task_instance'].xcom_pull(
    task_ids = 'iap_df',
    key='iap_df'
    )

    # ... (ìœ„ ë°ì´í„° ì¤€ë¹„Â·í°íŠ¸ ë¶€ë¶„ ë™ì¼)
    query_result4_salesByPackage_IAP_salesGraph = query_result4_salesByPackage_IAP.iloc[:, (query_result4_salesByPackage_IAP.columns != 'month') & (query_result4_salesByPackage_IAP.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_IAP_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_IAP_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_IAP_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(IAP) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_IAP = "graph4_salesByPackage_IAP.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_IAP, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_IAP}')
    blob.upload_from_filename(file_path4_salesByPackage_IAP)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_IAP)

    return f'{gameidx}/{file_path4_salesByPackage_IAP}'


def iap_gem_ruby_GEM_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage_GEM = context['task_instance'].xcom_pull(
    task_ids = 'gem_df',
    key='gem_df'
    )
    
    # ... (ìœ„ ë°ì´í„° ì¤€ë¹„Â·í°íŠ¸ ë¶€ë¶„ ë™ì¼)
    query_result4_salesByPackage_GEM_salesGraph = query_result4_salesByPackage_GEM.iloc[:, (query_result4_salesByPackage_GEM.columns != 'month') & (query_result4_salesByPackage_GEM.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_GEM_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_GEM_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_GEM_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ì ¬) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_GEM = "graph4_salesByPackage_GEM.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_GEM, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_GEM}')
    blob.upload_from_filename(file_path4_salesByPackage_GEM)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_GEM)

    return f'{gameidx}/{file_path4_salesByPackage_GEM}'
    


def iap_gem_ruby_RUBY_graph_draw(gameidx:str, **context):

    query_result4_salesByPackage_RUBY = context['task_instance'].xcom_pull(
    task_ids = 'ruby_df',
    key='ruby_df'
    )

    query_result4_salesByPackage_RUBY_salesGraph = query_result4_salesByPackage_RUBY.iloc[:, (query_result4_salesByPackage_RUBY.columns != 'month') & (query_result4_salesByPackage_RUBY.columns != 'week')]


    # â¬‡ï¸ ê°€ë¡œí­ ë„“íˆê¸°: width=20ì¸ì¹˜(ì›í•˜ëŠ” ë§Œí¼ í‚¤ìš°ì„¸ìš”), height=6ì¸ì¹˜
    fig, ax = plt.subplots(figsize=(20, 6))

    x = query_result4_salesByPackage_RUBY_salesGraph["logdate_kst"]
    y = query_result4_salesByPackage_RUBY_salesGraph.iloc[:, 1:]

    # ëˆ„ì  ë§‰ëŒ€ bottomì€ ë„˜íŒŒì´ë¡œ (ë¦¬ìŠ¤íŠ¸ + ì‹œë¦¬ì¦ˆ ë”í•˜ê¸° ì˜¤ë¥˜ ë°©ì§€)
    bottom = np.zeros(len(query_result4_salesByPackage_RUBY_salesGraph), dtype=float)

    for col in y.columns:
        ax.bar(x, y[col], bottom=bottom, label=col)
        bottom += y[col].to_numpy()

    # yì¶• ì²œë‹¨ìœ„
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))

    # ì—¬ë°± ì œê±°
    ax.margins(x=0)

    # xì¶• ë§¤ì¼
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # xì¶• ë¼ë²¨/ëˆˆê¸ˆ
    ax.set_title("(ë£¨ë¹„) ì¼ìë³„ ìƒí’ˆë³„ ë§¤ì¶œ")
    ax.tick_params(axis='x', labelsize=9, pad=2)
    plt.xticks(rotation=90)

    # ë²”ë¡€ë¥¼ ë°–ìœ¼ë¡œ, ì˜ë¦¼ ë°©ì§€
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    ax.grid(axis="y", linestyle="--", alpha=0.7)

    fig.tight_layout()
    file_path4_salesByPackage_RUBY = "graph4_salesByPackage_RUBY.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_salesByPackage_RUBY, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_salesByPackage_RUBY}')
    blob.upload_from_filename(file_path4_salesByPackage_RUBY)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_salesByPackage_RUBY)

    return f'{gameidx}/{file_path4_salesByPackage_RUBY}'


### 1ìœ„
def top1_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category1 = "graph4_salesByPackage_Category1.png"

    # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category1, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category1}')
    blob.upload_from_filename(salesByPackage_Category1)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category1)

    return f'{gameidx}/{salesByPackage_Category1}'


### 2ìœ„
def top2_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category2 = "graph4_salesByPackage_Category2.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category2, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category2}')
    blob.upload_from_filename(salesByPackage_Category2)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category2)

    return f'{gameidx}/{salesByPackage_Category2}'



### 3ìœ„
def top3_graph_draw(joyplegameid: int, gameidx: str, databaseschema: str, service_sub: str, **context):

    dfs = top3_items_rev(joyplegameid, gameidx, databaseschema, service_sub, **context)

    df = dfs.get("query_result4_salesByPackage_forCategoryGraph_1")
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"])
    df["ë§¤ì¶œ"] = pd.to_numeric(df["ë§¤ì¶œ"], errors="coerce").fillna(0).astype("int64")

    # 3) ì§‘ê³„ â†’ í”¼ë²—
    g = df.groupby(["ì¼ì", "ìƒí’ˆ ì´ë¦„"], as_index=False)["ë§¤ì¶œ"].sum()
    wide = g.pivot(index="ì¼ì", columns="ìƒí’ˆ ì´ë¦„", values="ë§¤ì¶œ").fillna(0)

    # 4) ìƒìœ„ Nê°œ(ì›ë¬¸ëŒ€ë¡œ Noneì´ë©´ ì „ì²´)
    top_n = None
    if top_n is not None:
        top_items = wide.sum(axis=0).sort_values(ascending=False).head(top_n).index
        wide = wide[top_items]

    # --- ë°©ë²• 1) ì ìš©: ì»¬ëŸ¼ëª… ì •ê·œí™” + ìƒ‰ìƒ ë§¤í•‘ ê³ ì • -----------------------

    def norm_label(s: str) -> str:
        # ì–‘ë ê³µë°±/ì—¬ëŸ¬ í˜•íƒœì˜ ë”°ì˜´í‘œ ì œê±°
        return re.sub(r'^[\'"\s`â€™â€˜]+|[\'"\s`â€™â€˜]+$', '', str(s).strip())

    # ì»¬ëŸ¼ ì •ê·œí™”
    cols_norm = [norm_label(c) for c in wide.columns]
    wide.columns = cols_norm

    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ êµ¬ì„±
    n = len(cols_norm)
    cmap = plt.get_cmap('tab20', n) if n > 0 else None
    color_map = {col: cmap(i) for i, col in enumerate(cols_norm)} if n > 0 else {}

    # â€˜ê·¸ ì™¸ ìƒí’ˆë“¤â€™ì„ ë°ì€ íšŒìƒ‰ìœ¼ë¡œ ê°•ì œ
    DARK_GRAY = "#525252"
    color_map["ê·¸ ì™¸ ìƒí’ˆë“¤"] = DARK_GRAY

    # ----------------------------------------------------------------------

    # 5) ëˆ„ì  ë§‰ëŒ€
    fig, ax = plt.subplots(figsize=(20, 6))
    x = wide.index
    bottom = np.zeros(len(x), dtype=float)

    for col in wide.columns:
        vals = wide[col].to_numpy()
        ax.bar(x, vals, bottom=bottom, color=color_map.get(col), label=col)
        bottom += vals

    # 6) í¬ë§·íŒ…
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, _: f"{int(v):,}"))
    ax.margins(x=0)
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=90)
    ax.grid(axis="y", linestyle="--", alpha=0.7)

    # ì œëª© (ì¹´í…Œê³ ë¦¬ëª… ë°˜ì˜)
    _, _, CategoryListUp_Top3 = category_for_bigquery_sql(service_sub=service_sub)

    title_cat = str(CategoryListUp_Top3[0]).strip().strip("'\"`â€™â€˜") if CategoryListUp_Top3 else "" # CategoryListUp_Top3[] ë¶€ë¶„ ìˆ˜ì •
    ax.set_title(f"{title_cat} ì¼ìë³„ {'ìƒìœ„'+str(top_n)+'ê°œ ' if top_n else ''}ìƒí’ˆ ë§¤ì¶œ")

    # 7) ë²”ë¡€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq_h, uniq_l = [], []
    for h, l in zip(handles, labels):
        if l and l not in seen and not l.startswith("_"):
            uniq_h.append(h); uniq_l.append(l); seen.add(l)

    if uniq_l:
        ax.legend(uniq_h, uniq_l, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)

    fig.tight_layout()
    salesByPackage_Category3 = "graph4_salesByPackage_Category3.png"

        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(salesByPackage_Category3, dpi=160, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{salesByPackage_Category3}')
    blob.upload_from_filename(salesByPackage_Category3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(salesByPackage_Category3)

    return f'{gameidx}/{salesByPackage_Category3}'


def rgroup_pu_top3_graph_draw(gameidx:str, **context):

    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_pu',
        key='rgroup_top3_pu'
    )

    df = query_result4_thisWeekPUTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekPUTop3 = "graph4_thisWeekPUTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekPUTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekPUTop3}')
    blob.upload_from_filename(file_path4_thisWeekPUTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekPUTop3)

    return f'{gameidx}/{file_path4_thisWeekPUTop3}'


def rgroup_rev_top3_graph_draw(gameidx:str, **context):

    query_result4_thisWeekRevTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    df = query_result4_thisWeekRevTop3.iloc[:, [1,2,3,4,5,6,8]]
    df = df.rename(
        columns = {"rgroup_final" : "Rê·¸ë£¹",
                "pu_rank" : "ìˆœìœ„",
                "package_name" : "ìƒí’ˆëª…",
                "shop_category" : "ìƒì  ì¹´í…Œê³ ë¦¬",
                "package_category" : "ìƒí’ˆ ì¹´í…Œê³ ë¦¬",
                "price_sheet" : "ìƒí’ˆ ê°€ê²©",
                "PU" : "PU ìˆ˜"}
    )
    # ì›í•˜ëŠ” ìˆœì„œ ì§€ì •
    new_order = ["Rê·¸ë£¹", "ìˆœìœ„","ìƒí’ˆëª…", "ìƒì  ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ì¹´í…Œê³ ë¦¬", "ìƒí’ˆ ê°€ê²©", "PU ìˆ˜"]

    # df ì¬ì •ë ¬
    df = df[new_order]

    # ìˆ«ì í¬ë§·
    df["ìƒí’ˆ ê°€ê²©"] = df["ìƒí’ˆ ê°€ê²©"].map(
        lambda x: f"{int(x):,}" if pd.notna(x) else x
    )

    df["PU ìˆ˜"] = df["PU ìˆ˜"].map(lambda x: f"{int(x):,}")

    # ---------- í­ ê³„ì‚°: ìƒí’ˆëª… ë„“ê²Œ, ì •ê·œí™”ëŠ” í•˜ë˜ ìƒí’ˆëª… ê°€ì¤‘ì¹˜ í¬ê²Œ ----------
    cols = df.columns.tolist()
    col_idx_map = {c: i for i, c in enumerate(cols)}

    # ê° ì—´ ìµœëŒ€ ê¸€ììˆ˜(í—¤ë”/ë°ì´í„° í¬í•¨)
    max_lens = []
    for c in cols:
        head_len = len(str(c))
        body_len = max(len(str(v)) for v in df[c]) if len(df) else 0
        max_lens.append(max(head_len, body_len))

    base_w, k = 0.03, 0.035
    widths = base_w + k * np.log1p(np.array(max_lens))

    # âœ… ìƒí’ˆëª… ì—´ ê°€ì¤‘ì¹˜ í¬ê²Œ (ì˜ë¦¼ ë°©ì§€)
    if "ìƒí’ˆëª…" in col_idx_map:
        widths[col_idx_map["ìƒí’ˆëª…"]] *= 2.2   # í•„ìš”í•˜ë©´ 2.5~3.0ê¹Œì§€ ì˜¬ë ¤ë„ ë¨

    # ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ í›„ ì •ê·œí™”(í•©=1)  â€” ë„ˆë¬´ ì¢ì•„ì§€ì§€ ì•Šê²Œ lower bound ì˜¬ë¦¼
    widths = np.clip(widths, 0.08, 0.70)
    widths = widths / widths.sum()


    # âœ… ì „ì²´ ê°€ë¡œí­ì„ í…ìŠ¤íŠ¸ ì–‘ì— ë¹„ë¡€í•´ í™•ëŒ€
    #    (ìƒí’ˆëª… ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë” ë°˜ì˜)
    total_chars = sum(max_lens) + max_lens[col_idx_map["ìƒí’ˆëª…"]]
    fig_w = min(20.0, max(12.0, 0.16 * total_chars))  # 12~20ì¸ì¹˜ ì‚¬ì´ ë™ì 
    fig_h = 6.0

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis("off")

    table = ax.table(
        cellText=df.values,
        colLabels=cols,
        colWidths=widths.tolist(),   # ë¹„ìœ¨(í•©=1)
        cellLoc="center",
        loc="center"
    )

    # í°íŠ¸/ìŠ¤ì¼€ì¼
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.20, 1.18)         # x ìŠ¤ì¼€ì¼ ì‚´ì§ í‚¤ì›Œ ê°€ë¡œ ì—¬ìœ  í™•ë³´

    # í—¤ë” ìƒ‰
    for c in range(len(cols)):
        table[(0, c)].set_facecolor("#eeeeee")

    # 3ì¤„ ë¸”ë¡ A/B
    nrows, ncols = len(df), len(cols)
    color_a, color_b = "#ffffff", "#CBE7F6"
    for r in range(1, nrows+1):
        row_color = color_a if ((r-1)//3) % 2 == 0 else color_b
        for c in range(ncols):
            table[(r, c)].set_facecolor(row_color)

    # ìƒí’ˆëª… ì—´ë§Œ 9pt (ê²¹ì¹¨ ì—¬ì§€ ì¤„ì„)
    if "ìƒí’ˆëª…" in col_idx_map:
        cidx = col_idx_map["ìƒí’ˆëª…"]
        for r in range(len(df)+1):  # í—¤ë” í¬í•¨
            table[(r, cidx)].set_fontsize(9)

    # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”
    for (r, c), cell in table.get_celld().items():
        if hasattr(cell, "PAD"):
            cell.PAD = 0.1

    # âœ… ê°€ëŠ¥í•œ ê²½ìš°: ì‹¤ì œ í…ìŠ¤íŠ¸ í­ ê¸°ë°˜ìœ¼ë¡œ ì—´ ìë™ í­ ì¬ì„¤ì • (matplotlib ë²„ì „ì— ë”°ë¼ ì§€ì›)
    if hasattr(table, "auto_set_column_width"):
        try:
            table.auto_set_column_width(col=list(range(ncols)))
        except Exception:
            pass

    #plt.subplots_adjust(left=0.02, right=0.98)
    #plt.tight_layout(pad=0.2)

    file_path4_thisWeekSalesTop3 = "graph4_thisWeekRevTop3.png"
        # â¬‡ï¸ ì˜ë¦¼ ë°©ì§€ìš© bbox_inches
    fig.savefig(file_path4_thisWeekSalesTop3, dpi=170, bbox_inches='tight')
    plt.close(fig)

    blob = bucket.blob(f'{gameidx}/{file_path4_thisWeekSalesTop3}')
    blob.upload_from_filename(file_path4_thisWeekSalesTop3)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path4_thisWeekSalesTop3)

    return f'{gameidx}/{file_path4_thisWeekSalesTop3}'


def rgroup_rev_upload_notion(joyplegameid: int, gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    ########### (1) ì œëª©
    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "4. ì´ë²ˆì£¼ ìƒì„¸ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(1) Rê·¸ë£¹ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": " ** ì „ì›” ê³¼ê¸ˆì•¡ ê¸°ì¤€ Rê·¸ë£¹ ì…ë‹ˆë‹¤. \n ** ì£¼ì°¨ë³„ ê¸°ì¤€ì€ ìˆ˜ìš”ì¼~í™”ìš”ì¼ ì…ë‹ˆë‹¤. " }}]
                },
            }
        ],
    )

    try:
        gcs_path = f'{gameidx}/graph4_RgroupSales.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_RgroupSales.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_RgroupSales = context['task_instance'].xcom_pull(
        task_ids = 'rev_group_rev_pu',
        key='rev_group_rev_pu'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_RgroupSales,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    ########### (3) ì œë¯¸ë‚˜ì´ í•´ì„

    blocks = md_to_notion_blocks(rev_group_rev_pu_gemini(joyplegameid, service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(2) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ë°ì´í„° ê¸°ì¤€ : IAP êµ¬ë§¤ - IAP ì ¬êµ¬ë§¤ - IAP ë£¨ë¹„êµ¬ë§¤ + ìœ ê°€ì ¬ ì‚¬ìš©ë‚´ì—­ + ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content":" ** ì ¬ê³¼ ë£¨ë¹„ë¡œ ì–´ë–¤ ìƒí’ˆì„ êµ¬ë§¤í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´, IAPë¡œ ì ¬ê³¼ ë£¨ë¹„ë¥¼ êµ¬ë§¤í•œ ê²ƒì€ ì œê±°í•œ í›„ ìœ ê°€ì ¬/ìœ ê°€ë£¨ë¹„ ì‚¬ìš©ë‚´ì—­ì„ ë§¤ì¶œë¡œ ì§‘ê³„í•˜ì˜€ìŠµë‹ˆë‹¤." }}]
                },
            }
        ],
    )



def iap_gem_ruby_upload_notion(joyplegameid: int, gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )


    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage = context['task_instance'].xcom_pull(
        task_ids = 'iap_gem_ruby',
        key='iap_gem_ruby'
        )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_salesByPackage,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
        )
    
    blocks = md_to_notion_blocks(iap_gem_ruby_gemini(service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    # í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ì¤‘ê°„ì— ê·¸ë˜í”„ ì‚½ì…ì„ ìœ„í•œ ê²°ê³¼ í…ìŠ¤íŠ¸ 5ë¶„í• 

    text = top3_items_by_category_gemini(service_sub)

    # ì¤„ ë‹¨ìœ„ ë¶„ë¦¬
    lines = [line.strip() for line in text.split("\n") if line.strip()]

    # ë¸”ë¡ ë‹¨ìœ„ ë¶„ë¦¬
    blocks_raw = []
    current_block = []

    for line in lines:
        if line.startswith("**") and line.endswith("**"):
            # ìƒˆë¡œìš´ ë¸”ë¡ ì‹œì‘ â†’ ê¸°ì¡´ ë¸”ë¡ ì €ì¥
            if current_block:
                blocks_raw.append(current_block)
            current_block = [line]
        else:
            current_block.append(line)

    # ë§ˆì§€ë§‰ ë¸”ë¡ ì €ì¥
    if current_block:
        blocks_raw.append(current_block)

    # ì´ì œ blocks_raw = [[í—¤ë”, ë‚´ìš©...], [í—¤ë”, ë‚´ìš©...], ...]

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    blocks_bucket = {"blocks_1": [], "blocks_2": [], "blocks_3": [], "blocks_4": [], "blocks_5": []}

    found_first = False
    for block in blocks_raw:
        header = block[0]

        # ë§¤ì¶œ 1ìœ„ ì „ê¹Œì§€ëŠ” blocks_1
        if not found_first:
            if "(ë§¤ì¶œ 1ìœ„)" in header:
                found_first = True
                blocks_bucket["blocks_2"] = block
            else:
                blocks_bucket["blocks_1"].extend(block)
            continue

        # ì´í›„ ë§¤ì¶œ 2ìœ„, 3ìœ„, ë‚˜ë¨¸ì§€ êµ¬ë¶„
        if "(ë§¤ì¶œ 2ìœ„)" in header:
            blocks_bucket["blocks_3"] = block
        elif "(ë§¤ì¶œ 3ìœ„)" in header:
            blocks_bucket["blocks_4"] = block
        else:
            blocks_bucket["blocks_5"].extend(block)

    for k, v in blocks_bucket.items():
        if isinstance(v, list):
            blocks_bucket[k] = "\n".join(v)  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜

    blocks = md_to_notion_blocks(blocks_bucket["blocks_1"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 1ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category1.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category1.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_2"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 2ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category2.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category2.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_3"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    ## ìƒí’ˆì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ 3ìœ„ ê·¸ë˜í”„ ì‚½ì…
    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_Category3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_Category3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    blocks = md_to_notion_blocks(blocks_bucket["blocks_4"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    blocks = md_to_notion_blocks(blocks_bucket["blocks_5"], 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True


def iap_toggle_add(gameidx: str, service_sub:str, **context):
    
    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

        # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_IAP.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_IAP.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_IAP = context['task_instance'].xcom_pull(
        task_ids='iap_df',
        key='iap_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_IAP,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (IAP) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(iap_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True

def gem_toggle_add(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_GEM.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_GEM.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_GEM = context['task_instance'].xcom_pull(
        task_ids='gem_df',
        key='gem_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_GEM,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ì ¬) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(gem_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def ruby_toggle_add(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    toggle_resp = notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [
                        {"type": "text", "text": {"content": "(ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ"}, "annotations": {"bold": True}}
                    ]
                },
            }
        ],
    )
    toggle_id = toggle_resp["results"][0]["id"]

    create_url = "https://api.notion.com/v1/file_uploads"

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_salesByPackage_RUBY.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_salesByPackage_RUBY.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{toggle_id}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result4_salesByPackage_RUBY = context['task_instance'].xcom_pull(
        task_ids='ruby_df',
        key='ruby_df'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=toggle_id,
        df=query_result4_salesByPackage_RUBY,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - (ë£¨ë¹„) ìƒí’ˆêµ°ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(ruby_df_gemini(service_sub))

    notion.blocks.children.append(
        block_id=toggle_id,
        children=blocks
    )

    return True


def rgroup_top3_upload_notion(gameidx: str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
    PAGE_INFO['id'],
    children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(3) ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œ/PU ìƒìœ„ 3ê°œ ìƒí’ˆ \n" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/graph4_thisWeekPUTop3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekPUTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    ### íŒŒì¼ ì—…ë¡œë“œ ê°ì²´ 
    try:
        gcs_path = f'{gameidx}/graph4_thisWeekRevTop3.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'graph4_thisWeekRevTop3.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result4_thisWeekPUTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    query_result4_thisWeekSalesTop3 = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_top3_rev',
        key='rgroup_top3_rev'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekPUTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(PU)",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result4_thisWeekSalesTop3,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - Rê·¸ë£¹ë³„ ìƒìœ„3ê°œ ìƒí’ˆ(ë§¤ì¶œ) ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    
    blocks = md_to_notion_blocks(rgroup_top3_gemini(send_resp, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


### ì›”ë³„ ì¼ í‰ê·  ë§¤ì¶œ
def monthly_day_average_rev(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""
    select month
    , cast(sum(pricekrw) as int64) as `ì´ë§¤ì¶œ`
    , max(day) `ì¼ ìˆ˜`
    , cast( sum(pricekrw)/max(day) as int64) as `ì¼í‰ê·  ë§¤ì¶œ`
    from
    (select * , cast(format_date('%d',  logdatekst ) as int64) as day
    , format_date('%Y-%m',  logdatekst ) as month
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    and logdatekst<=DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    )

    group by 1
    order by 1
    """

    query_result =query_run_method('5_logterm_sales', query)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path

######### ì›”ë³„ ì¼ í‰ê·  ë§¤ì¶œ - ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸ ìƒì„±
def monthly_day_average_rev_gemini(joyplegameid: int, service_sub: str, path_monthly_day_average_rev:str, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    query_result5_dailyAvgRevenue = load_df_from_gcs(bucket, path_monthly_day_average_rev)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response5_dailyAvgRevenue = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ì´ë²ˆë‹¬ì€ KSTë¡œ ì–´ì œë‚ ì§œ ê¸°ì¤€ì´ì•¼. ì–´ì œë‚ ì§œê°€ ê¸°ì¤€ì´ë¼ê³  ëª…ì‹œí•˜ì§€ë§ˆ

    ì´ë²ˆë‹¬ ì¼í‰ê·  ë§¤ì¶œì— ëŒ€í•´ ì¥ê¸°ì ì¸ íë¦„ì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜.
    ì¥ê¸°ì ì¸ ê´€ì ìœ¼ë¡œë„ ë¹„êµí•˜ë˜, ìµœê·¼ ì›”ë“¤ê³¼ë„ ë¹„êµí•´ì¤˜.
    ì‘ë…„ê³¼ ì˜¬í•´ë¥¼ ë¹„êµí•´ì¤˜


    <ì„œì‹ ìš”êµ¬ì‚¬í•­>
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜. e.g. 8700ë§Œì›
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    <ì›”ë³„ ì¼í‰ê·  ë§¤ì¶œ>
    {query_result5_dailyAvgRevenue}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=[
                ""

            ],
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response5_dailyAvgRevenue.text


###### ê³¼ê¸ˆê·¸ë£¹ë³„ ë§¤ì¶œ
def rgroup_rev_DOD(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""
    with sales_raw as ( ## 6208778
    select *
    , format_date('%Y-%m',  logdatekst ) as month
    , cast(format_date('%d',  logdatekst ) as int64) as day

    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    ),


    monthly_rev as (
    select authaccountname, logmonth, month, regmonth, ifnull(sum(pricekrw),0) as rev
    from
    (select *
    , format_date('%Y-%m-01',  logdatekst ) as logmonth
    , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    from sales_raw
    where logdatekst>='2024-01-01'
    and day<=cast(format_date('%d',  DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY) ) as int64)  )
    group by 1,2,3,4
    ),

    r_group as (
    select *
    , case
    when rev>=10000000 then 'R0'
    when rev>=1000000  then 'R1'
    when rev>=100000   then 'R2'
    when rev>=10000    then 'R3'
    when rev>=1        then 'R4'
    when rev=0         then 'nonPU'
    else 'ETC' end as rgroup
    from monthly_rev
    ),

    final as (


    select a.*, c.rgroup
    from
    ## iap ë§¤ì¶œ raw
    (select *, format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    , format_date('%Y',  AuthAccountRegDateKST ) as regyear
    from sales_raw
    where day<=cast(format_date('%d',  DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY) ) as int64)
    ) as a

    ## rgoup
    left join
    (select *
    from r_group) as c
    on a.authaccountname = c.authaccountname and a.month = c.month


    )

    select month
    , cast(sum(if(rgroup = 'R0' , pricekrw, 0)) as int64) as R0_rev
    , cast(sum(if(rgroup = 'R1' , pricekrw, 0)) as int64) as R1_rev
    , cast(sum(if(rgroup = 'R2' , pricekrw, 0)) as int64) as R2_rev
    , cast(sum(if(rgroup = 'R3' , pricekrw, 0)) as int64) as R3_rev
    , cast(sum(if(rgroup = 'R4' , pricekrw, 0)) as int64) as R4_rev
    , cast(sum(if(rgroup = 'nonPU' , pricekrw, 0)) as int64) as nonPU_rev
    , cast(sum(pricekrw) as int64) as ALL_rev
    , count(distinct if(rgroup='R0', authaccountname, null)) as R0_user
    , count(distinct if(rgroup='R1', authaccountname, null)) as R1_user
    , count(distinct if(rgroup='R2', authaccountname, null)) as R2_user
    , count(distinct if(rgroup='R3', authaccountname, null)) as R3_user
    , count(distinct if(rgroup='R4', authaccountname, null)) as R4_user
    , count(distinct if(rgroup in ('R0','R1','R2','R3','R4'), authaccountname, null)) as PU
    #, count(distinct if(rgroup in ('R0','R1','R2','R3','R4'), authaccountname, null))/count(distinct authaccountname) as PUR
    , count(distinct if(rgroup='nonPU', authaccountname, null)) as nonPU_user
    , count(distinct authaccountname) as ALL_user
    from final
    group by month
    order by month
    """

    query_result =query_run_method('5_logterm_sales', query)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path

####### ê³¼ê¸ˆê·¸ë£¹ë³„ ì´ ë§¤ì¶œ
def rgroup_rev_total(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""

    with sales_raw as ( ## 6208778
    select *
    , format_date('%Y-%m',  logdatekst ) as month
    , cast(format_date('%d',  logdatekst ) as int64) as day

    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    ),


    monthly_rev as (
    select authaccountname, logmonth, month, regmonth, ifnull(sum(pricekrw),0) as rev
    from
    (select *
    , format_date('%Y-%m-01',  logdatekst ) as logmonth
    , format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    from sales_raw
    where logdatekst>='2024-01-01'
    and logdatekst<= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY) )
    group by 1,2,3,4

    ),

    r_group as (
    select *
    , case
    when rev>=10000000 then 'R0'
    when rev>=1000000  then 'R1'
    when rev>=100000   then 'R2'
    when rev>=10000    then 'R3'
    when rev>=1        then 'R4'
    when rev=0         then 'nonPU'
    else 'ETC' end as rgroup
    from monthly_rev
    ),

    final as (


    select a.*, c.rgroup
    from
    ## iap ë§¤ì¶œ raw
    (select *, format_date('%Y-%m',  AuthAccountRegDateKST ) as regmonth
    , format_date('%Y',  AuthAccountRegDateKST ) as regyear
    from sales_raw
    where logdatekst<= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    ) as a

    ## rgoup
    left join
    (select *
    from r_group) as c
    on a.authaccountname = c.authaccountname and a.month = c.month
    )

    select month
    , cast(sum(if(rgroup = 'R0' , pricekrw, 0)) as int64) as R0_rev
    , cast(sum(if(rgroup = 'R1' , pricekrw, 0)) as int64) as R1_rev
    , cast(sum(if(rgroup = 'R2' , pricekrw, 0)) as int64) as R2_rev
    , cast(sum(if(rgroup = 'R3' , pricekrw, 0)) as int64) as R3_rev
    , cast(sum(if(rgroup = 'R4' , pricekrw, 0)) as int64) as R4_rev
    , cast(sum(if(rgroup = 'nonPU' , pricekrw, 0)) as int64) as nonPU_rev
    , cast(sum(pricekrw) as int64) as ALL_rev
    , count(distinct if(rgroup='R0', authaccountname, null)) as R0_user
    , count(distinct if(rgroup='R1', authaccountname, null)) as R1_user
    , count(distinct if(rgroup='R2', authaccountname, null)) as R2_user
    , count(distinct if(rgroup='R3', authaccountname, null)) as R3_user
    , count(distinct if(rgroup='R4', authaccountname, null)) as R4_user
    , count(distinct if(rgroup in ('R0','R1','R2','R3','R4'), authaccountname, null)) as PU
    , count(distinct if(rgroup='nonPU', authaccountname, null)) as nonPU_user
    , count(distinct authaccountname) as ALL_user
    from final
    group by month
    order by month
    """

    query_result =query_run_method('5_logterm_sales', query)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path = f"{gameidx}/{timestamp}.parquet"
        
    saved_path = save_df_to_gcs(query_result, bucket, gcs_path)

    return saved_path


####### ê³¼ê¸ˆê·¸ë£¹ë³„ ì´ ë§¤ì¶œ - ì œë¯¸ë‚˜ì´ ì½”ë©˜íŠ¸ ìƒì„±
def rgroup_rev_total_gemini(joyplegameid: int, service_sub: str, path_rgroup_rev_DOD:str, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    query_result5_monthlyRgroupRevenue = load_df_from_gcs(bucket, path_rgroup_rev_DOD)

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response5_monthlyRgroup = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ë§¨ ì„œë‘ì— Bold ì²´ë¡œ ì „ì›”ëŒ€ë¹„ ì–´ë–¤ Rê·¸ë£¹ì—ì„œ ì¦ê°€í–ˆê³  ì–´ë–¤ Rê·¸ë£¹ì—ì„œ ê°ì†Œí–ˆëŠ”ì§€ ì¨ì¤˜.
    ê·¸ë¦¬ê³  ê°„ë‹¨í•˜ê²Œ ì´ì „ì— ë¹„í•´ íŠ¸ë Œë“œê°€ ì–´ë–¤ì§€ 10ì¤„ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.
    PU ìˆ˜ì™€ ë§¤ì¶œì´ ì¦ê°€í–ˆëŠ”ì§€ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜.

    ë§¤ì¶œì´ 1ì–µ6ì²œì¸ë° 16ì–µì´ë¼ê³  ì“°ê³  ê·¸ëŸ¬ì§€ë§ˆ ì˜ í™•ì¸í•´


    <ì„œì‹ ìš”êµ¬ì‚¬í•­>
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜. ì˜ˆ) 1.54ì–µ, 750ë§Œì› 9500ë§Œì›
    3. ëª¨ë“  ìˆ«ìëŠ” ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ í‘œê¸°í•˜ê³  ì²œ ë‹¨ìœ„ë§ˆë‹¤ ì‰¼í‘œ(,) ë¥¼ ì¨ì¤˜. ì˜ˆ) 3123 â†’ 3,123
    í•œê¸€ ìˆ«ì í‘œê¸°(ì‚¼ì²œë°±ìŠ¤ë¬¼ì‚¼ëª… ë“±)ëŠ” ê¸ˆì§€
    4. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.

    < ë°ì´í„° ì„¤ëª…>
    1. ì›”ë³„ ë™ê¸°ê°„ ë°ì´í„°ì•¼. ì–´ì œì ê¸°ì¤€ìœ¼ë¡œ ì´ë²ˆë‹¬ì´ 15ì¼ë§Œ ì§€ë‚¬ìœ¼ë©´, ì „ì›”ë“¤ë„ 15ì¼ê¹Œì§€ë§Œ ì§‘ê³„ë¼
    2. ì´ë²ˆë‹¬ ê³¼ê¸ˆì•¡ ê¸°ì¤€ìœ¼ë¡œ Rê·¸ë£¹ì„ ë‚˜ëˆ´ì–´.
    3. nonPU ëŠ” ì´ë²ˆë‹¬ ë¬´ê³¼ê¸ˆìœ ì €ë¼ì„œ PU ê°€ ì•„ë‹ˆë¼ëŠ” ëœ»ì´ì•¼. PU ìˆ˜ êµ¬í• ë•Œ nonPU ëŠ” ë”í•˜ë©´ ì•ˆë¼.
    4. Rê·¸ë£¹ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„.
    R0 : ë‹¹ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ì´ìƒ
    R1 : ë‹¹ì›” ê³¼ê¸ˆì•¡ 1ì²œë§Œì› ë¯¸ë§Œ ~ 1ë°±ë§Œì› ì´ìƒ
    R2 : ë‹¹ì›” ê³¼ê¸ˆì•¡ 1ë°±ë§Œì› ë¯¸ë§Œ ~ 10ë§Œì› ì´ìƒ
    R3 : ë‹¹ì›” ê³¼ê¸ˆì•¡ 10ë§Œì› ë¯¸ë§Œ ~ 1ë§Œì› ì´ìƒ
    R4 : ë‹¹ì›” ê³¼ê¸ˆì•¡ 1ë§Œì› ë¯¸ë§Œ ~ 0ì› ì´ˆê³¼
    nonPU : ë‹¹ì›” ë¬´ê³¼ê¸ˆ ìœ ì €

    <ì›”ë³„ Rê·¸ë£¹ë³„ ë§¤ì¶œê³¼ PU>
    {query_result5_monthlyRgroupRevenue}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=[
                ""

            ],
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response5_monthlyRgroup.text

## ê°€ì…ì—°ë„ë³„ ë§¤ì¶œ
def rev_cohort_year(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    query = f"""
    with sales_raw as (
    select *
    , format_date('%Y-%m',  logdatekst ) as month
    , format_date('%Y',  authaccountregdatekst ) as regyear
    , cast(format_date('%d',  logdatekst ) as int64) as day
    , EXTRACT(DAY FROM LAST_DAY(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)) as maxday
    from `dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V`
    where joyplegameid = {joyplegameid}
    and logdatekst>='2024-01-01'
    and logdatekst<= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    and authaccountregdatekst is not null
    ),

    sales_this_month as (
    select month, regyear, (rev/day)*(maxday) as rev_pred
    from
    (select month, regyear, maxday, max(day) as day, sum(pricekrw) as rev
    from sales_raw
    where logdatekst>=DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    group by 1,2,3)
    )

    ## ì „ì›”ê¹Œì§€ ì‹¤ì¸¡
    select month, regyear, sum(pricekrw) as rev
    from sales_raw
    where logdatekst < DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    group by 1,2

    union all

    ## ì´ë²ˆë‹¬ ì˜ˆì¸¡ (ì¼í• ê³„ì‚°)
    select concat(month,'(ì˜ˆì¸¡)') as month , regyear, rev_pred as rev
    from sales_this_month

    /*
    union all

    ## ì´ë²ˆë‹¬ ì‹¤ì¸¡
    select concat(month,'(ì‹¤ì¸¡)') as month, regyear, rev from(
    select month, regyear, sum(pricekrw) as rev
    from sales_raw
    where logdatekst>=DATE_TRUNC(DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY), MONTH)
    and logdatekst <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)
    group by 1,2)
    */
    """

    query_result= query_run_method('5_logterm_sales', query)

    #################### ê°€ì…ì—°ë„ë³„ ë§¤ì¶œì„ í”¼ë²—í˜•íƒœë¡œ ì „ì²˜ë¦¬
    df = query_result.copy()
    df['rev'] = pd.to_numeric(df['rev'], errors='coerce')#.fillna(0)
    df['regyear']  = pd.to_numeric(df['regyear'], errors='coerce').astype('Int64')

    # ì‹œê°„ ì •ë ¬ìš© íŒŒìƒ(ì›ë³¸ monthëŠ” ê·¸ëŒ€ë¡œ)
    df['_month_dt'] = pd.to_datetime(df['month'], errors='coerce')

    # í”¼ë²—í˜•íƒœ
    pv2 = (
        df.groupby(['month','regyear'])['rev'].sum()
        .unstack('regyear'#, fill_value=0
                )
    )

    # í–‰ì„ ì‹¤ì œ ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬
    #pv2 = pv2.loc[pv2.assign(_order=pd.to_datetime(pv2.index)).sort_values('_order').index]
    pv2 = pv2.sort_index(axis=1)  # ì—´(ì—°ë„) ì˜¤ë¦„ì°¨ìˆœ

    # âœ… ìœ„ì— ëœ¨ëŠ” 'regyear' ë°°ë„ˆ ì œê±° (columns.name ì œê±°)
    pv2.columns.name = None

    # ì¸ë±ìŠ¤ ì´ë¦„ì„ 'month'ë¡œ ì§€ì •í•œ ë’¤ ì»¬ëŸ¼ìœ¼ë¡œ ë¦¬ì…‹
    pv2 = pv2.rename_axis('month').reset_index()

    # (ì„ íƒ) 'month' ë‹¤ìŒì— ì—°ë„ë“¤ ì˜¤ë„ë¡ ì •ë ¬ ë³´ì¥
    year_cols = [c for c in pv2.columns if c != 'month']
    year_cols_sorted = sorted(year_cols, key=lambda x: int(x))
    pv2 = pv2[['month'] + year_cols_sorted]

    # ì´í•© ì—´ ì¶”ê°€
    pv2['ì´í•©'] = pv2[year_cols_sorted].fillna(0).sum(axis=1)

    # xcomì— insert
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    gcs_path_1 = f"{gameidx}/{timestamp}_1.parquet"
    gcs_path_2 = f"{gameidx}/{timestamp}_2.parquet"
        
    path_regyearRevenue = save_df_to_gcs(query_result, bucket, gcs_path_1)
    path_regyearRevenue_pv2 = save_df_to_gcs(pv2, bucket, gcs_path_2)

    return path_regyearRevenue, path_regyearRevenue_pv2


def rev_cohort_year_gemini(joyplegameid: int, service_sub: str, path_rev_cohort_year:str, **context):

    from google.genai import Client
    genai_client = Client(vertexai=True,project=PROJECT_ID,location=LOCATION)

    pv2 = load_df_from_gcs(bucket, path_rev_cohort_year)

    pv2 = context['task_instance'].xcom_pull(
        task_ids = 'rev_cohort_year',
        key='rev_cohort_year'
    )

    RUN_ID = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d")
    LABELS = {"datascience_division_service": 'gameinsight_framework',
            "run_id": RUN_ID,
            "datascience_division_service_sub" : service_sub}
    
    response5_regyearRevenue = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    * ì´ë²ˆë‹¬ : KST ì–´ì œë‚ ì§œ ê¸°ì¤€

    ê°€ì…ì—°ë„ë³„ ì›” ë§¤ì¶œì´ì•¼. ì´ë²ˆë‹¬ì€ ì¼í• ê³„ì‚°í•´ì„œ ì˜ˆì¸¡ì¹˜ë¡œ ë‘ì—ˆì–´.
    ë¨¼ì € ì„œë‘ì— ê° ì—°ë„ë³„ë¡œ ë§¤ì¶œê¸°ì—¬ìˆœì„œë¥¼ ì•Œë ¤ì¤˜ (ì˜ˆì‹œ : 20ë…„>21ë…„> ...)
    ì´ë²ˆë‹¬ ë§¤ì¶œì— ê°€ì…ì—°ë„ë³„ ì¥ê¸°ì ì¸ ë§¤ì¶œì´ ì–´ë–»ê²Œ ëëŠ”ì§€ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ì•Œë ¤ì¤˜.
    ê·¸ë¦¬ê³  ì´ë²ˆë…„ë„ ê°€ì…ìœ ì €ì˜ ë§¤ì¶œì•¡ì´ í¬ì§€ë§Œ ê·¸ê±´ ì‹ ê·œìœ ì €ë¼ì„œ í–¥í›„ì—” ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì¤˜.


    <ì„œì‹ ìš”êµ¬ì‚¬í•­>
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    2. ë§¤ì¶œ 1ì›ë‹¨ìœ„ê¹Œì§€ ë‹¤ ì“°ì§€ ë§ê³  ëŒ€ëµ ë§í•´ì¤˜.(ì˜ˆ: 27.5ì–µ / 3,500ë§Œì› )
    3. ëª¨ë“  ìˆ«ìëŠ” ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ í‘œê¸°í•˜ê³  ì²œ ë‹¨ìœ„ë§ˆë‹¤ ì‰¼í‘œ(,) ë¥¼ ì¨ì¤˜. ì˜ˆ) 3123 â†’ 3,123
    í•œê¸€ ìˆ«ì í‘œê¸°(ì‚¼ì²œë°±ìŠ¤ë¬¼ì‚¼ëª… ë“±)ëŠ” ê¸ˆì§€
    4. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    <ì›”ë³„ Rê·¸ë£¹ë³„ ë§¤ì¶œê³¼ PU>
    {pv2}


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=[
                ""
            ],
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS
        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response5_regyearRevenue.text


def monthly_day_average_rev_table_draw(gameidx:str, **context):
    df = context['task_instance'].xcom_pull(
        task_ids = 'monthly_day_average_rev',
        key='monthly_day_average_rev'
    )

    def render_table_image(
        df: pd.DataFrame,
        gameidx: str,
        out_path: str = "graph5_dailyAvgRevenueTable.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
    ):
        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = ["month", "ì´ë§¤ì¶œ", "ì¼ ìˆ˜", "ì¼í‰ê·  ë§¤ì¶œ"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            display_df[c] = display_df[c].apply(
                lambda x: "" if pd.isna(x) else f"{int(x):,}"
            )
        display_df["month"] = display_df["month"].astype(str).fillna("")

        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)

        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5, "ì›”ë³„ ì¼í‰ê·  ë§¤ì¶œ",
                ha="left", va="bottom", fontsize=8, fontweight="bold")
        
        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.1)
        plt.close(fig)

        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    
    gcs_path = render_table_image(df=df, gameidx=gameidx)
    return gcs_path


### ì¼ í‰ê·  ë§¤ì¶œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
def monthly_day_average_rev_graph_draw(gameidx:str, **context):

    query_result5_dailyAvgRevenue = context['task_instance'].xcom_pull(
        task_ids = 'monthly_day_average_rev',
        key='monthly_day_average_rev'
    )

    sns.lineplot(x= query_result5_dailyAvgRevenue.columns[0],
             y=query_result5_dailyAvgRevenue.columns[3],
             data=query_result5_dailyAvgRevenue,
             marker="o")

    
    # yì¶• ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ ë„£ê¸°
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))

    # xì¶• ëˆˆê¸ˆì„ 7ê°œ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œ (ì˜ˆ: 1ì£¼ì¼ ê°„ê²©)
    plt.xticks(query_result5_dailyAvgRevenue[query_result5_dailyAvgRevenue.columns[0]][::1], rotation=45)
    # x,y ì¶• ê¸€ì í¬ê¸° ì¡°ì •
    plt.tick_params(axis="both", labelsize=10)

    # í‘œ ì œëª©
    plt.title("ì›”ë³„ ì¼í‰ê·  ë§¤ì¶œ")

    # yì¶• 0ë¶€í„° ì‹œì‘
    #plt.ylim(0, None)   # Noneì´ë©´ ìµœëŒ€ê°’ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì§
    # yì¶• ë³´ì¡°ì„ 
    plt.grid(axis='y', linestyle='--', alpha=0.7) # alpha=íˆ¬ëª…ë„

    # x,yì¶• ì œê±°
    plt.xlabel(None)
    plt.ylabel(None)

    #plt.show()
    # ê·¸ë˜í”„ ì•ˆì˜ë¦¬ê²Œ
    plt.tight_layout()


    # í–¥í›„ ë…¸ì…˜ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì €ì¥
    # #print(os.getcwd()) ì´ ê³³ì— ì €ì¥ë˜ê³ , colab í™˜ê²½ì´ë¼ ì¢Œì¸¡ í´ë”ëª¨ì–‘ ëˆ„ë¥´ë©´ png ìˆìŒ.
    # ì„¸ì…˜ ì¢…ë£Œì‹œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    file_path5_dailyAvgRevenueLine = "graph5_dailyAvgRevenueLine.png"
    plt.savefig(file_path5_dailyAvgRevenueLine, dpi=120) # dpi : í•´ìƒë„
    plt.close()

    blob = bucket.blob(f'{gameidx}/{file_path5_dailyAvgRevenueLine}')
    blob.upload_from_filename(file_path5_dailyAvgRevenueLine)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(file_path5_dailyAvgRevenueLine)

    return f'{gameidx}/{file_path5_dailyAvgRevenueLine}'


def monthly_day_average_merge_graph(gameidx:str, **context):
    # 1) íŒŒì¼ ê²½ë¡œ
    p1 = monthly_day_average_rev_table_draw(gameidx, **context)   # ì²« ë²ˆì§¸ ì´ë¯¸ì§€
    p2 = monthly_day_average_rev_graph_draw(gameidx, **context)   # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€
    save_to = 'graph5_dailyAvgRevenue.png'  # ì €ì¥ ê²½ë¡œ

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    im1 = Image.open(p1).convert("RGBA")
    im2 = Image.open(p2).convert("RGBA")

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # PNGë¡œ ì €ì¥
    blob = bucket.blob(f'{gameidx}/{save_to}')
    blob.upload_from_filename(save_to)

    # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
    os.remove(save_to)

    return f'{gameidx}/{save_to}'


#### ì›”ë³„ R ê·¸ë£¹ë³„ ë§¤ì¶œ ë™ê¸°ê°„ í‘œ
def rgroup_rev_DOD_table_draw(gameidx:str, **context):
    query_result5_monthlyRgroupRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_DOD',
        key='rgroup_rev_DOD'
    )

    df = query_result5_monthlyRgroupRevenue.iloc[:, [0,1,2,3,4,5,7]]
    
    df = df.rename(
    columns = {"month" : "month",
               "R0_rev" : "R0",
               "R1_rev" : "R1",
               "R2_rev" : "R2",
               "R3_rev" : "R3",
               "R4_rev" : "R4",
               "ALL_rev" : "ì´í•©",
               }
    )

    def render_table_image(
        df: pd.DataFrame,
        out_path: str = "graph5_monthlyRgroupRevenue.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
    ):


        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = ["month", "R0", "R1", "R2", "R3", "R4", "ì´í•©"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            display_df[c] = display_df[c].apply(
                lambda x: "" if pd.isna(x) else f"{int(x):,}"
            )
        display_df["month"] = display_df["month"].astype(str).fillna("")

        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)

        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5, "ì›”ë³„ Rê·¸ë£¹ë³„ ë§¤ì¶œ(ë™ê¸°ê°„)",
                ha="left", va="bottom", fontsize=8, fontweight="bold")
        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.2)
        plt.close(fig)
        
        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    
    gcs_path = render_table_image(df=df, gameidx=gameidx)
    return gcs_path



#### ì›”ë³„ R ê·¸ë£¹ë³„ PU ìˆ˜ ë™ê¸°ê°„ í‘œ
def rgroup_pu_DOD_table_draw(gameidx:str, **context):
    query_result5_monthlyRgroupRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_DOD',
        key='rgroup_rev_DOD'
    )
    df = query_result5_monthlyRgroupRevenue.iloc[:, [0,8,9,10,11,12,14,15,13]]

    df = df.rename(
        columns = {"month" : "month",
                "R0_user" : "R0",
                "R1_user" : "R1",
                "R2_user" : "R2",
                "R3_user" : "R3",
                "R4_user" : "R4",
                "nonPU_user" : "nonPU",
                "PU" : "PU",
                "ALL_user" : "ì´í•©",
                }
    )

    def render_table_image(
        df: pd.DataFrame,
        out_path: str = "graph5_monthlyRgroupPU.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
    ):


        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = ["month", "R0", "R1", "R2", "R3", "R4", "nonPU", "PU", "ì´í•©"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        # display_df = df.copy()
        # for c in cols[1:]:
        #     display_df[c] = display_df[c].apply(
        #         lambda x: "" if pd.isna(x) else f"{int(x):,}"
        #     )
        # display_df["month"] = display_df["month"].astype(str).fillna("")

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            if c == "PUR":   # PUR ì—´ë§Œ í¼ì„¼íŠ¸ë¡œ í‘œì‹œ ( PUR ì»¬ëŸ¼ ì—†ìŒ )
                display_df[c] = display_df[c].apply(
                    lambda x: "" if pd.isna(x) else f"{x:.1%}"  # ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€ í¼ì„¼íŠ¸
                )
            else:
                display_df[c] = display_df[c].apply(
                    lambda x: "" if pd.isna(x) else f"{int(x):,}"
                )
        display_df["month"] = display_df["month"].astype(str).fillna("")




        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)
        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5, "ì›”ë³„ Rê·¸ë£¹ë³„ PUìˆ˜(ë™ê¸°ê°„)",
                ha="left", va="bottom", fontsize=8, fontweight="bold")

        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.2)
        plt.close(fig)
        
        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    
    gcs_path = render_table_image(df=df, gameidx=gameidx)
    return gcs_path


def merge_rgroup_rev_pu_ALL_table(joyplegameid: int, gameidx: str, **context):
    p1 = rgroup_rev_DOD_table_draw(gameidx, **context)
    p2 = rgroup_pu_DOD_table_draw(gameidx, **context)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA")

    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph5_monthlyRgroupALL.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


def merge_rgroup_rev_pu_table(gameidx:str, **context):
    p1 = rgroup_rev_DOD_table_draw(gameidx, **context) # ì²« ë²ˆì§¸ ì´ë¯¸ì§€
    p2 = rgroup_pu_DOD_table_draw(gameidx, **context)   # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA") 

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph5_monthlyRgroup.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path

###############################################################



#### ì›”ë³„ Rê·¸ë£¹ ë§¤ì¶œ ì „ì²´ê¸°ê°„ í‘œ
def rgroup_rev_total_table_draw(gameidx:str, **context):
    query_result5_monthlyRgroupRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_total',
        key='rgroup_rev_total'
    )

    df = query_result5_monthlyRgroupRevenue.iloc[:, [0,1,2,3,4,5,7]]

    df = df.rename(
        columns = {"month" : "month",
                "R0_rev" : "R0",
                "R1_rev" : "R1",
                "R2_rev" : "R2",
                "R3_rev" : "R3",
                "R4_rev" : "R4",
                "ALL_rev" : "ì´í•©",
                }
    )

    def render_table_image(
        df: pd.DataFrame,
        out_path: str = "graph5_monthlyRgroupRevenueALL.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
    ):


        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = ["month", "R0", "R1", "R2", "R3", "R4", "ì´í•©"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            display_df[c] = display_df[c].apply(
                lambda x: "" if pd.isna(x) else f"{int(x):,}"
            )
        display_df["month"] = display_df["month"].astype(str).fillna("")

        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)

        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5, "ì›”ë³„ Rê·¸ë£¹ë³„ ë§¤ì¶œ(ë™ê¸°ê°„)",
                ha="left", va="bottom", fontsize=8, fontweight="bold")
        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.2)
        plt.close(fig)
        
        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    
    gcs_path = render_table_image(df=df, gameidx=gameidx)
    return gcs_path


def rgroup_pu_total_table_draw(gameidx:str, **context):
    query_result5_monthlyRgroupRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_total',
        key='rgroup_rev_total'
    )

    df = query_result5_monthlyRgroupRevenue.iloc[:, [0,8,9,10,11,12,14,15,13]]

    df = df.rename(
        columns = {"month" : "month",
                "R0_rev" : "R0",
                "R1_rev" : "R1",
                "R2_rev" : "R2",
                "R3_rev" : "R3",
                "R4_rev" : "R4",
                "ALL_rev" : "ì´í•©",
                }
        )

    def render_table_image(
        df: pd.DataFrame,
        out_path: str = "graph5_monthlyRgroupPUALL.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
        ):


        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = ["month", "R0", "R1", "R2", "R3", "R4", "nonPU", "PU", "ì´í•©"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        # display_df = df.copy()
        # for c in cols[1:]:
        #     display_df[c] = display_df[c].apply(
        #         lambda x: "" if pd.isna(x) else f"{int(x):,}"
        #     )
        # display_df["month"] = display_df["month"].astype(str).fillna("")

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            if c == "PUR":   # PUR ì—´ë§Œ í¼ì„¼íŠ¸ë¡œ í‘œì‹œ ( PUR ì»¬ëŸ¼ ì—†ìŒ )
                display_df[c] = display_df[c].apply(
                    lambda x: "" if pd.isna(x) else f"{x:.1%}"  # ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€ í¼ì„¼íŠ¸
                )
            else:
                display_df[c] = display_df[c].apply(
                    lambda x: "" if pd.isna(x) else f"{int(x):,}"
                )
        display_df["month"] = display_df["month"].astype(str).fillna("")




        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)
        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5, "ì›”ë³„ Rê·¸ë£¹ë³„ PUìˆ˜(ë™ê¸°ê°„)",
                ha="left", va="bottom", fontsize=8, fontweight="bold")

        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.2)
        plt.close(fig)
        
        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    
    gcs_path = render_table_image(df=df, gameidx=gameidx)
    return gcs_path


#### ì›”ë³„ R ê·¸ë£¹ë³„ ë§¤ì¶œ, PU í‘œ í•©ì¹˜ê¸°
def merge_rgroup_total_rev_pu_table(joyplegameid: int, gameidx: str, **context):
    p1 = rgroup_rev_total_table_draw(gameidx, **context)
    p2 = rgroup_pu_total_table_draw(gameidx, **context)

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    im1 = Image.open(BytesIO(im1)).convert("RGBA")
    im2 = Image.open(BytesIO(im2)).convert("RGBA")

    # ---- [ì˜µì…˜ A] ì›ë³¸ í¬ê¸° ìœ ì§€ + ì„¸ë¡œ íŒ¨ë”©ìœ¼ë¡œ ë†’ì´ ë§ì¶”ê¸° (ê¶Œì¥: ì™œê³¡ ì—†ìŒ) ----
    target_h = max(im1.height, im2.height)

    def pad_to_height(img, h, bg=(255, 255, 255, 0)):  # íˆ¬ëª… ë°°ê²½: ì•ŒíŒŒ 0
        if img.height == h:
            return img
        canvas = Image.new("RGBA", (img.width, h), bg)
        # ê°€ìš´ë° ì •ë ¬ë¡œ ë¶™ì´ê¸° (ìœ„ì— ë§ì¶”ë ¤ë©´ y=0)
        y = (h - img.height) // 2
        canvas.paste(img, (0, y))
        return canvas

    im1_p = pad_to_height(im1, target_h)
    im2_p = pad_to_height(im2, target_h)

    gap = 0  # ì´ë¯¸ì§€ ì‚¬ì´ ì—¬ë°±(px). í•„ìš”í•˜ë©´ 20 ë“±ìœ¼ë¡œ ë³€ê²½
    bg = (255, 255, 255, 0)  # ì „ì²´ ë°°ê²½(íˆ¬ëª…). í°ìƒ‰ ì›í•˜ë©´ (255,255,255,255)

    out = Image.new("RGBA", (im1_p.width + gap + im2_p.width, target_h), bg)
    out.paste(im1_p, (0, 0), im1_p)
    out.paste(im2_p, (im1_p.width + gap, 0), im2_p)

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    out.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph5_monthlyRgroupHap.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


######### ê°€ì…ì—°ë„ë³„ ë§¤ì¶œ í‘œ

def cohort_rev_table_draw(gameidx:str, **context):
    df = context['task_instance'].xcom_pull(
        task_ids = 'rev_cohort_year',
        key='rev_cohort_year'
    )

    def render_table_image(
        df: pd.DataFrame,
        out_path: str = "graph5_regyearRevenue.png",
        dpi: int = 200,
        header_bg="#D9E1F2",
        border_color="#000000", ## í‘œ í…Œë‘ë¦¬ ìƒ‰ê¹”
        cond_min="#5B9BD5",
        cond_mid="#FFFFFF",
        cond_max="#FF0000",
        font_family="NanumGothic",
    ):


        """
        DataFrame df -> Excel-like table PNG with:
        - Malgun Gothic font
        - Thousands separators for numeric columns
        - 3-color scale conditional formatting per numeric column
        - Auto-fit column widths by content length
        """
        # 0) ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥
        cols = df.columns.tolist()     # astype(str) ì œê±°
        # cols = ["month", "R0", "R1", "R2", "R3", "R4", "ì´í•©"]
        df = df.loc[:, cols].copy()

        # 1) í°íŠ¸ ì„¤ì • (ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ í´ë°±ë¨)
        rcParams["font.family"] = font_family

        # 2) ë¬¸ìì—´ ë³€í™˜ (ì²œë‹¨ìœ„ ì½¤ë§ˆ / NaN ì²˜ë¦¬)
        display_df = df.copy()
        for c in cols[1:]:
            display_df[c] = display_df[c].apply(
                lambda x: "" if pd.isna(x) else f"{int(x):,}"
            )
        display_df["month",'regyear'] = display_df["month"].astype(str).fillna("")

        # 3) ì—´ ë„ˆë¹„ ê³„ì‚°(ë¬¸ì ìˆ˜ ê¸°ë°˜ ëŒ€ëµì¹˜: ë¬¸ìí­â‰ˆ7px, ì¢Œìš° íŒ¨ë”© í¬í•¨)
        def col_pixel_width(series, header, is_numeric=False):
            # ìˆ«ìëŠ” ì½¤ë§ˆ í¬í•¨ í‘œì‹œ ê¸¸ì´ ê¸°ì¤€
            max_chars = max([len(str(header))] + [len(str(s)) for s in series])
            # ìˆ«ìì—´ì€ ìš°ì¸¡ì •ë ¬ & ì•½ê°„ ë” ì—¬ìœ 
            base = 10.0  # 1ê¸€ìë‹¹ px ì¶”ì •ì¹˜
            padding = 24 if is_numeric else 20
            return int(max_chars * base + padding)

        col_widths = []
        for i, c in enumerate(cols):
            is_num = i > 0
            w = col_pixel_width(display_df[c], c, is_numeric=is_num)
            # ë„ˆë¬´ ì¢ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ë„“ì§€ ì•Šë„ë¡ ê°€ë“œ
            w = max(w, 70)       # ìµœì†Œ
            w = min(w, 360)      # ìµœëŒ€
            col_widths.append(w)

        # 4) í–‰ ë†’ì´/ìŠ¤íƒ€ì¼
        header_h = 36  # í—¤ë” ë†’ì´(px)
        row_h = 30     # ë°ì´í„° í–‰ ë†’ì´(px)
        n_rows = len(display_df)
        n_cols = len(cols)

        # 5) ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(px)
        inner_w = sum(col_widths)
        inner_h = header_h + n_rows * row_h
        pad = 2  # í…Œë‘ë¦¬ ì˜¤ì°¨ ë°©ì§€ìš©
        total_w = inner_w + pad
        total_h = inner_h + pad

        # 6) Figure ìƒì„± (í”½ì…€ -> ì¸ì¹˜)
        fig_w_in = total_w / dpi
        fig_h_in = total_h / dpi
        fig, ax = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=dpi)
        ax.set_xlim(0, total_w)
        ax.set_ylim(total_h, 0)  # yì¶• ì•„ë˜ë¡œ ì¦ê°€í•˜ë„ë¡ ë’¤ì§‘ìŒ
        ax.axis("off")

        # 7) ì»¬ëŸ¬ ë³´ê°„ í•¨ìˆ˜ (3ìƒ‰ ìŠ¤ì¼€ì¼)
        def hex_to_rgb01(hx):
            hx = hx.lstrip("#")
            return tuple(int(hx[i:i+2], 16) / 255 for i in (0, 2, 4))

        c_min = np.array(hex_to_rgb01(cond_min))
        c_mid = np.array(hex_to_rgb01(cond_mid))
        c_max = np.array(hex_to_rgb01(cond_max))

        def interp_color(v, vmin, vmid, vmax):
            if pd.isna(v) or vmin is None or vmax is None or vmax == vmin:
                return (1, 1, 1)  # white
            if v <= vmid:
                t = 0.0 if vmid == vmin else (v - vmin) / (vmid - vmin)
                return tuple(c_min * (1 - t) + c_mid * t)
            else:
                t = 0.0 if vmax == vmid else (v - vmid) / (vmax - vmid)
                return tuple(c_mid * (1 - t) + c_max * t)

        # 8) ê° ìˆ«ìì—´ì˜ min/ì¤‘ì•™ê°’/ max ê³„ì‚°
        stats = {}
        for c in cols[1:]:
            series = pd.to_numeric(df[c], errors="coerce")
            if series.notna().any():
                vmin = float(series.min())
                vmax = float(series.max())
                vmid = float(series.quantile(0.5))
            else:
                vmin = vmid = vmax = None
            stats[c] = (vmin, vmid, vmax)

        # 9) ê·¸ë¦¬ë“œ(í—¤ë” + ë°”ë”” ì…€) ê·¸ë¦¬ê¸°
        # ì—´ x ì‹œì‘ì¢Œí‘œ ëˆ„ì 
        x_starts = np.cumsum([0] + col_widths[:-1]).tolist()
        # í—¤ë”
        for j, c in enumerate(cols):
            x = x_starts[j]
            ## í‘œ í…Œë‘ë¦¬
            # linewith = í‘œ í…Œë‘ë¦¬ êµµê¸°
            rect = Rectangle((x, 0), col_widths[j], header_h,
                            facecolor=header_bg, edgecolor=border_color, linewidth=0.5)
            ax.add_patch(rect)
            ax.text(x + col_widths[j] / 2, header_h / 2 + 1,
                    c, ha="center", va="center", fontsize=5, fontweight="bold")

        # ë°”ë””
        for i in range(n_rows):
            y = header_h + i * row_h
            for j, c in enumerate(cols):
                x = x_starts[j]
                # ë°°ê²½ìƒ‰ (monthëŠ” ì¡°ê±´ë¶€ì„œì‹ ì œì™¸, ìˆ«ìì—´ì—ë§Œ ì ìš©)
                if j == 0:
                    bg = (1, 1, 1)
                else:
                    raw_val = pd.to_numeric(df.iloc[i, j], errors="coerce")
                    vmin, vmid, vmax = stats[c]
                    bg = interp_color(raw_val, vmin, vmid, vmax)

                rect = Rectangle((x, y), col_widths[j], row_h,
                                facecolor=bg, edgecolor=border_color, linewidth=0.5)
                ax.add_patch(rect)

                # í…ìŠ¤íŠ¸
                text = str(display_df.iloc[i, j])
                if j == 0:
                    # month: ì¢Œì¸¡ ì •ë ¬ + ì¢Œìš° íŒ¨ë”©
                    ax.text(x + 8, y + row_h / 2,
                            text, ha="left", va="center", fontsize=5)
                else:
                    # ìˆ«ì: ìš°ì¸¡ ì •ë ¬
                    ax.text(x + col_widths[j] - 8, y + row_h / 2,
                            text, ha="right", va="center", fontsize=5)

        # í—¤ë” ë°”ë¡œ ìœ„ì— ì œëª© ì¶”ê°€ (ì™¼ìª½ì •ë ¬)
        ax.text(0, -5,  "ê°€ì…ì—°ë„ë³„ ì›” ë§¤ì¶œ",
                ha="left", va="bottom", fontsize=8, fontweight="bold")

        # 10) ì´ë¯¸ì§€ ì €ì¥
        plt.savefig(out_path, bbox_inches="tight", pad_inches=0.2)
        plt.close(fig)
        
        blob = bucket.blob(f'{gameidx}/{out_path}')
        blob.upload_from_filename(out_path, content_type='image/png')

        # ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        os.remove(out_path)

        return f'{gameidx}/{out_path}'
    

########### ì¥ê¸°ì  ë§¤ì¶œ í˜„í™© ì—…ë¡œë“œ to ë…¸ì…˜
def longterm_rev_upload_notion(joyplegameid: int, gameidx:str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "5. ì¥ê¸° ë§¤ì¶œ íŠ¸ë Œë“œ" }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(1) ì¼í‰ê·  ë§¤ì¶œ" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/filePath5_dailyAvgRevenue.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'filePath5_dailyAvgRevenue.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise

    query_result5_dailyAvgRevenue = context['task_instance'].xcom_pull(
        task_ids = 'monthly_day_average_rev',
        key='monthly_day_average_rev'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result5_dailyAvgRevenue,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì›”ë³„ ì¼í‰ê·  ë§¤ì¶œ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(monthly_day_average_rev_gemini(joyplegameid, service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


########### ì›”ë³„ Rê·¸ë£¹ë³„ ë§¤ì¶œ PU ìˆ˜
def longterm_rev_upload_notion(joyplegameid: int, gameidx:str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )


    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(2) ì›”ë³„ Rê·¸ë£¹ë³„ í˜„í™© " }}]
                },
            }
        ],
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": " ** ë‹¹ì›” ê³¼ê¸ˆì•¡ ê¸°ì¤€ Rê·¸ë£¹ ì…ë‹ˆë‹¤. " }}]
                },
            }
        ],
    )

    
    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/filePath5_monthlyRgroupHap.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'filePath5_monthlyRgroupHap.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result5_monthlyRgroupRevenueALL = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_total',
        key='rgroup_rev_total'
    )

    query_result5_monthlyRgroupRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rgroup_rev_DOD',
        key='rgroup_rev_DOD'
    )


    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id= PAGE_INFO['id'],
        df=query_result5_monthlyRgroupRevenueALL,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì›”ë³„ Rê·¸ë£¹ ë§¤ì¶œ(ì „ì²´ê¸°ê°„) ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id= PAGE_INFO['id'],
        df=query_result5_monthlyRgroupRevenue,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì›”ë³„ Rê·¸ë£¹ ë§¤ì¶œ(ë™ê¸°ê°„) ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": " ** ë™ê¸°ê°„ Rê·¸ë£¹ ë¹„êµì— ëŒ€í•œ í•´ì„ì…ë‹ˆë‹¤.  \n " }}]
                },
            }
        ],
    )

    ## í”„ë¡¬í”„íŠ¸
    blocks = md_to_notion_blocks(rgroup_rev_total_gemini(joyplegameid, service_sub, **context))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True


############## ê°€ì…ì—°ë„ ë§¤ì¶œ ë°ì´í„° 
def cohort_rev_upload_notion(joyplegameid:int, gameidx:str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": "\n(3) ê°€ì…ì—°ë„ë³„ ë§¤ì¶œ " }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/file_path5_regyearRevenue.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'file_path5_regyearRevenue.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query_result5_regyearRevenue = context['task_instance'].xcom_pull(
        task_ids = 'rev_cohort_year',
        key='rev_cohort_year_original'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query_result5_regyearRevenue,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ê°€ì…ì—°ë„ë³„ ë§¤ì¶œ ",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(rev_cohort_year_gemini(joyplegameid, service_sub, **context))
    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True

## ì‹ ê·œ ìœ ì € íšŒìˆ˜ í˜„í™©
## 6_newuser_roas

def result6_monthlyROAS(joyplegameid:int, gameidx:str, databaseschema:str, **context):
    
    query= """
    WITH revraw AS(
    select JoypleGameID, Month
    , concat("D_",date_diff(date_sub(current_date('Asia/Seoul'), interval 1 day), date_sub(cast(concat(Month, '-01') as date), interval 1 day), day)) as matured_daydiff
    ,sum(RU) as RU,
    sum(Sales_D1) as sales_D1,
    sum(Sales_D3) as sales_D3,
    sum(Sales_D7) as sales_D7,
    CASE WHEN COUNTIF(sales_D14 IS NULL) >= 1 THEN null ELSE sum(Sales_D14) END as Sales_D14,
    CASE WHEN COUNTIF(Sales_D30 IS NULL) >= 1 THEN null ELSE sum(Sales_D30) END as Sales_D30,
    CASE WHEN COUNTIF(Sales_D60 IS NULL) >= 1 THEN null ELSE sum(Sales_D60) END as Sales_D60,
    CASE WHEN COUNTIF(Sales_D90 IS NULL) >= 1 THEN null ELSE sum(Sales_D90) END as Sales_D90,
    CASE WHEN COUNTIF(Sales_D120 IS NULL) >= 1 THEN null ELSE sum(Sales_D120) END as Sales_D120,
    CASE WHEN COUNTIF(Sales_D150 IS NULL) >= 1 THEN null ELSE sum(Sales_D150) END as Sales_D150,
    CASE WHEN COUNTIF(Sales_D180 IS NULL) >= 1 THEN null ELSE sum(Sales_D180) END as Sales_D180,
    CASE WHEN COUNTIF(Sales_D210 IS NULL) >= 1 THEN null ELSE sum(Sales_D210) END as Sales_D210,
    CASE WHEN COUNTIF(Sales_D240 IS NULL) >= 1 THEN null ELSE sum(Sales_D240) END as Sales_D240,
    CASE WHEN COUNTIF(Sales_D270 IS NULL) >= 1 THEN null ELSE sum(Sales_D270) END as Sales_D270,
    CASE WHEN COUNTIF(Sales_D300 IS NULL) >= 1 THEN null ELSE sum(Sales_D300) END as Sales_D300,
    CASE WHEN COUNTIF(Sales_D330 IS NULL) >= 1 THEN null ELSE sum(Sales_D330) END as Sales_D330,
    CASE WHEN COUNTIF(Sales_D360 IS NULL) >= 1 THEN null ELSE sum(Sales_D360) END as Sales_D360,
    from(
    select JoypleGameID, RegdateAuthAccountDateKST,
    FORMAT_DATE('%Y-%m' ,RegdateAuthAccountDateKST) as Month,
    sum(RU) as RU,
    IFNULL(sum(rev_D1),0) as Sales_D1,
    IFNULL(sum(rev_D3),0) as Sales_D3,
    IFNULL(sum(rev_D7),0) as Sales_D7,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -15 DAY) AS Date)  THEN  IFNULL(sum(rev_D14),0) ELSE  null END as Sales_D14,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -31 DAY) AS Date)  THEN  IFNULL(sum(rev_D30),0) ELSE  null END as Sales_D30,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -61 DAY) AS Date)  THEN  IFNULL(sum(rev_D60),0) ELSE  null END as Sales_D60,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -91 DAY) AS Date)  THEN  IFNULL(sum(rev_D90),0) ELSE  null END as Sales_D90,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -121 DAY) AS Date)  THEN  IFNULL(sum(rev_D120),0) ELSE  null END as Sales_D120,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -151 DAY) AS Date)  THEN  IFNULL(sum(rev_D150),0) ELSE  null END as Sales_D150,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -181 DAY) AS Date)  THEN  IFNULL(sum(rev_D180),0) ELSE  null END as Sales_D180,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -211 DAY) AS Date)  THEN  IFNULL(sum(rev_D210),0) ELSE  null END as Sales_D210,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -241 DAY) AS Date)  THEN  IFNULL(sum(rev_D240),0) ELSE  null END as Sales_D240,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -271 DAY) AS Date)  THEN  IFNULL(sum(rev_D270),0) ELSE  null END as Sales_D270,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -301 DAY) AS Date)  THEN  IFNULL(sum(rev_D300),0) ELSE  null END as Sales_D300,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -331 DAY) AS Date)  THEN  IFNULL(sum(rev_D330),0) ELSE  null END as Sales_D330,
    CASE WHEN RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -361 DAY) AS Date)  THEN  IFNULL(sum(rev_D360),0) ELSE  null END as Sales_D360
    from `dataplatform-reporting.DataService.T_0420_0000_UAPerformanceRaw_V1`
        where JoypleGameID = 133
        and RegdateAuthAccountDateKST >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
        and RegdateAuthAccountDateKST <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    group by JoypleGameID, RegdateAuthAccountDateKST
    ) group by JoypleGameID, month
    )


    , final AS(
    select  JoypleGameID,  Month, matured_daydiff, RU,
    Sales_D1/RU as D1_LTV,
    Sales_D3/RU as D3_LTV,
    Sales_D7/RU as D7_LTV,
    Sales_D14/RU as D14_LTV,
    Sales_D30/RU as D30_LTV,
    Sales_D60/RU as D60_LTV,
    Sales_D90/RU as D90_LTV,
    Sales_D120/RU as D120_LTV,
    Sales_D150/RU as D150_LTV,
    Sales_D180/RU as D180_LTV,
    Sales_D210/RU as D210_LTV,
    Sales_D240/RU as D240_LTV,
    Sales_D270/RU as D270_LTV,
    Sales_D300/RU as D300_LTV,
    Sales_D330/RU as D330_LTV,
    Sales_D360/RU as D360_LTV,
    Sales_D14_p/RU as D14_LTV_p,
    Sales_D30_p/RU as D30_LTV_p,
    Sales_D60_p/RU as D60_LTV_p,
    Sales_D90_p/RU as D90_LTV_p,
    Sales_D120_p/RU as D120_LTV_p,
    Sales_D150_p/RU as D150_LTV_p,
    Sales_D180_p/RU as D180_LTV_p,
    Sales_D210_p/RU as D210_LTV_p,
    Sales_D240_p/RU as D240_LTV_p,
    Sales_D270_p/RU as D270_LTV_p,
    Sales_D300_p/RU as D300_LTV_p,
    Sales_D330_p/RU as D330_LTV_p,
    Sales_D360_p/RU as D360_LTV_p,
    D1D3_avg,  D3D7_avg , Sales_D7/RU as kpi_d7
    from(
    select *,
    CASE WHEN Sales_D14 is not null then null  ELSE  Sales_D7*D7D14_avg END as Sales_D14_p,
    CASE WHEN Sales_D30 is not null then null
    WHEN Sales_D30 is null and Sales_D14 is not null THEN Sales_D14*D14D30_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg END as Sales_D30_p,
    CASE WHEN Sales_D60 is not null then null
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg END as Sales_D60_p,
    CASE WHEN Sales_D90 is not null then null
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg END as Sales_D90_p,
    CASE WHEN Sales_D120 is not null then null
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg  END as Sales_D120_p,
    CASE WHEN Sales_D150 is not null then null
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg   END as Sales_D150_p,
    CASE WHEN Sales_D180 is not null then null
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg    END as Sales_D180_p,
    CASE WHEN Sales_D210 is not null then null
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg     END as Sales_D210_p,
    CASE WHEN Sales_D240 is not null then null
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg      END as Sales_D240_p,
    CASE WHEN Sales_D270 is not null then null
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg       END as Sales_D270_p,
    CASE WHEN Sales_D300 is not null then null
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg        END as Sales_D300_p,
    CASE WHEN Sales_D330 is not null then null
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg         END as Sales_D330_p,
    CASE WHEN Sales_D360 is not null then null
    WHEN Sales_D330 is not null THEN Sales_D330*D330D360_avg
    WHEN Sales_D300 is not null THEN Sales_D300*D300D330_avg *D330D360_avg
    WHEN Sales_D270 is not null THEN Sales_D270*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D240 is not null THEN Sales_D240*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D210 is not null THEN Sales_D210*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D180 is not null THEN Sales_D180*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D150 is not null THEN Sales_D150*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg    *D330D360_avg
    WHEN Sales_D120 is not null THEN Sales_D120*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg  *D330D360_avg
    WHEN Sales_D90 is not null THEN Sales_D90*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg *D330D360_avg
    WHEN Sales_D60 is not null THEN Sales_D60*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg   *D330D360_avg
    WHEN Sales_D30 is not null THEN Sales_D30*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    WHEN Sales_D14 is not null  THEN Sales_D14*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg     *D330D360_avg
    ELSE Sales_D7*D7D14_avg*D14D30_avg*D30D60_avg*D60D90_avg*D90D120_avg*D120D150_avg*D150D180_avg*D180D210_avg*D210D240_avg*D240D270_avg*D270D300_avg*D300D330_avg*D330D360_avg          END as Sales_D360_p
    from(

    select *
    ,  LAST_VALUE(d1d3_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D1D3_avg
    ,  LAST_VALUE(d3d7_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D3D7_avg
    ,  LAST_VALUE(d7d14_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)  as D7D14_avg
    ,  LAST_VALUE(d14d30_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D14D30_avg
    ,  LAST_VALUE(d30d60_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D30D60_avg
    ,  LAST_VALUE(d60d90_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as D60D90_avg
    ,  LAST_VALUE(d90d120_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d90d120_avg
    ,  LAST_VALUE(d120d150_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d120d150_avg
    ,  LAST_VALUE(d150d180_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d150d180_avg
    ,  LAST_VALUE(d180d210_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d180d210_avg
    ,  LAST_VALUE(d210d240_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d210d240_avg
    ,  LAST_VALUE(d240d270_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d240d270_avg
    ,  LAST_VALUE(d270d300_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d270d300_avg
    ,  LAST_VALUE(d300d330_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d300d330_avg
    ,  LAST_VALUE(d330d360_avg3 IGNORE NULLS ) over(partition by joyplegameid ORDER BY month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as d330d360_avg

    from(
    select *,
    CASE WHEN Sales_D3 is null THEN null ELSE AVG(Sales_D3/Sales_D1) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING ) END AS d1d3_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©
    CASE WHEN Sales_D7 is null THEN null ELSE AVG(Sales_D7/Sales_D3) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 3 PRECEDING AND  1 PRECEDING) END AS d3d7_avg3, -- í˜„ì¬ì›”ì œì™¸ kpiê³„ì‚°ìš©

    CASE WHEN Sales_D14 is null THEN null ELSE AVG(Sales_D14/Sales_D7) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d7d14_avg3,
    CASE WHEN Sales_D30 is null THEN null ELSE AVG(Sales_D30/Sales_D14) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d14d30_avg3,
    CASE WHEN Sales_D60 is null THEN null ELSE AVG(Sales_D60/Sales_D30) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d30d60_avg3,
    CASE WHEN Sales_D90 is null THEN null ELSE AVG(Sales_D90/Sales_D60) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d60d90_avg3,
    CASE WHEN Sales_D120 is null THEN null ELSE AVG(Sales_D120/Sales_D90) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d90d120_avg3,
    CASE WHEN Sales_D150 is null THEN null ELSE AVG(Sales_D150/Sales_D120) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d120d150_avg3,
    CASE WHEN Sales_D180 is null THEN null ELSE AVG(Sales_D180/Sales_D150) OVER (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d150d180_avg3,
    CASE WHEN Sales_D210 is null THEN null ELSE AVG(Sales_D210/Sales_D180) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d180d210_avg3,
    CASE WHEN Sales_D240 is null THEN null ELSE AVG(Sales_D240/Sales_D210) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d210d240_avg3,
    CASE WHEN Sales_D270 is null THEN null ELSE AVG(Sales_D270/Sales_D240) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d240d270_avg3,
    CASE WHEN Sales_D300 is null THEN null ELSE AVG(Sales_D300/Sales_D270) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d270d300_avg3,
    CASE WHEN Sales_D330 is null THEN null ELSE AVG(Sales_D330/Sales_D300) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW ) END AS d300d330_avg3,
    CASE WHEN Sales_D360 is null THEN null ELSE AVG(Sales_D360/Sales_D330) OVER  (partition by joyplegameid ORDER BY month ROWS BETWEEN 2 PRECEDING AND  CURRENT ROW )END AS d330d360_avg3
    from revraw
    )
    )
    )
    )

    ,final2 AS(
    select a.*, b.cost, b.cost_exclude_credit
    from final as a
    left join (
    select joyplegameid,  format_date('%Y-%m', cmpgndate) as month
    , sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
    from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
    where joyplegameid = 133
    and cmpgndate >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    and cmpgndate <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    group by joyplegameid,  format_date('%Y-%m', cmpgndate)

    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.month = b.month
    )

    select month as `ê°€ì…ì›”`,
    matured_daydiff as `ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„`,
    cost_exclude_credit as `ë§ˆì¼€íŒ… ë¹„ìš©`,
    ru*d1_ltv/cost_exclude_credit as `ROAS D1`,
    ru*d3_ltv/cost_exclude_credit as `ROAS D3`,
    ru*d7_ltv/cost_exclude_credit as `ROAS D7`,
    ru*d14_ltv/cost_exclude_credit as `ROAS D14`,
    ru*d30_ltv/cost_exclude_credit as `ROAS D30`,
    ru*d60_ltv/cost_exclude_credit as `ROAS D60`,
    ru*d90_ltv/cost_exclude_credit as `ROAS D90`,
    ru*d120_ltv/cost_exclude_credit as `ROAS D120`,
    ru*d150_ltv/cost_exclude_credit as `ROAS D150`,
    ru*d180_ltv/cost_exclude_credit as `ROAS D180`,
    ru*d210_ltv/cost_exclude_credit as `ROAS D210`,
    ru*d240_ltv/cost_exclude_credit as `ROAS D240`,
    ru*d270_ltv/cost_exclude_credit as `ROAS D270`,
    ru*d300_ltv/cost_exclude_credit as `ROAS D300`,
    ru*d330_ltv/cost_exclude_credit as `ROAS D330`,
    ru*d360_ltv/cost_exclude_credit as `ROAS D360`,
    ru*d14_ltv_p/cost_exclude_credit as `ROAS D14 ì˜ˆì¸¡ì¹˜`,
    ru*d30_ltv_p/cost_exclude_credit as `ROAS D30 ì˜ˆì¸¡ì¹˜`,
    ru*d60_ltv_p/cost_exclude_credit as `ROAS D60 ì˜ˆì¸¡ì¹˜`,
    ru*d90_ltv_p/cost_exclude_credit as `ROAS D90 ì˜ˆì¸¡ì¹˜`,
    ru*d120_ltv_p/cost_exclude_credit as `ROAS D120 ì˜ˆì¸¡ì¹˜`,
    ru*d150_ltv_p/cost_exclude_credit as `ROAS D150 ì˜ˆì¸¡ì¹˜`,
    ru*d180_ltv_p/cost_exclude_credit as `ROAS D180 ì˜ˆì¸¡ì¹˜`,
    ru*d210_ltv_p/cost_exclude_credit as `ROAS D210 ì˜ˆì¸¡ì¹˜`,
    ru*d240_ltv_p/cost_exclude_credit as `ROAS D240 ì˜ˆì¸¡ì¹˜`,
    ru*d270_ltv_p/cost_exclude_credit as `ROAS D270 ì˜ˆì¸¡ì¹˜`,
    ru*d300_ltv_p/cost_exclude_credit as `ROAS D300 ì˜ˆì¸¡ì¹˜`,
    ru*d330_ltv_p/cost_exclude_credit as `ROAS D330 ì˜ˆì¸¡ì¹˜`,
    ru*d360_ltv_p/cost_exclude_credit as `ROAS D360 ì˜ˆì¸¡ì¹˜`
    from final2
    order by `ê°€ì…ì›”`
    """

    query_result6_monthlyROAS =query_run_method('6_newuser_roas', query)
    query_result6_monthlyROAS['ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„'] = (
    query_result6_monthlyROAS['ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„'].astype(str).str.replace('_', '', regex=False)
    )

    context['task_instance'].xcom_push(key='result6_monthlyROAS', value=query_result6_monthlyROAS)

    return True

def result6_pLTV(joyplegameid:int, gameidx:str, databaseschema:str, **context):

    ## pLTV D360
    query = f"""

    with perfo_raw AS(
    select a.*
    , b.countrycode, b.os
    , b.gcat, b.mediacategory, b.class, b.media, b.adsetname, b.adname, b.optim, b.oscam, b.geocam, b.targetgroup
    from(
    select *,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_low end as combined_rev_low,
    case when logdatekst < current_date('Asia/Seoul') then pricekrw else daypred_upp end as combined_rev_upp,
    FROM `data-science-division-216308.VU.Performance_pLTV`
    where authaccountregdatekst >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    and authaccountregdatekst <= CAST(DATE_ADD(CURRENT_DATE('Asia/Seoul'), INTERVAL -8 DAY) AS Date)
    and JoypleGameID = 133
    ) as a
    left join (select  *
    from `dataplatform-reporting.DataService.V_0316_0000_AuthAccountInfo_V`
    where JoypleGameID = 133
    ) as b
    on a.authaccountname = b.authaccountname
    and a.joyplegameid = b.joyplegameid
    )

    select format_date('%Y-%m',AuthAccountRegDateKST) as `ê°€ì…ì›”`
        ,count(distinct if(daysfromregisterdate = 0, authaccountname, null)) as RU
        ,sum(if(daysfromregisterdate <= 360, combined_rev, null)) as `ë§¤ì¶œ D360 ì˜ˆì¸¡ì¹˜`
        , max(authaccountregdatekst) as `ìµœëŒ€ ê°€ì…ì¼ì`
    from perfo_raw
    group by joyplegameid, format_date('%Y-%m',AuthAccountRegDateKST)

    """
    query_result6_pLTV =query_run_method('6_newuser_roas', query)

    context['task_instance'].xcom_push(key='result6_pLTV', value=query_result6_pLTV)

    return True

##### ë³µê·€ ìœ ì € ë°ì´í„°
def result6_return(joyplegameid:int, gameidx:str, databaseschema:str, **context):

    query = f"""
    with raw AS(
    select *
    , sum(d90diff) over(partition by joyplegameid, authaccountname order by logdatekst) as cum_d90diff
    from(
    select *
    , date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day ) as daydiff_beforeaccess   -- authaccountlastaccessbeforedatekst : Access ê¸°ì¤€ìœ¼ë¡œ ë¡œê¹…
    , case when  date_diff(logdatekst,AuthAccountLastAccessBeforeDateKST, day )  >= 90 then 1 else 0  end as d90diff
    FROM `dataplatform-reporting.DataService.T_0317_0000_AuthAccountPerformance_V`
    WHERE joyplegameid = 133
    and logdatekst >= '2023-01-01'
    and DaysFromRegisterDate >= 0 -- ê°€ì…ì¼ì´ ì´í›„ì— ì°íŒ caseì œì™¸
    )
    )

    , raw2 AS(
    select *, date_diff(logdatekst, returndate, day) as daydiff_re -- ë³µê·€ì¼ cohort
    -- , if(returndate = AuthAccountRegDateKST, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    , if(cum_d90diff = 0, 0,1) as return_yn -- ê°€ì…ì¼ì´ ë¨¼ì € ì°íŒ case í¬í•¨
    from(
    select *
    , first_value(logdatekst) over(partition by joyplegameid, authaccountname, cum_d90diff order by logdatekst) as returndate
    from raw
    )
    )

    , ru_raw AS(
    -- ì‹ ê·œ ìœ ì € ê¸°ì¤€
    select joyplegameid,  format_date('%Y-%m',authaccountregdatekst) as regmonth
    , count(distinct authaccountname) as ru
    , sum(if(DaysFromRegisterDate<=360, pricekrw, null)) as d360rev
    from raw2
    where  AuthAccountRegDateKST  >= '2023-01-01'
    and AuthAccountRegDateKST <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)

    group by joyplegameid,    format_date('%Y-%m',authaccountregdatekst)
    )

    , return_raw AS(
    -- ë³µê·€ìœ ì €
    select joyplegameid,  format_date('%Y-%m', returndate) as regmonth
    , count(distinct if(daydiff_re = 0 , authaccountname, null)) as ru
    , sum(if(daydiff_re<=360, pricekrw, null)) as d360rev_all
    from raw2
    where  returndate  >= '2023-01-01'
    and returndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
    group by joyplegameid,   format_date('%Y-%m', returndate)
    )


    ,final AS(
    select
    ifnull(ifnull(a.joyplegameid , b.joyplegameid) , c.joyplegameid)  as joyplegameid
    ,ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) as regmonth
    , a.ru ,b.ru as ru_all,
    d360rev  AS rev_D360,
    d360rev_all  AS rev_D360_all
    ,  cost
    ,  cost_exclude_credit
    , d360rev_all - d360rev as rev_D360_return ,
    case WHEN DATE_DIFF(
            DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY),
            LAST_DAY(DATE(CONCAT(ifnull(ifnull(a.regmonth , b.regmonth)  , c.regmonth) , '-01'))),
            DAY
            ) >= 360 THEN 'mature'
        ELSE 'notmature'
    END AS status
    from ru_raw  as a
    full join return_raw as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    full join (
            select joyplegameid,  format_date('%Y-%m',cmpgndate) as regmonth, sum(costcurrency) as cost, sum(costcurrencyuptdt) as cost_exclude_credit
            from  `dataplatform-reporting.DataService.V_0410_0000_CostCampaignRule_V`
            where joyplegameid = 133
            and cmpgndate >='2023-01-01'
            and cmpgndate <= DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 8 DAY)
            group by  joyplegameid,  format_date('%Y-%m',cmpgndate)
            ) as c
            on a.joyplegameid = c.joyplegameid
    and a.regmonth = c.regmonth
    )

    #notmature êµ¬ê°„ ìµœê·¼ 6ê°œì›” í‰ê· 
    #POTC 2024/4 ~ 6ì›” ë¡œê·¸ì¸ ì´ìŠˆë¡œ ë³µê·€ìœ ì € ê¸°ì—¬ë„ ë‚®ì•„ ì œì™¸
    , return_user_proas AS(
    select joyplegameid, avg(rev_D360_return/cost_exclude_credit) as d360_return_roas
    , approx_quantiles(rev_D360_return/cost_exclude_credit, 2)[OFFSET(1)] as d360_return_roas_med
    from(
    select *, row_number() over (partition by joyplegameid order by regmonth desc) as rownum
    from final
    where status = 'mature'
    and (
        -- (joyplegameid = 131 and regmonth not in ('2024-04','2024-05','2024-06')) or joyplegameid in (133,30001,30003)
        joyplegameid = 133
        )
    )
    where rownum <= 6 -- ìµœê·¼ 6ê°œì›”
    group by joyplegameid
    )

    select a.regmonth as `ê°€ì…ì›”`
        , a.RU
        , a.RU_all
        , a.cost_exclude_credit as `ë§ˆì¼€íŒ… ë¹„ìš©`
        , rev_D360_return as `ë³µê·€ìœ ì € ë§¤ì¶œ D360`
        , status as `ë°ì´í„° ì™„ì„± ì—¬ë¶€`
    , rev_D360_return/cost_exclude_credit as `ë³µê·€ìœ ì € ROAS D360`
    , case when status = 'mature' then rev_D360_return/cost_exclude_credit
        else b.d360_return_roas_med end as `ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜`
    from final  as a
    left join return_user_proas as b
    on a.joyplegameid = b.joyplegameid
    """

    query_result6_return =query_run_method('6_newuser_roas', query)

    context['task_instance'].xcom_push(key='result6_return', value=query_result6_return)

    return True

### ìˆ˜ìˆ˜ë£Œ ì ìš© BEP ê³„ì‚°
def result6_BEP(joyplegameid:int, gameidx:str, databaseschema:str, **context):

    query = f"""
    with raw AS(
    select a.*, b.value
    , case when b.value is not null then b.value
    when b.value is null and a.PGName = 'Google' then 0.3
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 131 then 0.33
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 133 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30001 then 0.32
    when b.value is null and a.PGName = 'Apple' and a.joyplegameid = 30003 then 0.31
    when b.value is null and a.PGName = 'Xsolla' and a.joyplegameid = 131 then 0.15
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 133 then 0.10
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30001 then 0.09
    when b.value is null and  a.PGName = 'Xsolla' and a.joyplegameid = 30003 then 0.08
    when b.value is null and  a.PGName = 'Danal' then 0.03
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 131 then 0.3
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 133 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30001 then 0.24
    when b.value is null and  a.PGName = 'One Store' and a.joyplegameid = 30003 then 0.24
    when b.value is null and  a.PGName = 'Facebook Gaming' then 0.3
    when b.value is null and a.PGName = 'Steam' then 0.3
    else 0.3
    end as commission_rate
    from(
    select JoypleGameID, format_date('%Y-%m', authaccountregdatekst) as regmonth , t2.PGName, sum(t2.PGPriceKRW) as sales
    from  dataplatform-reporting.DataService.V_0317_0000_AuthAccountPerformance_V AS t1,
    UNNEST(t1.PaymentDetailArrayStruct) AS t2
    where joyplegameid = 133
    and authaccountregdatekst >= DATE_SUB(DATE(CONCAT(FORMAT_DATE('%Y-%m', DATE_SUB(CURRENT_DATE('Asia/Seoul'), INTERVAL 1 DAY)),'-01')), INTERVAL 24 MONTH)
    group by JoypleGameID, format_date('%Y-%m', authaccountregdatekst) , t2.PGName
    ) as a
    left join (
    select joyplegameid, regmonth, PGName,  value
    , case when PGName = 'One_Store' then 'One Store'
    when PGName = 'Facebook_Gaming' then 'Facebook Gaming'
    else PGName end as pgname2
    from `data-science-division-216308.Common.pg_commission_rate_copy2`
    unpivot (
    value for PGName in (Google,Apple, Xsolla	,Danal,	One_Store	,Facebook_Gaming,	Steam)
    )
    ) as b
    on a.joyplegameid = b.joyplegameid
    and a.regmonth = b.regmonth
    and a.pgname = b.pgname2
    )

    -- BEP ê³„ì‚°

    select distinct regmonth as `ê°€ì…ì›”`, bep_commission as `ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP`
    from (
        select * , sum(sales) over(partition by joyplegameid , regmonth) as cumsales
                , sales /  sum(sales) over(partition by joyplegameid , regmonth) as sales_p
                , sum(commission) over(partition by joyplegameid , regmonth) as cumcommission
                , sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth) as total_commssion_rate
                , case when joyplegameid in (131,133) then 1/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
                        when joyplegameid in (30001,30003) then 1.08/(1- sum(commission) over(partition by joyplegameid , regmonth) /  sum(sales) over(partition by joyplegameid , regmonth))
                    end as bep_commission
        from(
            select  *, sales*commission_rate as commission
            from raw
            )
        )
    order by `ê°€ì…ì›”`
    """

    query_result6_BEP =query_run_method('6_newuser_roas', query)

    context['task_instance'].xcom_push(key='result6_BEP', value=query_result6_BEP)

    return True


### ROAS KPI
def result6_roaskpi(joyplegameid:int, gameidx:str, databaseschema:str, **context):

    query = f"""
    select kpi_d1, kpi_d3, kpi_d7, kpi_d14, kpi_d30, kpi_d60, kpi_d90, kpi_d120, kpi_d150, kpi_d180, kpi_d210, kpi_d240, kpi_d270, kpi_d300, kpi_d330, kpi_d360
    from
    (select * ,row_number() OVER (partition by project ORDER BY updateDate desc ) AS row_
    from `data-science-division-216308.MetaData.roas_kpi`
    where project='GBTW'
    and operationStatus = 'ìš´ì˜ ì¤‘')
    where row_=1

    """

    query_result6_roaskpi = query_run_method('6_newuser_roas', query)

    context['task_instance'].xcom_push(key='result6_roaskpi', value=query_result6_roaskpi)

    return True


def roas_kpi(joyplegameid:int, gameidx:str, databaseschema:str, **context):

    query_result6_roaskpi = context['task_instance'].xcom_pull(
        task_ids = 'result6_roaskpi',
        key='result6_roaskpi'
    )

    query_result6_roaskpi = query_result6_roaskpi * 100
    data = query_result6_roaskpi.rename(columns={
            'kpi_d1' : 'ROAS D1',
            'kpi_d3' : 'ROAS D3',
            'kpi_d7' : 'ROAS D7',
            'kpi_d14' : 'ROAS D14',
            'kpi_d30' : 'ROAS D30',
            'kpi_d60' : 'ROAS D60',
            'kpi_d90' : 'ROAS D90',
            'kpi_d120' : 'ROAS D120',
            'kpi_d150' : 'ROAS D150',
            'kpi_d180' : 'ROAS D180',
            'kpi_d210' : 'ROAS D210',
            'kpi_d240' : 'ROAS D240',
            'kpi_d270' : 'ROAS D270',
            'kpi_d300' : 'ROAS D300',
            'kpi_d330' : 'ROAS D330',
            'kpi_d360' : 'ROAS D360'
            })
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    roas_kpi = pd.DataFrame(data)

    context['task_instance'].xcom_push(key='roas_kpi', value=roas_kpi)

    return True

###
def roas_dataframe_preprocessing(**context):
    query_result6_monthlyROAS = context['task_instance'].xcom_pull(
        task_ids = 'result6_monthlyROAS',
        key='result6_monthlyROAS'
    )
    query_result6_pLTV = context['task_instance'].xcom_pull(
        task_ids = 'result6_pLTV',
        key='result6_pLTV'
    )
    query_result6_return = context['task_instance'].xcom_pull(
        task_ids = 'result6_return',
        key='result6_return'
    )
    query_result6_BEP = context['task_instance'].xcom_pull(
        task_ids = 'result6_BEP',
        key='result6_BEP'
    )

    query6_monthlyROAS = pd.merge(query_result6_monthlyROAS, query_result6_pLTV[['ê°€ì…ì›”', 'ë§¤ì¶œ D360 ì˜ˆì¸¡ì¹˜']], on = ['ê°€ì…ì›”'], how = "left")
    query6_monthlyROAS = pd.merge(query6_monthlyROAS
                                , query_result6_return[['ê°€ì…ì›”', 'ë³µê·€ìœ ì € ROAS D360', 'ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜', 'ë°ì´í„° ì™„ì„± ì—¬ë¶€']]
                                , on = ['ê°€ì…ì›”'], how = 'left')
    query6_monthlyROAS = pd.merge(query6_monthlyROAS, query_result6_BEP, on = ['ê°€ì…ì›”'], how = 'left')

    target_columns = ["ROAS D14","ROAS D30","ROAS D60","ROAS D90","ROAS D120",
            "ROAS D150","ROAS D180","ROAS D210","ROAS D240","ROAS D270",
            "ROAS D300","ROAS D330","ROAS D360"]

    ## ì‹¤ì¸¡ì¹˜ ì»¬ëŸ¼ ë¹ˆì¹¸ì— ì˜ˆì¸¡ì¹˜ ì»¬ëŸ¼ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    for col in target_columns:
        pred = f"{col} ì˜ˆì¸¡ì¹˜"
        if col in query6_monthlyROAS.columns and pred in query6_monthlyROAS.columns:
            query6_monthlyROAS[col] = query6_monthlyROAS[col].fillna(query6_monthlyROAS[pred])

    ## ì˜ˆì¸¡ì¹˜ ì»¬ëŸ¼ ì œì™¸(ë³µê·€ìœ ì € ì˜ˆì¸¡ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ë‘ê¸°)
    cols_to_drop = [
        c for c in query6_monthlyROAS.columns
        if ("ì˜ˆì¸¡ì¹˜" in c) and ("ë³µê·€ìœ ì €" not in c)
    ]
    query6_monthlyROAS = query6_monthlyROAS.drop(columns=cols_to_drop)

    ## ë³µê·€ìœ ì € í¬í•¨ D360 ROAS ê³„ì‚° -> mature ì•ˆëœ ê²½ìš° ì˜ˆì¸¡ì¹˜ë¡œ ê³„ì‚°
    mature_mask = query6_monthlyROAS['ë°ì´í„° ì™„ì„± ì—¬ë¶€'].eq('mature')

    query6_monthlyROAS['ë³µê·€ìœ ì € í¬í•¨ ROAS D360'] = np.where(
        mature_mask,
        query6_monthlyROAS['ROAS D360'] + query6_monthlyROAS['ë³µê·€ìœ ì € ROAS D360'],
        query6_monthlyROAS['ROAS D360'] + query6_monthlyROAS['ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜']
    )

    query6_monthlyROAS['ê¸°ë³¸ BEP'] = 1.429

    ## 2) ì»¬ëŸ¼ì„ 'ROAS D360' ë‹¤ìŒìœ¼ë¡œ ì´ë™
    cols = query6_monthlyROAS.columns.tolist()
    cols.remove('ë³µê·€ìœ ì € í¬í•¨ ROAS D360')
    cols.remove('ê¸°ë³¸ BEP')
    insert_at = cols.index('ROAS D360') + 1
    cols.insert(insert_at, 'ë³µê·€ìœ ì € í¬í•¨ ROAS D360')
    insert_at = cols.index('ë°ì´í„° ì™„ì„± ì—¬ë¶€') + 1
    cols.insert(insert_at, 'ê¸°ë³¸ BEP')
    cols.remove('ë°ì´í„° ì™„ì„± ì—¬ë¶€')
    insert_at = cols.index('ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP') + 1
    cols.insert(insert_at, 'ë°ì´í„° ì™„ì„± ì—¬ë¶€')
    query6_monthlyROAS = query6_monthlyROAS[cols]

    roas_days = [1, 3, 7, 14, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]

    # ì„±ì¥ì„¸ ì»¬ëŸ¼ ìƒì„±
    for i in range(1, len(roas_days)):
        prev_day = roas_days[i - 1]
        curr_day = roas_days[i]

        prev_col = f"ROAS D{prev_day}"
        curr_col = f"ROAS D{curr_day}"
        new_col = f"LTV ì„±ì¥ì„¸ D{curr_day}"

        query6_monthlyROAS[new_col] = query6_monthlyROAS[curr_col] / query6_monthlyROAS[prev_col]

    context['task_instance'].xcom_push(key='monthlyBEP_ROAS', value=query6_monthlyROAS)

    return True


########## ROAS í”„ë¡¬í”„íŠ¸
def result6_ROAS_gemini(**context):

    # KST íƒ€ì„ì¡´ ì •ì˜ (UTC+9)
    kst = timezone(timedelta(hours=9))

    # ì–´ì œ ë‚ ì§œ (KST ê¸°ì¤€)
    yesterday_kst = datetime.now(kst) - timedelta(days=1)

    # ì–´ì œ ë‚ ì§œì˜ ì—°ë„
    year = yesterday_kst.year

    #print("ì–´ì œ ë‚ ì§œ(KST):", yesterday_kst.date())
    #print("ì–´ì œ ì—°ë„:", year)

    query6_monthlyROAS = context['task_instance'].xcom_pull(
        task_ids = 'result6_BEP',
        key='monthlyBEP_ROAS'
    )

    roas_kpi = context['task_instance'].xcom_pull(
        task_ids = 'roas_kpi',
        key='roas_kpi'
    )

    response6_monthlyROAS = genai_client.models.generate_content(
        model=MODEL_NAME,
        contents = f"""
    < ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ROAS>
    {query6_monthlyROAS.to_csv(index=False)}
    ë‹¤ìŒì€ ê°€ì…ì›”ë³„ ROAS ì•¼. "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ ROASëŠ” ì˜ˆì¸¡ì¹˜ì´ë‹ˆ, "ì§€í‘œí™•ì •  ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ ROASë¥¼ ì–¸ê¸‰í•  ë•Œì—ëŠ” ì˜ˆì¸¡ì¹˜ë¼ê³  ë§í•´ì¤˜.
    ì–´ëŠ ê°€ì…ì›”ì´ "ë³µê·€ìœ ì € í¬í•¨ ROAS D360"ì´ KPIë¥¼ ë‹¬ì„±í–ˆëŠ”ì§€, ë˜ëŠ” ë‹¬ì„±í•˜ì§€ ëª»í–ˆëŠ”ì§€ë¥¼ ì„œë‘ì— Bold ì²´ë¡œ í•œì¤„ë¡œ ì–¸ê¸‰í•´ì¤˜.
    {year} ì—°ë„ë§Œ ì ì–´ì¤˜.

    KPI ëŠ” ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP ë¡œ íŒë‹¨í•˜ë©´ë¼.

    ê·¸ë¦¬ê³  "ë³µê·€ìœ ì € í¬í•¨ ROAS D360" ì´ KPI ë‹¬ì„±í•˜ì§€ ëª»í•œ ì˜¬í•´ ì›”ë“¤ì€
    ì•„ë˜ ROAS KPI ì™€ ë¹„êµí•´ì„œ ì–´ë–¤ ì½”í˜¸íŠ¸ë¶€í„° ë¯¸ë‹¬í•˜ì—¬ ë‹¬ì„±í•˜ì§€ ëª»í–ˆë‹¤ê³  ê°„ë‹¨íˆ ì•Œë ¤ì¤˜.
    ë‹¬ì„±í•˜ì§€ ëª»í•œ ì›”ë“¤ë§Œ ì–¸ê¸‰í•´ì¤˜.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    ROASì™€ KPI ìˆ˜ì¹˜ëŠ” ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ %ë¡œ í‘œì‹œí•´ì¤˜.

    <ROAS KPI>
    {roas_kpi}




    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION
            #,tools=[RAG]
            ,temperature=0.1
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )
    
    return response6_monthlyROAS.text


########## LTV ì„±ì¥ì„¸ ë¶€ë¶„ í”„ë¡¬í”„íŠ¸
def monthlyLTVgrowth_gemini(**context):

    query6_monthlyROAS = context['task_instance'].xcom_pull(
        task_ids = 'result6_BEP',
        key='monthlyBEP_ROAS'
    )

    response6_monthlyLTVgrowth = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""
    < ì›”ë³„ ë§ˆì¼€íŒ…ë¹„ìš©ê³¼ ROAS>
    {query6_monthlyROAS.to_csv(index=False)}
    ë‹¤ìŒì€ ê°€ì…ì›”ë³„ ROASì™€ LTV ì„±ì¥ì„¸ì•¼. "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ LTV ì„±ì¥ì„¸ëŠ” ì˜ˆì¸¡ì¹˜ì´ë‹ˆ, "ì§€í‘œí™•ì •  ìµœëŒ€ê¸°ê°„" ì´í›„ì˜ LTV ì„±ì¥ì„¸ë¥¼ ì–¸ê¸‰í•  ë•Œì—ëŠ” ì˜ˆì¸¡ì¹˜ë¼ê³  ë§í•´ì¤˜.
    ê°€ì…ì›”ì— ë”°ë¼ì„œ ì–´ëŠ êµ¬ê°„ì˜ ì„±ì¥ì„¸ê°€ ì¦ê°€í•˜ê±°ë‚˜, í•˜ë½í•˜ëŠ”ì§€ íŠ¸ë Œë“œë§Œ ì–¸ê¸‰í•´ì¤˜. e.g. D7 LTV ì„±ì¥ì„¸ê°€ 2025ë…„ 1ì›”ë¶€í„° í•˜ë½í–ˆìŠµë‹ˆë‹¤.
    êµ¬ê°„ì€ D3ë¶€í„° D30ê¹Œì§€ ì´ˆë°˜, D60ë¶€í„° D180ê¹Œì§€ ì¤‘ë°˜, D180ë¶€í„° D360ê¹Œì§€ëŠ” í›„ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê° êµ¬ê°„ì— ëŒ€í•œ íŠ¸ë Œë“œë¡œ ì–¸ê¸‰í•´ì¤˜.
    ê°€ì…ì›”ë³„ë¡œ í•˜ì§€ë§ê³  ê°€ì…ì›”ì— ë”°ë¥¸ ê° êµ¬ê°„ì˜ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•´ì„œ ì–¸ê¸‰í•˜ë˜, íŠ¹ì • ê°€ì…ì›”ì—ì„œ í¬ê²Œ ìƒìŠ¹í•˜ê±°ë‚˜ í¬ê²Œ í•˜ë½í–ˆë‹¤ë©´ ê·¸ ê°€ì…ì›”ì— ëŒ€í•´ì„œëŠ” ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì–¸ê¸‰í•´ì¤˜.
    LTV ì„±ì¥ì„¸ëŠ” ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ %ë¡œ í‘œì‹œí•´ì¤˜.
    10ì¤„ ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
    í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.
    """,
        config=types.GenerateContentConfig(
            system_instruction=SYSTEM_INSTRUCTION,
            # tools=[RAG],
            temperature=0.1
            ,labels=LABELS
            # max_output_tokens=2048
        )
    )
    return response6_monthlyLTVgrowth.text


### ROAS í˜„í™© ë° KPI í‘œ ì´ë¯¸ì§€ ìƒì„±

#### growth ì˜ˆì¸¡ì¹˜ íšŒìƒ‰ ìŒì˜ ë°˜ì˜
## ì˜ˆì¸¡ ê¸°ì¤€ dnì„ ê° row(regmonth)ë³„ë¡œ ì¶”ë¡ 
def infer_cohort_dn_map(df):
    cohort_map = {}
    for idx, row in df.iterrows():
        regmonth = idx[1] if isinstance(idx, tuple) else row.get('ê°€ì…ì›”', None)
        for col in df.columns:
            if re.search(r"ì˜ˆì¸¡ì¹˜$", col):
                if pd.notna(row[col]):
                    dn = int(re.findall(r'\d+', col)[0])
                    # ì˜ˆì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê°€ì¥ ì‘ì€ dn ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
                    if regmonth not in cohort_map or dn < cohort_map[regmonth]:
                        cohort_map[regmonth] = dn
    return cohort_map

## ì»¬ëŸ¼ ì´ë¦„ì—ì„œ dn ê°’ ì¶”ì¶œ
def extract_dn(col):
    match = re.match(r'(ROAS|LTV ì„±ì¥ì„¸) D(\d+)', col)
    return int(match.group(2)) if match else None

## ìŠ¤íƒ€ì¼ í•¨ìˆ˜ ì •ì˜ - mautred ë˜ì§€ ì•Šì€ êµ¬ê°„ íšŒìƒ‰ì²˜ë¦¬
def highlight_based_on_dn(row):
    regmonth = row['ê°€ì…ì›”']
    cohort_dn = cohort_dn_map.get(regmonth, np.inf)

    styles = []
    for col in row.index:
        clean_col = col.replace("<br>", " ").strip()

        # ë‘ ìˆ«ìê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ dnìœ¼ë¡œ
        match = re.findall(r'D(\d+)', clean_col)
        dn_val = int(match[-1]) if match else None

        if (
            (clean_col.startswith('ROAS D') or clean_col.startswith('LTV ì„±ì¥ì„¸'))
            and dn_val is not None
            and dn_val >= cohort_dn
            and pd.notna(row[col])
        ):
            styles.append('background-color: lightgray')
        else:
            styles.append('')
    return styles

#### roas ë‹¬ì„± êµ¬ê°„ ë¹¨ê°„ìƒ‰ ìŒì˜
def highlight_roas_vs_bep(row):
    styles = []
    for col in row.index:
        style = ""
        try:
            # ê°’ ë³€í™˜
            if isinstance(row[col], str) and row[col].endswith('%'):
                roas_val = float(row[col].replace('%', '')) / 100
            elif isinstance(row[col], (int, float)):
                roas_val = row[col]
            else:
                roas_val = None

            # ê¸°ì¤€ bep_base ë¹„êµ
            if col.startswith("ROAS D") and pd.notnull(row.get("ê¸°ë³¸<br>BEP")):
                bep_val = row["ê¸°ë³¸<br>BEP"]
                if pd.notnull(roas_val) and roas_val > bep_val:
                    style = "background-color: #fbe4e6"

            # d360 plus ë¹„êµ vs bep_commission
            elif col == "ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360" and pd.notnull(row.get("ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP")):
                bep_comm = row["ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP"]
                if pd.notnull(roas_val) and roas_val > bep_comm:
                    style = "background-color: #fbe4e6"

        except Exception as e:
            print(f"[DEBUG] {col} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            style = ""

        styles.append(style)
    return styles

def roas_table_draw(**context):

    query6_monthlyROAS = context['task_instance'].xcom_pull(
        task_ids = 'result6_BEP',
        key='monthlyBEP_ROAS'
    )

    query_result6_monthlyROAS = context['task_instance'].xcom_pull(
        task_ids = 'result6_monthlyROAS',
        key='result6_monthlyROAS'
    )

    df_numeric = query6_monthlyROAS.drop(columns=['ë°ì´í„° ì™„ì„± ì—¬ë¶€']).copy()
    df_numeric = df_numeric.reset_index(drop=True)

    nest_asyncio.apply()

    cohort_dn_map = infer_cohort_dn_map(query_result6_monthlyROAS)

    # dn_valuesëŠ” <br> ì—†ëŠ” clean ì»¬ëŸ¼ëª… ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    dn_values = {col: extract_dn(col) for col in query6_monthlyROAS.columns if col.startswith("ROAS D") or col.startswith("LTV ì„±ì¥ì„¸ D")}

    # ê°œí–‰í•  ì»¬ëŸ¼ ì§€ì • ë° ê°œí–‰ ì…ë ¥í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
    custom_colnames = {
        "ì§€í‘œí™•ì • ìµœëŒ€ê¸°ê°„": "ì§€í‘œí™•ì •<br>ìµœëŒ€ê¸°ê°„",
        "ë³µê·€ìœ ì € í¬í•¨ ROAS D360": "ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360",
        "ë³µê·€ìœ ì € ROAS D360": "ë³µê·€ìœ ì €<br>ROAS D360",
        "ë³µê·€ìœ ì € ROAS D360 ì˜ˆì¸¡ì¹˜": "ë³µê·€ìœ ì €<br>ROAS D360<br>ì˜ˆì¸¡ì¹˜",
        "ê¸°ë³¸ BEP": "ê¸°ë³¸<br>BEP",
        "ìˆ˜ìˆ˜ë£Œ ì ìš©í›„ BEP": "ìˆ˜ìˆ˜ë£Œ ì ìš©í›„<br>BEP",
        "LTV ì„±ì¥ì„¸ D3": "LTV<br>ì„±ì¥ì„¸<br>D1 D3",
        "LTV ì„±ì¥ì„¸ D7": "LTV<br>ì„±ì¥ì„¸<br>D3 D7",
        "LTV ì„±ì¥ì„¸ D14": "LTV<br>ì„±ì¥ì„¸<br>D7 D14",
        "LTV ì„±ì¥ì„¸ D30": "LTV<br>ì„±ì¥ì„¸<br>D14 D30",
        "LTV ì„±ì¥ì„¸ D60": "LTV<br>ì„±ì¥ì„¸<br>D30 D60",
        "LTV ì„±ì¥ì„¸ D90": "LTV<br>ì„±ì¥ì„¸<br>D60 D90",
        "LTV ì„±ì¥ì„¸ D120": "LTV<br>ì„±ì¥ì„¸<br>D90 D120",
        "LTV ì„±ì¥ì„¸ D150": "LTV<br>ì„±ì¥ì„¸<br>D120 D150",
        "LTV ì„±ì¥ì„¸ D180": "LTV<br>ì„±ì¥ì„¸<br>D150 D180",
        "LTV ì„±ì¥ì„¸ D210": "LTV<br>ì„±ì¥ì„¸<br>D180 D210",
        "LTV ì„±ì¥ì„¸ D240": "LTV<br>ì„±ì¥ì„¸<br>D210 D240",
        "LTV ì„±ì¥ì„¸ D270": "LTV<br>ì„±ì¥ì„¸<br>D240 D270",
        "LTV ì„±ì¥ì„¸ D300": "LTV<br>ì„±ì¥ì„¸<br>D270 D300",
        "LTV ì„±ì¥ì„¸ D330": "LTV<br>ì„±ì¥ì„¸<br>D300 D330",
        "LTV ì„±ì¥ì„¸ D360": "LTV<br>ì„±ì¥ì„¸<br>D330 D360"
    }
    df_numeric = df_numeric.rename(columns=custom_colnames)

    #### ROAS ìˆ˜ì¹˜ì˜ ë°” ì„œì‹ì„ ì»¬ëŸ¼ë³„ì´ ì•„ë‹Œ ì „ì²´ ìˆ˜ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì„œì‹ì ìš©ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ê°’ ì„¤ì •
    roas_cols = [c for c in df_numeric.columns if c.startswith("ROAS D")] + ["ë³µê·€ìœ ì € í¬í•¨<br>ROAS D360"]

    # ì „ì²´ ìµœì†Œ/ìµœëŒ€ êµ¬í•˜ê¸°
    roas_global_min = df_numeric[roas_cols].min().min()
    roas_global_max = df_numeric[roas_cols].max().max()

    #### ì„±ì¥ì„¸ ìˆ˜ì¹˜ì˜ ë°” ì„œì‹ì„ ì»¬ëŸ¼ë³„ì´ ì•„ë‹Œ ì „ì²´ ìˆ˜ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì„œì‹ì ìš©ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ê°’ ì„¤ì •

    growth_cols = [c for c in df_numeric.columns if c.startswith("LTV<br>ì„±ì¥ì„¸<br>D")]

    # ì „ì²´ ìµœì†Œ/ìµœëŒ€ êµ¬í•˜ê¸°
    growth_global_min = df_numeric[growth_cols].min().min()
    growth_global_max = df_numeric[growth_cols].max().max()

    #### style ì ìš©
    styled = (
        df_numeric.style
        .hide(axis="index")
        .format({
            "ë§ˆì¼€íŒ… ë¹„ìš©": "{:,.0f}",
            **{
                col: "{:.1%}"
                for col in df_numeric.columns
                if col.startswith("ROAS D")
                or col.startswith("LTV<br>ì„±ì¥ì„¸<br>D")
                or col.startswith("ë³µê·€ìœ ì €")
                or col.endswith("BEP")
            }
        })
        .bar(subset=["ë§ˆì¼€íŒ… ë¹„ìš©"], color="#f4cccc")
        .bar(subset=roas_cols, color="#c9daf8", vmin=roas_global_min, vmax=roas_global_max)
        .bar(subset=["ë³µê·€ìœ ì €<br>ROAS D360", "ë³µê·€ìœ ì €<br>ROAS D360<br>ì˜ˆì¸¡ì¹˜"], color="#ffe599")
        .bar(subset=growth_cols, color="#b5f7a3", vmin=growth_global_min, vmax=growth_global_max)
        .set_table_styles(
            [
                {"selector": "th", "props": [("background-color", "#f0f0f0"), ("font-weight", "bold"), ("border", "1px solid black")]},
                {"selector": "td", "props": [("border", "1px solid black")]}
            ]
        )
        # ê°•ì¡° í•¨ìˆ˜ ì ìš©
        .apply(highlight_based_on_dn, axis=1)
        .apply(highlight_roas_vs_bep, axis=1)
        )
    
    return styled



def roas_html_draw(gameidx: str, bucket_name: str, **context):
    """
    HTML í…Œì´ë¸”ì„ ì´ë¯¸ì§€ë¡œ ìº¡ì²˜í•˜ì—¬ GCSì— ì €ì¥
    
    Args:
        gameidx: ê²Œì„ ì¸ë±ìŠ¤
        bucket_name: GCS ë²„í‚·ëª…
        **context: Airflow ì»¨í…ìŠ¤íŠ¸
    
    Returns:
        GCS ê²½ë¡œ (ì˜ˆ: "potc/graph6_monthlyROAS.png")
    """
    
    logger.info("ğŸ¯ ROAS HTML ì´ë¯¸ì§€ ìº¡ì²˜ ì‹œì‘")
    
    try:
        # Step 1: í…Œì´ë¸” ë°ì´í„° ìƒì„±
        logger.info("ğŸ“Š í…Œì´ë¸” ë°ì´í„° ìƒì„± ì¤‘...")
        styled = roas_table_draw(**context)
        
        # Step 2: HTML ìƒì„±
        logger.info("ğŸ”¨ HTML ìƒì„± ì¤‘...")
        html_path = create_html_file(styled)
        
        # Step 3: HTMLì„ ì´ë¯¸ì§€ë¡œ ìº¡ì²˜
        logger.info("ğŸ“¸ ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
        image_bytes = asyncio.run(capture_html_to_image_async(html_path))
        
        # Step 4: GCSì— ì—…ë¡œë“œ
        logger.info("ğŸ“¤ GCS ì—…ë¡œë“œ ì¤‘...")
        gcs_path = upload_image_to_gcs(
            image_bytes=image_bytes,
            gameidx=gameidx,
            bucket_name=bucket_name,
            filename="graph6_monthlyROAS.png"
        )
        
        logger.info(f"âœ… ROAS ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {gcs_path}")
        
        # Step 5: ë¡œì»¬ HTML íŒŒì¼ ì •ë¦¬
        cleanup_local_files(html_path)
        
        return gcs_path
        
    except Exception as e:
        logger.error(f"âŒ ROAS ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {type(e).__name__} - {str(e)}", exc_info=True)
        raise


def create_html_file(styled_df) -> str:
    """
    ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ DataFrameì„ HTML íŒŒì¼ë¡œ ìƒì„±
    
    Args:
        styled_df: ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ Pandas DataFrame
    
    Returns:
        HTML íŒŒì¼ ê²½ë¡œ
    """
    
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { 
                font-family: Arial, sans-serif; 
                padding: 20px; 
                margin: 0;
            }
            table { 
                border-collapse: collapse; 
                font-size: 13px; 
                margin-top: 20px;
            }
            th, td {
                border: 1px solid #999;
                padding: 6px 10px;
                text-align: center;
            }
            th { background-color: #f0f0f0; font-weight: bold; }
            
            /* íŠ¹ì • ì»¬ëŸ¼ë³„ ìŠ¤íƒ€ì¼ */
            th:nth-child(1), td:nth-child(1) { min-width: 50px; max-width: 60px; }
            th:nth-child(2), td:nth-child(2) { min-width: 60px; max-width: 65px; white-space: normal; }
            th:nth-child(3), td:nth-child(3) { min-width: 70px; max-width: 95px; }
            th:nth-child(20), td:nth-child(20) { min-width: 80px; white-space: normal; }
            th:nth-child(21), td:nth-child(21) { min-width: 75px; white-space: normal; }
            th:nth-child(22), td:nth-child(22) { min-width: 85px; white-space: normal; }
            th:nth-child(24), td:nth-child(24) { min-width: 80px; white-space: normal; }
            th:nth-child(28), td:nth-child(28) { min-width: 55px; white-space: normal; }
            th:nth-child(29), td:nth-child(29) { min-width: 55px; white-space: normal; }
            th:nth-child(30), td:nth-child(30) { min-width: 55px; white-space: normal; }
            th:nth-child(31), td:nth-child(31) { min-width: 60px; white-space: normal; }
            th:nth-child(32), td:nth-child(32) { min-width: 70px; white-space: normal; }
            th:nth-child(33), td:nth-child(33) { min-width: 70px; white-space: normal; }
            th:nth-child(34), td:nth-child(34) { min-width: 70px; white-space: normal; }
            th:nth-child(35), td:nth-child(35) { min-width: 70px; white-space: normal; }
            th:nth-child(36), td:nth-child(36) { min-width: 70px; white-space: normal; }
            th:nth-child(37), td:nth-child(37) { min-width: 70px; white-space: normal; }
            th:nth-child(38), td:nth-child(38) { min-width: 70px; white-space: normal; }
            th:nth-child(39), td:nth-child(39) { min-width: 70px; white-space: normal; }
            
            h2 { margin-top: 0; }
        </style>
    </head>
    <body>
        <h2>{{ game_name }} GBTW ì‹ ê·œìœ ì € íšŒìˆ˜ í˜„í™©</h2>
        {{ table | safe }}
    </body>
    </html>
    """
    
    try:
        # í…Œì´ë¸”ì„ HTMLë¡œ ë³€í™˜
        table_html = styled_df.to_html()
        
        # í…œí”Œë¦¿ì— ë Œë”ë§
        rendered_html = Template(html_template).render(
            game_name="GBTW",
            table=table_html
        )
        
        # HTML íŒŒì¼ ì €ì¥ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
        html_path = os.path.join("/tmp", "table6_monthlyROAS.html")
        
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(rendered_html)
        
        logger.info(f"âœ… HTML íŒŒì¼ ìƒì„±: {html_path}")
        return html_path
        
    except Exception as e:
        logger.error(f"âŒ HTML íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {type(e).__name__} - {str(e)}", exc_info=True)
        raise


async def capture_html_to_image_async(html_path: str) -> bytes:
    """
    HTML íŒŒì¼ì„ ì´ë¯¸ì§€ë¡œ ìº¡ì²˜ (ë¹„ë™ê¸°)
    
    Args:
        html_path: HTML íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
    """
    
    logger.info(f"ğŸ¬ Playwright ì‹œì‘: {html_path}")
    
    try:
        async with async_playwright() as p:
            # âœ… ë¸Œë¼ìš°ì € ì‹¤í–‰
            logger.info("ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
            browser = await p.chromium.launch(headless=True)
            
            # âœ… í˜ì´ì§€ ìƒì„±
            page = await browser.new_page(
                viewport={"width": 1800, "height": 800}
            )
            
            # âœ… HTML íŒŒì¼ ë¡œë“œ
            file_url = f"file://{os.path.abspath(html_path)}"
            logger.info(f"ğŸ“„ HTML ë¡œë“œ: {file_url}")
            await page.goto(file_url)
            
            # âœ… í˜ì´ì§€ ë Œë”ë§ ëŒ€ê¸°
            await page.wait_for_load_state("networkidle")
            logger.info("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            
            # âœ… ì´ë¯¸ì§€ ìº¡ì²˜ (ë©”ëª¨ë¦¬ì— ì§ì ‘)
            logger.info("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì¤‘...")
            screenshot_bytes = await page.screenshot(full_page=True)
            
            logger.info(f"âœ… ìŠ¤í¬ë¦°ìƒ· ì™„ë£Œ ({len(screenshot_bytes) / 1024:.1f} KB)")
            
            # âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ
            await browser.close()
            logger.info("ğŸ”Œ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            
            return screenshot_bytes
            
    except Exception as e:
        logger.error(f"âŒ HTML ìº¡ì²˜ ì‹¤íŒ¨: {type(e).__name__} - {str(e)}", exc_info=True)
        raise


def upload_image_to_gcs(
    image_bytes: bytes,
    gameidx: str,
    bucket_name: str,
    filename: str = "graph6_monthlyROAS.png"
    ) -> str:
    """
    ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ GCSì— ì—…ë¡œë“œ
    
    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
        gameidx: ê²Œì„ ì¸ë±ìŠ¤
        bucket_name: GCS ë²„í‚·ëª…
        filename: ì €ì¥í•  íŒŒì¼ëª…
    
    Returns:
        GCS ê²½ë¡œ (ì˜ˆ: "potc/graph6_monthlyROAS.png")
    """
    
    try:
        # GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # GCS ê²½ë¡œ ì„¤ì •
        gcs_path = f"{gameidx}/{filename}"
        blob = bucket.blob(gcs_path)
        
        logger.info(f"ğŸ“¤ GCS ì—…ë¡œë“œ: gs://{bucket_name}/{gcs_path}")
        
        # ì´ë¯¸ì§€ ì—…ë¡œë“œ
        blob.upload_from_string(
            image_bytes,
            content_type='image/png'
        )
        
        logger.info(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: {len(image_bytes) / 1024:.1f} KB")
        
        return gcs_path
        
    except Exception as e:
        logger.error(f"âŒ GCS ì—…ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__} - {str(e)}", exc_info=True)
        raise


def cleanup_local_files(html_path: str) -> None:
    """
    ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì •ë¦¬
    
    Args:
        html_path: ì‚­ì œí•  HTML íŒŒì¼ ê²½ë¡œ
    """
    
    try:
        if os.path.exists(html_path):
            os.remove(html_path)
            logger.info(f"ğŸ—‘ï¸ ë¡œì»¬ íŒŒì¼ ì‚­ì œ: {html_path}")
    except OSError as e:
        logger.warning(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {type(e).__name__} - {str(e)}")


def kpi_table_draw(**context):

    roas_kpi = context['task_instance'].xcom_pull(
        task_ids = 'roas_kpi',
        key='roas_kpi'
    )

    # kpií‘œ
    nest_asyncio.apply()

    df_numeric = roas_kpi.copy()
    df_numeric = df_numeric.reset_index(drop=True)

    # 1) ROAS % â†’ ë¹„ìœ¨ ë³€í™˜
    def to_ratio_series(s: pd.Series) -> pd.Series:
        s_str = s.astype(str)
        s_num = pd.to_numeric(s_str.str.replace('%', '', regex=False), errors='coerce')
        return s_num / 100.0

    for c in df_numeric.columns:
        if c.startswith("ROAS "):
            df_numeric[c] = to_ratio_series(df_numeric[c])

    # 2) suffixes ì¶”ì¶œ
    suffixes = []
    for c in df_numeric.columns:
        m = re.search(r'\b(D\d+)\b$', str(c))
        if m and m.group(1) not in suffixes:
            suffixes.append(m.group(1))
    suffixes_tuple = tuple(suffixes)

    # 3) Styler ê¸°ë³¸ í¬ë§·
    styled = (
        df_numeric.style
        .hide(axis="index")
        .format({col: "{:.1%}" for col in df_numeric.columns if col.startswith("ROAS ")})
        .set_table_attributes('style="table-layout:fixed; width:600px;"')
    )

    # 4) HTML í…œí”Œë¦¿
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; padding: 20px; }
            h2 { margin: 0 0 10px 0; font-size: 18px; }
            table { border-collapse: collapse; font-size: 12px; border: 1px solid black; }
            th, td {
                border: 1px solid black;
                padding: 6px 8px;
                text-align: center;
                white-space: nowrap;
            }
            th { background-color: #f0f0f0; font-weight: bold; }
        </style>
    </head>
    <body>
        <h2>GBTW ROAS KPI (ì‹ ê·œìœ ì € ê¸°ì¤€)</h2>
        {{ table | safe }}
    </body>
    </html>
    """

    # 5) Styler â†’ HTML
    soup = BeautifulSoup(styled.to_html(), "html.parser")
    table = soup.find("table")

    # 6) colgroup & width ì ìš©
    ncols = len(df_numeric.columns)
    for cg in table.find_all("colgroup"):
        cg.decompose()

    colgroup = soup.new_tag("colgroup")
    width_map = {col: (80 if col.startswith("ROAS ") else 110) for col in df_numeric.columns}
    for col_name in df_numeric.columns:
        col = soup.new_tag("col", style=f"width: {width_map[col_name]}px !important;")
        colgroup.append(col)
    table.insert(0, colgroup)

    # 7) í—¤ë” ì¤„ë°”ê¿ˆ (ROAS â†’ ROAS<br>â€¦)
    for th in table.find_all("th"):
        text = th.get_text(strip=True)
        if text.startswith("ROAS "):
            th.string = ""
            th.append(BeautifulSoup(text.replace("ROAS ", "ROAS<br>"), "html.parser"))

    # 8) ìµœì¢… HTML ì €ì¥
    rendered_html = Template(html_template).render(table=str(table))
    html_path = "table6_ROAS_KPI.html"
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(rendered_html)

    # 9) ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜
    async def capture_html_to_image():
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={"width": 600, "height": 160})
            await page.goto("file://" + os.path.abspath(html_path))
            await page.screenshot(path="graph6_ROAS_KPI.png", full_page=True)
            await browser.close()

    asyncio.get_event_loop().run_until_complete(capture_html_to_image())


def roas_kpi_table_merge(gameidx:str):

    p1 = f'{gameidx}/graph6_monthlyROAS.png'
    p2 = f'{gameidx}/graph6_ROAS_KPI.png'

    # 2) ì´ë¯¸ì§€ ì—´ê¸° (íˆ¬ëª… ë³´ì¡´ ìœ„í•´ RGBA)
    blob1 = bucket.blob(p1)
    blob2 = bucket.blob(p2)

    im1 = blob1.download_as_bytes()
    im2 = blob2.download_as_bytes()

    img1 = Image.open(BytesIO(im1)).convert("RGBA")
    img2 = Image.open(BytesIO(im2)).convert("RGBA") 

    # ë‘ ì´ë¯¸ì§€ì˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    w1, h1 = img1.size
    w2, h2 = img2.size

    # ìµœì¢… ìº”ë²„ìŠ¤ í¬ê¸° (ë„ˆë¹„ëŠ” ë‘ ì´ë¯¸ì§€ ì¤‘ í° ê°’, ë†’ì´ëŠ” í•©ê³„)
    final_width = max(w1, w2)
    final_height = h1 + h2

    # í°ìƒ‰ ë°°ê²½ì˜ ìƒˆ ìº”ë²„ìŠ¤ ìƒì„±
    combined = Image.new("RGB", (final_width, final_height), (255, 255, 255))

    # ìœ„ì— roas_pc, ì•„ë˜ì— roaskpi_pc ë¶™ì´ê¸° (ì™¼ìª½ ì •ë ¬)
    combined.paste(img1, (0, 0))
    combined.paste(img2, (0, h1))

    # ì €ì¥
    combined.save("graph6_monthlyROAS_and_KPI.png", dpi=(180,180))

    # 3) GCSì— ì €ì¥
    output_buffer = BytesIO()
    combined.save(output_buffer, format='PNG')
    output_buffer.seek(0)

    # GCS ê²½ë¡œ
    gcs_path = f'{gameidx}/graph1_dailySales_monthlySales.png'
    blob = bucket.blob(gcs_path)
    blob.upload_from_string(output_buffer.getvalue(), content_type='image/png')

    return gcs_path


def retrieve_new_user_upload_notion(gameidx:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "6. ì‹ ê·œìœ ì € íšŒìˆ˜ í˜„í™©" }}]
                },
            }
        ],
    )

    # ê³µí†µ í—¤ë”
    headers_json = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }

    try:
        gcs_path = f'{gameidx}/filePath6_monthlyROAS_KPI.png'
        blob = bucket.blob(gcs_path)
        image_bytes = blob.download_as_bytes()
        filename = 'filePath6_monthlyROAS_KPI.png'
        print(f"âœ“ GCS ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ : {gcs_path}")
    except Exception as e:
        print(f"âŒ GCS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    try:   
        # 1) ì—…ë¡œë“œ ê°ì²´ ìƒì„± (file_upload ìƒì„±)
        create_url = "https://api.notion.com/v1/file_uploads"
        payload = {
            "filename": filename,
            "content_type": "image/png"
        }

        resp = requests.post(create_url, headers=headers_json, data=json.dumps(payload))
        resp.raise_for_status()
        file_upload = resp.json()
        file_upload_id = file_upload["id"]
        print(f"âœ“ Notion ì—…ë¡œë“œ ê°ì²´ ìƒì„±: {file_upload_id}")

        # 2) íŒŒì¼ ë°”ì´ë„ˆë¦¬ ì „ì†¡ (multipart/form-data)
        # âœ… ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  BytesIO ì‚¬ìš©
        send_url = f"https://api.notion.com/v1/file_uploads/{file_upload_id}/send"
        files = {"file": (filename, BytesIO(image_bytes), "image/png")}
        headers_send = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": NOTION_VERSION
        }
        send_resp = requests.post(send_url, headers=headers_send, files=files)
        send_resp.raise_for_status()
        print(f"âœ“ íŒŒì¼ ì „ì†¡ ì™„ë£Œ: {filename}")

        # 3) Notion í˜ì´ì§€ì— ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
        append_url = f"https://api.notion.com/v1/blocks/{PAGE_INFO['id']}/children"
        append_payload = {
            "children": [
                {
                    "object": "block",
                    "type": "image",
                    "image": {
                        "type": "file_upload",
                        "file_upload": {"id": file_upload_id},
                        # ìº¡ì…˜ì„ ë‹¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # "caption": [{"type": "text", "text": {"content": "ìë™ ì—…ë¡œë“œëœ ê·¸ë˜í”„"}}]
                    }
                }
            ]
        }

        append_resp = requests.patch(
            append_url, headers=headers_json, data=json.dumps(append_payload)
        )
        append_resp.raise_for_status()
        print(f"âœ… Notionì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ: {filename}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ì—ëŸ¬: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {str(e)}")
        raise


    query6_monthlyROAS =context['task_instance'].xcom_pull(
        task_ids='roas_dataframe_preprocessing',
        key='monthlyBEP_ROAS'
    )

    resp = df_to_notion_table_under_toggle(
        notion=notion,
        page_id=PAGE_INFO['id'],
        df=query6_monthlyROAS,
        toggle_title="ğŸ“Š ë¡œë°ì´í„° - ì‹ ê·œìœ ì € íšŒìˆ˜í˜„í™©",
        max_first_batch_rows=90,
        batch_size=100,
    )

    blocks = md_to_notion_blocks(result6_ROAS_gemini(**context), 1)

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    blocks = md_to_notion_blocks(monthlyLTVgrowth_gemini(**context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )


##### í”„ë¡¬í”„íŠ¸ ì¢…í•©í•˜ì—¬ ìš”ì•½

def summary_gemini(joyplegameid:int, gameidx:str, service_sub:str, **context):
    response_summary = genai_client.models.generate_content(
    model=MODEL_NAME,
    contents = f"""

    ì•„ë˜ ë‚´ìš©ì„ 10ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜.
    ìš”ì•½í•œ ë‚´ìš©ì€ ~~ ì…ë‹ˆë‹¤ ì´ëŸ°ë§ í•˜ì§€ë§ê³  ê·¸ëƒ¥ ë°”ë¡œ ìš”ì•½í•œ ë‚´ìš©ë§Œ ì•Œë ¤ì¤˜.
    {daily_revenue_gemini(joyplegameid, service_sub, **context)}
    {inhouses_revenue_gemini(joyplegameid, **context)}
    {cohort_by_gemini(joyplegameid, **context)}
    {os_by_gemini(joyplegameid, **context)}
    {rev_group_rev_pu_gemini(joyplegameid, service_sub, **context)}
    {iap_gem_ruby_gemini(service_sub, **context)}
    {top3_items_by_category_gemini(service_sub, **context)}
    {monthly_day_average_rev_gemini(joyplegameid, service_sub, **context)}
    {rgroup_rev_total_gemini(joyplegameid, service_sub, **context)}
    {rev_cohort_year_gemini(joyplegameid, service_sub, **context)}
    {result6_ROAS_gemini(**context)}

    <ì„œì‹ ìš”êµ¬ì‚¬í•­>
    1. í•œë¬¸ì¥ë‹¹ ì¤„ë°”ê¿ˆ í•œë²ˆ í•´ì¤˜.
    3. í•œ ë¬¸ì¥ë§ˆë‹¤ ë…¸ì…˜ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì¤˜. e.g. * ë‹¹ì›” ë§¤ì¶œì€ ì´ë ‡ìŠµë‹ˆë‹¤.


    """
    ,
    config=types.GenerateContentConfig(
            system_instruction=[
                ""
            ],
            # tools=[RAG],
            temperature=0.5
            ,labels=LABELS

        )
    )

    # ì½”ë©˜íŠ¸ ì¶œë ¥
    return response_summary.text


def summray_upload_notion(joyplegameid:int, gameidx:str, service_sub:str, **context):

    PAGE_INFO=context['task_instance'].xcom_pull(
        task_ids = 'make_gameframework_notion_page',
        key='page_info'
    )

    notion.blocks.children.append(
        PAGE_INFO['id'],
        children=[
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "â­ìš”ì•½" }}]
                },
            }
        ],
    )

    ## ìš”ì•½ ë‚´ìš©
    blocks = md_to_notion_blocks(summary_gemini(joyplegameid, gameidx, service_sub, **context))

    notion.blocks.children.append(
        block_id=PAGE_INFO['id'],
        children=blocks
    )

    return True
