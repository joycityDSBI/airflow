## notion_utils.py
 
import os
import time
import json
import random
import logging
from datetime import datetime
from typing import Dict, Set, Tuple, List
import requests
import pandas as pd
from airflow.models import Variable
 
logging.basicConfig(level=logging.INFO)
 

def get_var(key: str, default: str = None) -> str:
    """í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Airflow Variable ì¡°íšŒ"""
    return os.environ.get(key) or Variable.get(key, default_var=default)


# ===== Notion ENV (ì´ 4ê°œë§Œ í™˜ê²½ë³€ìˆ˜ë¡œ ì‚¬ìš©) =====
NOTION_TOKEN = get_var("NOTION_TOKEN")
DB_A_ID = get_var("NOTION_DB_A")  # í…Œì´ë¸” ì •ë³´ìš© (í‚¤: Hub í…Œì´ë¸”ëª…)
DB_B_ID = get_var("NOTION_DB_B")  # ì»¬ëŸ¼ ì •ë³´ìš© (í‚¤: Hub ì»¬ëŸ¼ëª…)
DB_C_ID = get_var("NOTION_DB_C")  # í…Œì´ë¸”-ì»¬ëŸ¼ ë§¤í•‘ìš© (relation: A/B)
 
if not (NOTION_TOKEN and DB_A_ID and DB_B_ID and DB_C_ID):
    logging.warning("âš ï¸ NOTION_TOKEN / NOTION_DB_A / NOTION_DB_B / NOTION_DB_C ì¤‘ ëˆ„ë½ì´ ìˆìŠµë‹ˆë‹¤.")
 
HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28",
}
 
# ===== ê³ ì • íŒŒë¼ë¯¸í„°(í•˜ë“œì½”ë”©) =====
PAGE_SIZE        = 100      # Notion API ìµœëŒ€ 100
MAX_PAGES_SCAN   = 200      # 100 * 200 = ìµœëŒ€ 20,000ê±´ ìŠ¤ìº”
MAX_RETRY        = 5        # 429/5xx ì¬ì‹œë„ íšŸìˆ˜
RATE_LIMIT_SLEEP = 0.35     # 3 rps ê°€ë“œ
REQ_TIMEOUT      = 30       # ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)
 
# ===== ê³µí†µ HTTP ìš”ì²­ (ì¬ì‹œë„/ë°±ì˜¤í”„) =====
def request_with_retry(method: str, url: str, **kwargs) -> requests.Response:
    if "headers" not in kwargs:
        kwargs["headers"] = HEADERS
    if "timeout" not in kwargs:
        kwargs["timeout"] = REQ_TIMEOUT
 
    last = None
    for attempt in range(1, MAX_RETRY + 1):
        try:
            res = requests.request(method, url, **kwargs)
        except requests.RequestException as e:
            wait = min(2 ** attempt, 16) + random.random()
            logging.warning(f"â³ ìš”ì²­ ì˜ˆì™¸ ì¬ì‹œë„ {attempt}/{MAX_RETRY}: {e} â€” {wait:.1f}s ëŒ€ê¸°")
            time.sleep(wait)
            continue
 
        if res.status_code in (429, 500, 502, 503, 504):
            wait = min(2 ** attempt, 16) + random.random()
            logging.warning(f"â³ {res.status_code} ì¬ì‹œë„ {attempt}/{MAX_RETRY}: {res.text[:200]} â€” {wait:.1f}s ëŒ€ê¸°")
            time.sleep(wait)
            last = res
            continue
        return res
    return last
 
# ===== í…ìŠ¤íŠ¸ ì¶”ì¶œ ìœ í‹¸ =====
def extract_text(prop: dict) -> str | None:
    arr = prop.get("title") or prop.get("rich_text")
    if not arr:
        return None
    parts = []
    for item in arr:
        t = item.get("text", {})
        if "content" in t:
            parts.append(t["content"])
    txt = "".join(parts).strip()
    return txt or None
 
# ===== DB í˜ì´ì§€ë„¤ì´ì…˜ =====
def notion_paginate(database_id: str, page_size: int = PAGE_SIZE, max_pages: int = MAX_PAGES_SCAN):
    next_cursor = None
    pages = 0
    while True:
        body = {"page_size": page_size}
        if next_cursor:
            body["start_cursor"] = next_cursor
 
        res = request_with_retry("POST", f"https://api.notion.com/v1/databases/{database_id}/query", json=body)
        if not res or res.status_code != 200:
            logging.error(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {res.status_code if res else 'ERR'} - {res.text[:200] if res else ''}")
            break
 
        data = res.json()
        for page in data.get("results", []):
            yield page
 
        if not data.get("has_more"):
            break
        next_cursor = data.get("next_cursor")
        pages += 1
        if pages >= max_pages:
            logging.warning(f"âš ï¸ í˜ì´ì§€ ìƒí•œ ë„ë‹¬(max_pages={max_pages})")
            break
        time.sleep(RATE_LIMIT_SLEEP)
 
# ===== A/B ID ë§µ ì¡°íšŒ (ì „ì²´ í˜ì´ì§•) =====
def notion_query_id_map(database_id: str, target_column: str, *, silent: bool = False, label: str = "") -> dict:
    id_map: dict[str, str] = {}
    for page in notion_paginate(database_id):
        pid = page["id"]
        prop = page["properties"].get(target_column, {})
        key = extract_text(prop)
        if key:
            id_map[key] = pid
    if not silent:
        suffix = f" {label}" if label else ""
        logging.info(f"ğŸ” ID ë§µ ë¡œë“œ{suffix}: {target_column} {len(id_map)}ê±´")
    return id_map
 
# ===== ë‹¨ì¼ Insert (ì¬ì‹œë„/ë°±ì˜¤í”„) =====
def notion_insert(database_id: str, field_name: str, value: str, is_title: bool = True):
    field_type = "title" if is_title else "rich_text"
    payload = {
        "parent": {"database_id": database_id},
        "properties": {
            field_name: {field_type: [{"text": {"content": value}}]}
        }
    }
    res = request_with_retry("POST", "https://api.notion.com/v1/pages", json=payload)
    if res and res.status_code == 200:
        logging.info(f"âœ… Insert ì„±ê³µ: {value}")
        return True
    logging.error(f"âŒ Insert ì‹¤íŒ¨: {res.status_code if res else 'ERR'} - {res.text[:200] if res else ''}")
    return False
 
# ===== ê´€ê³„(Relation) í—¬í¼ =====
def get_relation_ids(page_props: dict, prop_name: str) -> List[str]:
    rel = page_props.get(prop_name, {})
    return [r.get("id") for r in rel.get("relation", []) if "id" in r]
 
def archive_page(page_id: str) -> bool:
    """C í˜ì´ì§€ ì•„ì¹´ì´ë¸Œ"""
    res = request_with_retry("PATCH", f"https://api.notion.com/v1/pages/{page_id}", json={"archived": True})
    code = res.status_code if res else "ERR"
    logging.info(f"ğŸ—„ï¸ archive C page {page_id} -> {code}")
    return bool(res and res.status_code == 200)
 
def _parse_iso8601_to_ts(s: str) -> float:
    """ISO8601(ì˜ˆ: 2025-09-25T01:23:45.678Z) â†’ epoch seconds"""
    try:
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        return datetime.fromisoformat(s).timestamp()
    except Exception:
        return 0.0
 
def cleanup_db_c_orphans(a_id_map: Dict[str, str], b_id_map: Dict[str, str], *, dry_run: bool = False) -> dict:
    """
    Pre-clean ë‹¨ê³„:
      1) relation ë¹„ì—ˆìœ¼ë©´ â†’ archive
      2) relation idê°€ A/Bì˜ ìœ íš¨ id ì§‘í•©ì— ì—†ìœ¼ë©´ â†’ archive
      3) (a_id, b_id) ì¤‘ë³µì´ ì—¬ëŸ¬ ê±´ì´ë©´ ê°€ì¥ ì˜¤ë˜ëœ 1ê±´ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ â†’ archive
    """
    valid_a: Set[str] = set(a_id_map.values())
    valid_b: Set[str] = set(b_id_map.values())
 
    orphan: List[str] = []      # relation ë¹„ì–´ìˆëŠ” C í˜ì´ì§€
    invalid: List[str] = []     # A/Bì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” idë¥¼ ì°¸ì¡°í•˜ëŠ” C í˜ì´ì§€
    dedup: List[str] = []       # (a,b) ì¤‘ë³µ ì œê±° ëŒ€ìƒ
 
    # pair â†’ (pid_kept, created_ts)
    best_for_pair: Dict[Tuple[str, str], Tuple[str, float]] = {}
 
    # ì „ì²´ C ìŠ¤ìº”
    for page in notion_paginate(DB_C_ID):
        pid = page["id"]
        props = page["properties"]
        a_list = get_relation_ids(props, "Hub í…Œì´ë¸”ëª…")
        b_list = get_relation_ids(props, "Hub ì»¬ëŸ¼ëª…")
 
        # 1) orphan
        if not a_list or not b_list:
            orphan.append(pid)
            continue
 
        a_pid, b_pid = a_list[0], b_list[0]
 
        # 2) invalid ref
        if a_pid not in valid_a or b_pid not in valid_b:
            invalid.append(pid)
            continue
 
        # 3) dedup: ê°™ì€ (a_pid, b_pid) ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ 1ê±´ë§Œ ìœ ì§€
        pair = (a_pid, b_pid)
        created_ts = _parse_iso8601_to_ts(page.get("created_time", "1970-01-01T00:00:00Z"))
        if pair not in best_for_pair:
            best_for_pair[pair] = (pid, created_ts)
        else:
            keep_pid, keep_ts = best_for_pair[pair]
            # ì˜¤ë˜ëœ(ì‘ì€ created_ts) ê²ƒ ìœ ì§€
            if created_ts < keep_ts:
                # ì§€ê¸ˆ í˜ì´ì§€ê°€ ë” ì˜¤ë˜ë¨ â†’ ê¸°ì¡´ ê²ƒì„ ì¤‘ë³µìœ¼ë¡œ ë³´ëƒ„
                dedup.append(keep_pid)
                best_for_pair[pair] = (pid, created_ts)
            else:
                dedup.append(pid)
 
    # ìµœì¢… ì•„ì¹´ì´ë¸Œ ëŒ€ìƒ (ì¤‘ë³µ ì œê±°)
    to_archive = set(orphan) | set(invalid) | set(dedup)
    logging.info(
        f"ğŸ§¹ C pre-clean: orphan={len(orphan)}, invalid_ref={len(invalid)}, "
        f"dedup={len(dedup)}, unique_archive={len(to_archive)}"
    )
 
    if dry_run:
        logging.info("ğŸŸ¡ DRY-RUN: archive ì‹¤í–‰ ì•ˆ í•¨")
        return {
            "orphan": len(orphan),
            "invalid_ref": len(invalid),
            "dedup": len(dedup),
            "archived": 0
        }
 
    # ì‹¤ì œ ì•„ì¹´ì´ë¸Œ
    archived = 0
    for pid in to_archive:
        if archive_page(pid):
            archived += 1
        time.sleep(0.2)
 
    logging.info(f"âœ… C pre-clean archived: {archived}")
    return {
        "orphan": len(orphan),
        "invalid_ref": len(invalid),
        "dedup": len(dedup),
        "archived": archived
    }
 
 
# ===== C í…Œì´ë¸” UPSERT(diff) =====
def sync_db_c(df: pd.DataFrame, a_id_map: Dict[str, str], b_id_map: Dict[str, str]):
    # ì—­ë§¤í•‘: page_id -> ì´ë¦„
    a_pid_to_name = {pid: name for name, pid in a_id_map.items()}
    b_pid_to_name = {pid: name for name, pid in b_id_map.items()}
 
    # íƒ€ê¹ƒ ìŒ ìƒì„±
    unique_pairs = df.drop_duplicates(subset=["full_table_id", "column_name"])
    desired: Set[Tuple[str, str]] = set()
    for row in unique_pairs.itertuples():
        tid = row.full_table_id
        cname = row.column_name
        if tid in a_id_map and cname in b_id_map:
            desired.add((a_id_map[tid], b_id_map[cname]))
 
    # í˜„ì¬ ì¡´ì¬í•˜ëŠ” ìŒ ìˆ˜ì§‘
    existing: Set[Tuple[str, str]] = set()
    page_by_pair: Dict[Tuple[str, str], str] = {}
    for page in notion_paginate(DB_C_ID):
        pid = page["id"]
        props = page["properties"]
        a_list = get_relation_ids(props, "Hub í…Œì´ë¸”ëª…")
        b_list = get_relation_ids(props, "Hub ì»¬ëŸ¼ëª…")
        if a_list and b_list:
            pair = (a_list[0], b_list[0])
            existing.add(pair)
            page_by_pair[pair] = pid
 
    to_add = desired - existing
    to_remove = existing - desired
    logging.info(f"ğŸ“Š C diff â€” add:{len(to_add)} / remove:{len(to_remove)} / keep:{len(existing & desired)}")
 
    # ì¶”ê°€
    idx_base = len(existing & desired) + 1
    for i, (a_pid, b_pid) in enumerate(to_add, start=idx_base):
        tname = a_pid_to_name.get(a_pid, "UNKNOWN_TABLE")
        cname = b_pid_to_name.get(b_pid, "UNKNOWN_COLUMN")
 
        payload = {
            "parent": {"database_id": DB_C_ID},
            "properties": {
                "idx": {"title": [{"text": {"content": str(i)}}]},
                "Hub í…Œì´ë¸”ëª…": {"relation": [{"id": a_pid}]},
                "Hub ì»¬ëŸ¼ëª…": {"relation": [{"id": b_pid}]}
            }
        }
        res = request_with_retry("POST", "https://api.notion.com/v1/pages", json=payload)
        code = res.status_code if res else "ERR"
        # âœ… ì‚¬ëŒì´ ë°”ë¡œ ì½íˆëŠ” ë¡œê·¸
        logging.info(f"â• C add [{tname}] Ã— [{cname}] -> {code}")
        time.sleep(0.25)
 
    # ì œê±°(archive)
    for pair in to_remove:
        pid = page_by_pair.get(pair)
        res = request_with_retry("PATCH", f"https://api.notion.com/v1/pages/{pid}", json={"archived": True})
        code = res.status_code if res else "ERR"
        tname = a_pid_to_name.get(pair[0], "UNKNOWN_TABLE")
        cname = b_pid_to_name.get(pair[1], "UNKNOWN_COLUMN")
        logging.info(f"â– C remove [{tname}] Ã— [{cname}] -> {code}")
        time.sleep(0.2)
 
# ===== ì „ì²´ ë™ê¸°í™” ì—”íŠ¸ë¦¬ =====
def update_notion_databases(df: pd.DataFrame):
    try:
        # [1] A ë“±ë¡ (í…Œì´ë¸”)
        table_ids = sorted(set(df["full_table_id"]))
        a_id_map = notion_query_id_map(DB_A_ID, "Hub í…Œì´ë¸”ëª…")
        for tid in table_ids:
            if tid not in a_id_map:
                notion_insert(DB_A_ID, "Hub í…Œì´ë¸”ëª…", tid)
        a_id_map = notion_query_id_map(DB_A_ID, "Hub í…Œì´ë¸”ëª…")
 
        # [2] B ë“±ë¡ (ì»¬ëŸ¼)
        column_names = sorted(set(df["column_name"]))
        b_id_map = notion_query_id_map(DB_B_ID, "Hub ì»¬ëŸ¼ëª…")
        for cname in column_names:
            if cname not in b_id_map:
                notion_insert(DB_B_ID, "Hub ì»¬ëŸ¼ëª…", cname)
        b_id_map = notion_query_id_map(DB_B_ID, "Hub ì»¬ëŸ¼ëª…")
 
        # âœ… [2.5] C ì‚¬ì „ ì •ë¦¬(ê³ ì•„/ë¬´íš¨ì°¸ì¡°/ì¤‘ë³µ ì œê±°)
        stats = cleanup_db_c_orphans(a_id_map, b_id_map, dry_run=False)  # í•„ìš” ì‹œ Trueë¡œ ì‹œë²” ì‹¤í–‰
        logging.info(f"ğŸ“‹ pre-clean stats: {stats}")
 
        # [3] C UPSERT(diff)
        sync_db_c(df, a_id_map, b_id_map)
 
    except Exception as e:
        logging.exception(f"ğŸ”¥ Notion ì ì¬ ì‹¤íŒ¨: {e}")