import requests
import pandas as pd
import time
import os
from datetime import datetime, timedelta
from databricks import sql
from airflow import DAG, Dataset
from airflow.operators.python import PythonOperator
from airflow.models import Variable

injoy_notion_permission_producer = Dataset('injoy_notion_permission_producer')

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=15),
}

with DAG(
    dag_id='injoy_notion_permission_consumer',
    default_args=default_args,
    description='Databricks ë°ì´í„°ë¥¼ Notion DBì— ë™ê¸°í™”í•˜ëŠ” DAG',
    schedule= [injoy_notion_permission_producer],
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['notion', 'sync', 'databricks'],
) as dag:

    # ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
    
    def get_var(key: str, default: str = None, required: bool = False) -> str:
        """í™˜ê²½ ë³€ìˆ˜ â†’ Airflow Variable ìˆœì„œë¡œ ì¡°íšŒ"""
        env_value = os.environ.get(key)
        if env_value:
            print(f"âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ {key} ë¡œë“œë¨")
            return env_value
        
        try:
            try:
                var_value = Variable.get(key, default=None)
            except TypeError:
                var_value = Variable.get(key, default_var=None)
            
            if var_value:
                print(f"âœ“ Airflow Variableì—ì„œ {key} ë¡œë“œë¨")
                return var_value
        except Exception as e:
            print(f"âš ï¸  Variable.get({key}) ì˜¤ë¥˜: {str(e)}")
        
        if default is not None:
            print(f"â„¹ï¸  ê¸°ë³¸ê°’ìœ¼ë¡œ {key} ì„¤ì •ë¨: {default}")
            return default
        
        if required:
            raise ValueError(f"í•„ìˆ˜ ì„¤ì • {key}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"â„¹ï¸  {key} ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)")
        return None

    def get_notion_headers():
        """Notion API í—¤ë” ìƒì„±"""
        return {
            "Authorization": f"Bearer {get_var('NOTION_TOKEN', required=True)}",
            "Notion-Version": get_var("NOTION_API_VERSION", "2022-06-28"),
            "Content-Type": "application/json"
        }

    def get_databricks_connection():
        """Databricks ì—°ê²° ì„¤ì • ë°˜í™˜"""
        return {
            'server_hostname': get_var('DATABRICKS_SERVER_HOSTNAME', required=True),
            'http_path': get_var('databricks_http_path', required=True),
            'access_token': get_var('databricks_token', required=True)
        }

    # ==================== Notion í•¨ìˆ˜ ====================

    MULTI_SELECT_KEYS = {
        "user": ["ê·¸ë£¹ ê¶Œí•œ", "ê¶Œí•œ(ì—ì…‹íƒ€ì…|ì—ì…‹ëª…|ì—ì…‹ê¶Œí•œ)"],
        "group": ["ê·¸ë£¹ ê¶Œí•œ", "ê¶Œí•œ(ì—ì…‹íƒ€ì…|ì—ì…‹ëª…|ì—ì…‹ê¶Œí•œ)"],
        "asset": ["ì—ì…‹ê¶Œí•œ"]
    }
    
    RELATION_KEYS = {
        "user": ["ì†Œì† ê·¸ë£¹"],
        "group": [],
        "asset": ["ê¶Œí•œ íšë“ ê·¸ë£¹", "ê¶Œí•œ íšë“ ê°œì¸"]
    }

    def generate_row_key(row: dict, db_type: str) -> str:
        """ë³µí•© í‚¤ ìƒì„±"""
        if db_type == "user":
            return f"{row.get('ìœ ì € ì´ë¦„','').strip()}_{row.get('email','').strip()}"
        elif db_type == "group":
            return f"{row.get('ê·¸ë£¹ì´ë¦„','').strip()}"
        elif db_type == "asset":
            asset_type = row.get('ì—ì…‹ íƒ€ì…','').strip()
            asset_name = row.get('ì—ì…‹ëª…','').strip()
            
            permissions = str(row.get('ì—ì…‹ê¶Œí•œ','')).strip()
            sorted_permissions = ",".join(sorted(permissions.split(',')))

            groups = str(row.get('ê¶Œí•œ íšë“ ê·¸ë£¹','')).strip()
            sorted_groups = ",".join(sorted(groups.split(',')))

            users = str(row.get('ê¶Œí•œ íšë“ ê°œì¸','')).strip()
            sorted_users = ",".join(sorted(users.split(',')))

            return f"{asset_type}_{asset_name}_{sorted_permissions}_{sorted_groups}_{sorted_users}"
        return ""

    def get_all_notion_pages(database_id: str, headers: dict):
        """Notion DB ì „ì²´ ì¡°íšŒ"""
        url = f"https://api.notion.com/v1/databases/{database_id}/query"
        results = []
        has_more = True
        next_cursor = None
        
        while has_more:
            payload = {"page_size": 100}
            if next_cursor:
                payload["start_cursor"] = next_cursor
            try:
                res = requests.post(url, headers=headers, json=payload)
                res.raise_for_status()
                data = res.json()
                results.extend(data.get("results", []))
                next_cursor = data.get("next_cursor")
                has_more = data.get("has_more", False)
            except requests.exceptions.RequestException as e:
                print(f"âŒ Notion API Error: {e}")
                break
            time.sleep(0.3)
        return results

    def extract_notion_data_map(pages, db_type, id_to_name_maps=None):
        """Notion í˜ì´ì§€ â†’ ë³µí•© í‚¤ ê¸°ë°˜ dict ìƒì„±"""
        page_dict = {}
        if id_to_name_maps is None:
            id_to_name_maps = {}
        
        group_id_map = id_to_name_maps.get("group", {})
        user_id_map = id_to_name_maps.get("user", {})

        for page in pages:
            props = page.get("properties", {})
            row = {}
            
            for key, val in props.items():
                if "title" in val:
                    row[key] = val["title"][0]["plain_text"] if val["title"] else ""
                elif "rich_text" in val:
                    row[key] = val["rich_text"][0]["plain_text"] if val["rich_text"] else ""
                elif "multi_select" in val:
                    row[key] = ",".join(sorted([opt["name"] for opt in val.get("multi_select", [])]))
                elif "relation" in val:
                    ids = [rel.get("id") for rel in val.get("relation", [])]
                    names = []
                    if (key == 'ì†Œì† ê·¸ë£¹' or key == 'ê¶Œí•œ íšë“ ê·¸ë£¹') and group_id_map:
                        names = [group_id_map.get(id, '') for id in ids]
                    elif key == 'ê¶Œí•œ íšë“ ê°œì¸' and user_id_map:
                        names = [user_id_map.get(id, '') for id in ids]
                    row[key] = ",".join(sorted([name for name in names if name]))
            
            row_key = generate_row_key(row, db_type)
            if row_key:
                page_dict[row_key] = {"page_id": page["id"], "values": row}
        
        return page_dict

    def build_properties_payload(row_data, db_type, group_name_to_id_map, user_name_to_id_map):
        """Notion ì†ì„± í˜ì´ë¡œë“œ ìƒì„±"""
        properties = {}
        title_key = {"user": "ìœ ì € ì´ë¦„", "group": "ê·¸ë£¹ì´ë¦„", "asset": "ì—ì…‹ íƒ€ì…"}.get(db_type)

        for key, value in row_data.items():
            if key == title_key:
                properties[key] = {"title": [{"text": {"content": str(value or "")}}]}
            elif key in RELATION_KEYS.get(db_type, []):
                related_ids = []
                names = [s.strip() for s in str(value).split(',') if s.strip()]
                if key == 'ê¶Œí•œ íšë“ ê°œì¸':
                    for name in names:
                        page_id = user_name_to_id_map.get(name)
                        if page_id:
                            related_ids.append(page_id)
                elif key == 'ì†Œì† ê·¸ë£¹' or key == 'ê¶Œí•œ íšë“ ê·¸ë£¹':
                    for name in names:
                        page_id = group_name_to_id_map.get(name)
                        if page_id:
                            related_ids.append(page_id)
                properties[key] = {"relation": [{"id": pid} for pid in related_ids]}
            elif key in MULTI_SELECT_KEYS.get(db_type, []):
                delimiter = ',' if db_type == 'asset' and key == 'ì—ì…‹ê¶Œí•œ' else '//'
                items = [s.strip() for s in str(value or "").split(delimiter) if s.strip()]
                properties[key] = {"multi_select": [{"name": item} for item in items]}
            else:
                properties[key] = {"rich_text": [{"text": {"content": str(value or "")}}]}
        
        return properties

    def has_data_changed(source_row, notion_row, db_type):
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ ë¹„êµ"""
        for key, new_value in source_row.items():
            existing_value = notion_row.get(key, "")
            
            if key in MULTI_SELECT_KEYS.get(db_type, []):
                delimiter_for_source = '//'
                if db_type == 'asset' and key == 'ì—ì…‹ê¶Œí•œ':
                    delimiter_for_source = ','

                new_value_sorted = ",".join(sorted(str(new_value or "").split(delimiter_for_source)))
                existing_value_sorted = ",".join(sorted(str(existing_value or "").split(',')))

                if new_value_sorted != existing_value_sorted:
                    return True
            elif str(new_value or "") != str(existing_value or ""):
                return True
        
        return False

    def deduplicate_notion_rows(database_id: str, db_type: str, headers: dict, id_to_name_maps=None):
        """ì¤‘ë³µ ì œê±°"""
        print(f"\nğŸ“¦ [{db_type.upper()}] Starting final deduplication process...")
        pages = get_all_notion_pages(database_id, headers)
        notion_data_map = extract_notion_data_map(pages, db_type, id_to_name_maps)
        
        key_to_page_ids = {}
        for page in pages:
            temp_row = {}
            for key, val in page.get("properties", {}).items():
                if "title" in val:
                    temp_row[key] = val["title"][0]["plain_text"] if val["title"] else ""
                elif "rich_text" in val:
                    temp_row[key] = val["rich_text"][0]["plain_text"] if val["rich_text"] else ""
                elif "multi_select" in val:
                    temp_row[key] = ",".join(sorted([opt["name"] for opt in val.get("multi_select", [])]))
                elif "relation" in val:
                    ids = [rel.get("id") for rel in val.get("relation", [])]
                    names = []
                    if id_to_name_maps:
                        group_id_map = id_to_name_maps.get("group", {})
                        user_id_map = id_to_name_maps.get("user", {})
                        if (key == 'ì†Œì† ê·¸ë£¹' or key == 'ê¶Œí•œ íšë“ ê·¸ë£¹') and group_id_map:
                            names = [group_id_map.get(id, '') for id in ids]
                        elif key == 'ê¶Œí•œ íšë“ ê°œì¸' and user_id_map:
                            names = [user_id_map.get(id, '') for id in ids]
                    temp_row[key] = ",".join(sorted([name for name in names if name]))

            row_key = generate_row_key(temp_row, db_type)
            if row_key:
                key_to_page_ids.setdefault(row_key, []).append(page["id"])

        for row_key, page_ids in key_to_page_ids.items():
            if len(page_ids) > 1:
                for dup_page_id in page_ids[1:]:
                    res = requests.patch(
                        f"https://api.notion.com/v1/pages/{dup_page_id}",
                        headers=headers,
                        json={"archived": True}
                    )
                    if res.ok:
                        print(f"ğŸ—‘ï¸ Archived duplicate ({db_type}) row: {row_key} (page: {dup_page_id})")
                    else:
                        print(f"âŒ Failed to archive duplicate: {dup_page_id} - {res.text}")
                    time.sleep(0.3)
        
        print(f"âœ… [{db_type.upper()}] Duplicate cleanup completed.")

    def run_sync_for_db(db_type, db_id, source_df, headers, dependency_maps=None):
        """DB ë™ê¸°í™” ì‹¤í–‰"""
        if dependency_maps is None:
            dependency_maps = {}
        
        print("\n" + "="*30 + f"\nğŸ›¡ï¸ Starting {db_type.upper()} DB Synchronization...\n" + "="*30)

        # ë°ì´í„° ì¤€ë¹„
        print(f"â˜ï¸ [{db_type.upper()}] Preparing data maps...")
        source_map = {generate_row_key(row, db_type): row for row in source_df.to_dict("records")}
        
        notion_pages = get_all_notion_pages(db_id, headers)
        notion_map = extract_notion_data_map(notion_pages, db_type, dependency_maps)
        
        group_name_to_id_map = dependency_maps.get("group_name_to_id", {})
        user_name_to_id_map = dependency_maps.get("user_name_to_id", {})

        print(f"Found {len(notion_map)} rows in Notion. Source has {len(source_map)} rows.")

        # ì‘ì—… ëª©ë¡ ìƒì„±
        to_insert_keys = source_map.keys() - notion_map.keys()
        to_delete_keys = notion_map.keys() - source_map.keys()
        to_check_update_keys = source_map.keys() & notion_map.keys()

        print(f"Sync plan: {len(to_insert_keys)} to insert, {len(to_delete_keys)} to delete, {len(to_check_update_keys)} to check for updates.")

        # Insert
        print(f"\nğŸš€ [{db_type.upper()}] Inserting {len(to_insert_keys)} new rows...")
        for key in to_insert_keys:
            row_data = source_map[key]
            properties = build_properties_payload(row_data, db_type, group_name_to_id_map, user_name_to_id_map)
            payload = {"parent": {"database_id": db_id}, "properties": properties}
            res = requests.post("https://api.notion.com/v1/pages", headers=headers, json=payload)
            if not res.ok:
                print(f"âŒ Insert failed ({key}): {res.text}")
            time.sleep(0.3)

        # Update
        print(f"\nğŸš€ [{db_type.upper()}] Checking {len(to_check_update_keys)} rows for updates...")
        update_count = 0
        for key in to_check_update_keys:
            source_row = source_map[key]
            notion_row = notion_map[key]
            if has_data_changed(source_row, notion_row['values'], db_type):
                update_count += 1
                properties = build_properties_payload(source_row, db_type, group_name_to_id_map, user_name_to_id_map)
                page_id = notion_row['page_id']
                res = requests.patch(
                    f"https://api.notion.com/v1/pages/{page_id}",
                    headers=headers,
                    json={"properties": properties}
                )
                if not res.ok:
                    print(f"âŒ Update failed ({key}): {res.text}")
                time.sleep(0.3)
        print(f"Found and performed {update_count} updates.")

        # Delete
        print(f"\nğŸ§¹ [{db_type.upper()}] Deleting {len(to_delete_keys)} removed rows...")
        for key in to_delete_keys:
            page_id = notion_map[key]['page_id']
            res = requests.patch(
                f"https://api.notion.com/v1/pages/{page_id}",
                headers=headers,
                json={"archived": True}
            )
            if not res.ok:
                print(f"âŒ Delete failed ({key}): {res.text}")
            time.sleep(0.3)

        # ì¤‘ë³µ ì œê±°
        deduplicate_notion_rows(db_id, db_type, headers, dependency_maps)
        print(f"\nâœ… {db_type.upper()} DB Synchronization Finished.")

    # ==================== Task í•¨ìˆ˜ ====================

    def load_user_permissions(**context):
        """ì‚¬ìš©ìë³„ ê¶Œí•œ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Loading user permissions from Databricks...")
        
        config = get_databricks_connection()
        query = """
        SELECT
            user_name,
            email,
            REPLACE(concat_ws('// ', collect_list(CONCAT_WS(' | ', object_type, object_name, privileges))), ',', ' &') AS assets_with_permissions,
            all_groups
        FROM datahub.injoy_ops_schema.user_permission_snapshot
        GROUP BY user_name, email, all_groups
        ORDER BY user_name
        """
        
        with sql.connect(**config) as conn:
            df = pd.read_sql(query, conn)
        
        df = df.rename(columns={
            "user_name": "ìœ ì € ì´ë¦„",
            "email": "email",
            "all_groups": "ì†Œì† ê·¸ë£¹",
            "assets_with_permissions": "ê¶Œí•œ(ì—ì…‹íƒ€ì…|ì—ì…‹ëª…|ì—ì…‹ê¶Œí•œ)"
        })
        
        print(f"âœ… Loaded {len(df)} user permission records")
        context['task_instance'].xcom_push(key='df_user_perm', value=df.to_json(orient='records'))

    def load_group_permissions(**context):
        """ê·¸ë£¹ë³„ ê¶Œí•œ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Loading group permissions from Databricks...")
        
        config = get_databricks_connection()
        query = """
        SELECT 
          inherited_group_name,
          REPLACE(
            MIN(CASE 
                WHEN TRIM(inherited_entitlements) = '' THEN 'N/A'
                ELSE TRIM(inherited_entitlements)
                END)
            , ',', '//' ) AS inherited_entitlements,
          REPLACE(concat_ws('// ', collect_list(CONCAT_WS(' | ', object_type, object_name, privileges))), ',', ' &') AS assets_with_permissions 
        FROM (
          SELECT DISTINCT
            inherited_group_name,
            inherited_entitlements,
            object_type,
            object_name,
            privileges
          FROM datahub.injoy_ops_schema.user_permission_snapshot
        )
        WHERE TRIM(inherited_group_name) != ''
        GROUP BY inherited_group_name, inherited_entitlements
        ORDER BY inherited_group_name
        """
        
        with sql.connect(**config) as conn:
            df = pd.read_sql(query, conn)
        
        df = df.rename(columns={
            "inherited_group_name": "ê·¸ë£¹ì´ë¦„",
            "inherited_entitlements": "ê·¸ë£¹ ê¶Œí•œ", 
            "assets_with_permissions": "ê¶Œí•œ(ì—ì…‹íƒ€ì…|ì—ì…‹ëª…|ì—ì…‹ê¶Œí•œ)"
        })
        
        print(f"âœ… Loaded {len(df)} group permission records")
        context['task_instance'].xcom_push(key='df_group_perm', value=df.to_json(orient='records'))

    def load_asset_permissions(**context):
        """ì—ì…‹ë³„ ê¶Œí•œ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Loading asset permissions from Databricks...")
        
        config = get_databricks_connection()
        query = """
        SELECT DISTINCT 
            object_type, 
            object_name, 
            privileges,
            CASE WHEN TRIM(inherited_group_name) = '' THEN 'ê°œì¸'
                 ELSE TRIM(inherited_group_name)
            END AS inherited_group_name,
            IF(TRIM(inherited_group_name) = '', user_name, '') AS inherited_user_name 
        FROM datahub.injoy_ops_schema.user_permission_snapshot
        ORDER BY object_type, object_name, privileges, inherited_group_name
        """
        
        with sql.connect(**config) as conn:
            df = pd.read_sql(query, conn)
        
        df = df.rename(columns={
            "object_type": "ì—ì…‹ íƒ€ì…",
            "object_name": "ì—ì…‹ëª…",
            "privileges": "ì—ì…‹ê¶Œí•œ",
            "inherited_group_name": "ê¶Œí•œ íšë“ ê·¸ë£¹",
            "inherited_user_name": "ê¶Œí•œ íšë“ ê°œì¸"
        })
        
        print(f"âœ… Loaded {len(df)} asset permission records")
        context['task_instance'].xcom_push(key='df_asset_perm', value=df.to_json(orient='records'))

    def sync_group_to_notion(**context):
        """ê·¸ë£¹ ë°ì´í„°ë¥¼ Notionì— ë™ê¸°í™”"""
        print("ğŸ”„ Syncing groups to Notion...")
        
        headers = get_notion_headers()
        db_id = get_var("NOTION_DB_ID_GROUP", required=True)
        
        df_json = context['task_instance'].xcom_pull(task_ids='load_group_permissions', key='df_group_perm')
        df = pd.read_json(df_json, orient='records')
        
        run_sync_for_db("group", db_id, df, headers)
        print("âœ… Group sync completed")

    def sync_user_to_notion(**context):
        """ì‚¬ìš©ì ë°ì´í„°ë¥¼ Notionì— ë™ê¸°í™”"""
        print("ğŸ”„ Syncing users to Notion...")
        
        headers = get_notion_headers()
        db_id_user = get_var("NOTION_DB_ID_USER", required=True)
        db_id_group = get_var("NOTION_DB_ID_GROUP", required=True)
        
        df_json = context['task_instance'].xcom_pull(task_ids='load_user_permissions', key='df_user_perm')
        df = pd.read_json(df_json, orient='records')
        
        # ê·¸ë£¹ ì˜ì¡´ì„± ë§µ ì¤€ë¹„
        final_group_pages = get_all_notion_pages(db_id_group, headers)
        final_group_map = extract_notion_data_map(final_group_pages, "group")
        group_id_to_name = {v['page_id']: v['values']['ê·¸ë£¹ì´ë¦„'] for k, v in final_group_map.items()}
        group_name_to_id = {v: k for k, v in group_id_to_name.items()}
        
        user_dependency_maps = {
            "group": group_id_to_name,
            "group_name_to_id": group_name_to_id
        }
        
        run_sync_for_db("user", db_id_user, df, headers, user_dependency_maps)
        print("âœ… User sync completed")

    def sync_asset_to_notion(**context):
        """ì—ì…‹ ë°ì´í„°ë¥¼ Notionì— ë™ê¸°í™”"""
        print("ğŸ”„ Syncing assets to Notion...")
        
        headers = get_notion_headers()
        db_id_asset = get_var("NOTION_DB_ID_ASSET", required=True)
        db_id_group = get_var("NOTION_DB_ID_GROUP", required=True)
        db_id_user = get_var("NOTION_DB_ID_USER", required=True)
        
        df_json = context['task_instance'].xcom_pull(task_ids='load_asset_permissions', key='df_asset_perm')
        df = pd.read_json(df_json, orient='records')
        
        # ê·¸ë£¹ ì˜ì¡´ì„± ë§µ ì¤€ë¹„
        final_group_pages = get_all_notion_pages(db_id_group, headers)
        final_group_map = extract_notion_data_map(final_group_pages, "group")
        group_id_to_name = {v['page_id']: v['values']['ê·¸ë£¹ì´ë¦„'] for k, v in final_group_map.items()}
        group_name_to_id = {v: k for k, v in group_id_to_name.items()}
        
        # ì‚¬ìš©ì ì˜ì¡´ì„± ë§µ ì¤€ë¹„
        user_dependency_maps = {"group": group_id_to_name, "group_name_to_id": group_name_to_id}
        final_user_pages = get_all_notion_pages(db_id_user, headers)
        final_user_map = extract_notion_data_map(final_user_pages, "user", user_dependency_maps)
        user_id_to_name = {v['page_id']: v['values']['ìœ ì € ì´ë¦„'] for k, v in final_user_map.items()}
        user_name_to_id = {v: k for k, v in user_id_to_name.items()}
        
        asset_dependency_maps = {
            "group": group_id_to_name,
            "user": user_id_to_name,
            "group_name_to_id": group_name_to_id,
            "user_name_to_id": user_name_to_id
        }
        
        run_sync_for_db("asset", db_id_asset, df, headers, asset_dependency_maps)
        print("âœ… Asset sync completed")

    # ==================== Task ì •ì˜ ====================

    task_load_user = PythonOperator(
        task_id='load_user_permissions',
        python_callable=load_user_permissions,
        dag=dag,
    )

    task_load_group = PythonOperator(
        task_id='load_group_permissions',
        python_callable=load_group_permissions,
        dag=dag,
    )

    task_load_asset = PythonOperator(
        task_id='load_asset_permissions',
        python_callable=load_asset_permissions,
        dag=dag,
    )

    task_sync_group = PythonOperator(
        task_id='sync_group_to_notion',
        python_callable=sync_group_to_notion,
        dag=dag,
    )

    task_sync_user = PythonOperator(
        task_id='sync_user_to_notion',
        python_callable=sync_user_to_notion,
        dag=dag,
    )

    task_sync_asset = PythonOperator(
        task_id='sync_asset_to_notion',
        python_callable=sync_asset_to_notion,
        dag=dag,
    )

    # ==================== Task ì˜ì¡´ì„± ====================
    
    # ë°ì´í„° ë¡œë“œëŠ” ë³‘ë ¬ ì‹¤í–‰
    [task_load_user, task_load_group, task_load_asset]
    
    # ê·¸ë£¹ ë™ê¸°í™” ë¨¼ì € ì‹¤í–‰
    task_load_group >> task_sync_group
    
    # ì‚¬ìš©ì ë™ê¸°í™”ëŠ” ê·¸ë£¹ ë™ê¸°í™” ì´í›„
    [task_load_user, task_sync_group] >> task_sync_user
    
    # ì—ì…‹ ë™ê¸°í™”ëŠ” ê·¸ë£¹, ì‚¬ìš©ì ë™ê¸°í™” ì´í›„
    [task_load_asset, task_sync_user] >> task_sync_asset