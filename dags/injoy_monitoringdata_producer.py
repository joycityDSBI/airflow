from datetime import datetime, timedelta, timezone as dt_timezone
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from airflow.models import Variable
import pandas as pd
import requests
from airflow import Dataset
import numpy as np
import json
import pyspark
from airflow.operators.bash import BashOperator

injoy_monitoringdata_producer = Dataset('injoy_monitoringdata_producer')

# ì œì™¸ ê·¸ë£¹ í•„í„°ë§
exclude_groups = ["Operators", "admins", "users", 
                    "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ë§ˆì¼€íŒ…ì‚¬ì´ì–¸ìŠ¤íŒ€", 
                    "ì „ëžµì‚¬ì—…ë³¸ë¶€-ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ì‹¤-ì˜ˆì¸¡ëª¨ë¸ë§íŒ€"]

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(seconds=10),
}

dag = DAG(
    dag_id='injoy_monitoringdata_producer',
    default_args=default_args,
    description='Process Databricks audit logs for aibiGenie',
    schedule='0 0 * * *',  # ë§¤ì¼ ì•„ì¹¨ 9ì‹œ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['databricks', 'audit', 'genie'],
)

def get_databricks_config():
    """Databricks ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'instance': Variable.get('databricks_instance'),
        'token': Variable.get('databricks_token')
    }

# ì„¤ì •
headers = {
    "Authorization": f"Bearer {get_databricks_config()['token']}"
}

url = f"https://{get_databricks_config()['instance']}/api/2.0/token/list"

def tokenize_databricks(url, headers):
    
    resp = requests.get(url, headers=headers)

    if resp.status_code != 200:
        print(f"âŒ í† í° ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        print(resp.text)
    else:
        tokens = resp.json().get("token_infos", [])
        
        def convert_ms(ms):
            if ms == -1:
                return None
            return datetime.fromtimestamp(ms / 1000)

        for t in tokens:
            creation = convert_ms(t.get("creation_time"))
            expiry = convert_ms(t.get("expiry_time"))
            if creation:
                print(f"   ìƒì„±ì‹œê°„: {creation.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ìƒì„±ì‹œê°„: ì•Œ ìˆ˜ ì—†ìŒ")
            if expiry:
                print(f"   ë§Œë£Œì‹œê°„: {expiry.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                print("   ë§Œë£Œì‹œê°„: ë¬´ê¸°í•œ ë˜ëŠ” ì—†ìŒ")



def extract_audit_logs(**context):
    """Databricks audit ë¡œê·¸ ì¶”ì¶œ ë° ì²˜ë¦¬"""
    from databricks import sql
    
    config = get_databricks_config()
    ds = context.get('ds')
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )

    # ì•„ì¹¨ 8ì‹œì— ë°°ì¹˜ê°€ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì— intervarl 2daysì™€ 1daysë¥¼ ì§„í–‰
    query = f"""
    WITH raw_log AS (
        SELECT 
            'audit_log' as log_type,
            user_identity.email as user_email,
            action_name,
            request_params,
            response.result,
            CAST(event_time AS TIMESTAMP) AS event_time_kst
        FROM system.access.audit
        WHERE service_name = 'aibiGenie'
            AND action_name IN ('createConversationMessage', 'updateConversationMessageFeedback', 'getMessageQueryResult')
            AND DATE(event_time) >= CURRENT_DATE - INTERVAL 1 DAYS
            AND DATE(event_time) < CURRENT_DATE - INTERVAL 0 DAYS
    ),

    message_tb AS (
        SELECT
            'audit_log' as log_type,
            user_email,
            get_json_object(result, '$.user_id') AS user_id,
            action_name,
            event_time_kst,
            get_json_object(result, '$.space_id') AS space_id,
            get_json_object(result, '$.conversation_id') AS conversation_id,
            get_json_object(result, '$.message_id') AS message_id
        FROM raw_log
        WHERE action_name IN ('createConversationMessage')
    ),

    feedback_rb AS (
        SELECT * EXCEPT(rn)
        FROM (
            SELECT 
                request_params.space_id, 
                request_params.conversation_id, 
                request_params.message_id, 
                request_params.feedback_rating,
                ROW_NUMBER() OVER(
                    PARTITION BY request_params.space_id, request_params.conversation_id, request_params.message_id 
                    ORDER BY event_time_kst DESC
                ) as rn
            FROM raw_log
            WHERE action_name = 'updateConversationMessageFeedback'
        )
        WHERE rn = 1
    )

    SELECT 
        a.* EXCEPT(event_time_kst), 
        b.feedback_rating, 
        a.event_time_kst 
    FROM message_tb AS a
    LEFT JOIN feedback_rb AS b
        ON a.space_id = b.space_id
        AND a.conversation_id = b.conversation_id
        AND a.message_id = b.message_id
    """
    cursor = connection.cursor()
    cursor.execute(query)
    df_audit = cursor.fetchall_arrow().to_pandas()
    
    cursor.close()
    connection.close()
    
    print(f"âœ… Audit log ì¶”ì¶œ ì™„ë£Œ: {len(df_audit)} rows")
    
    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_audit', value=df_audit.to_json(orient='split', date_format='iso'))
    
    return len(df_audit)


def get_user_groups(**context):
    """
    Task 2: SCIM APIë¡œ ê·¸ë£¹ ë° ì‚¬ìš©ìž ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # Step 1: ê·¸ë£¹ ì „ì²´ ëª©ë¡ ì¡°íšŒ
    group_url = f"http://{config['instance']}/api/2.0/preview/scim/v2/Groups"
    group_resp = requests.get(group_url, headers=headers)
    
    if group_resp.status_code != 200:
        raise Exception(f"ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {group_resp.status_code} - {group_resp.text}")
    
    groups = group_resp.json().get("Resources", [])
    print(f"â­ï¸ ì „ì²´ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸: {groups}")
    
    # Step 2: ê° ê·¸ë£¹ì˜ êµ¬ì„±ì› ìˆ˜ì§‘ (exclude_groups ì œì™¸)
    user_group_list = []
    
    for g in groups:
        group_name = g.get("displayName", "")
        group_id = g.get("id", "")
        
        # exclude_groupsì— í¬í•¨ëœ ê·¸ë£¹ì€ ê±´ë„ˆë›°ê¸°
        if group_name in exclude_groups:
            print(f"â­ï¸ ì œì™¸ëœ ê·¸ë£¹ ê±´ë„ˆë›°ê¸°: {group_name}")
            continue
        
        group_detail_url = f"https://{config['instance']}/api/2.0/preview/scim/v2/Groups/{group_id}"
        detail_resp = requests.get(group_detail_url, headers=headers)
        
        if detail_resp.status_code != 200:
            print(f"âš ï¸ ê·¸ë£¹ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ (group_id={group_id}): {detail_resp.status_code}")
            continue
        
        group_detail = detail_resp.json()
        members = group_detail.get("members", [])
        
        for m in members:
            user_id = m.get("value")
            if user_id:
                user_group_list.append({"user_id": user_id, "group_name": group_name})
    
    # Step 3: DataFrameìœ¼ë¡œ ë³€í™˜ ë° ê·¸ë£¹í•‘
    if not user_group_list:
        print("âš ï¸ ìˆ˜ì§‘ëœ ì‚¬ìš©ìž-ê·¸ë£¹ ë§¤í•‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        df_user_groups = pd.DataFrame(columns=["user_id", "group_name"])
    else:
        df_user_groups = pd.DataFrame(user_group_list).drop_duplicates()
        
        df_user_groups = (
            df_user_groups
            .groupby("user_id")["group_name"]
            .apply(lambda x: sorted(set(x)))
            .reset_index()
        )
    
    print(f"âœ… ì‚¬ìš©ìž ê·¸ë£¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_user_groups)} users")

    # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
    context['ti'].xcom_push(key='df_user_groups', value=df_user_groups.to_json(orient='split'))
    
    return len(df_user_groups)

def enrich_with_groups(**context):
    """
    Task 3: Audit logì— ê·¸ë£¹ ì •ë³´ ë³‘í•©
    """
    ti = context['ti']
    
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit = pd.read_json(ti.xcom_pull(task_ids='extract_audit_logs', key='df_audit'), orient='split')
    df_user_groups = pd.read_json(ti.xcom_pull(task_ids='get_user_groups', key='df_user_groups'), orient='split')
    
    # ë³‘í•©
    df_audit_with_group = df_audit.merge(df_user_groups, on="user_id", how="left")
    
    # group_nameì´ NaNì¸ í–‰ ì œê±°
    df_audit_with_group = df_audit_with_group.dropna(subset=["group_name"])
    
    print(f"âœ… ê·¸ë£¹ ì •ë³´ ë³‘í•© ì™„ë£Œ: {len(df_audit_with_group)} rows")
    print(f"âœ… ì‚¬ìš©ìž ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸:")
    for _, row in df_audit_with_group.iterrows():
        print(f"  - {row['user_email']} ({row['user_id']})")
    
    context['ti'].xcom_push(key='df_audit_with_group', value=df_audit_with_group.to_json(orient='split', date_format='iso'))
    
    return len(df_audit_with_group)

def get_space_info(**context):
    """
    Task 4: Genie APIë¡œ ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
    """
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
    spaces_url = f"https://{config['instance']}/api/2.0/genie/spaces"
    resp = requests.get(spaces_url, headers=headers)
    
    if resp.status_code != 200:
        print(f"âš ï¸ ìŠ¤íŽ˜ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
        space_id_to_name = {}
    else:
        spaces = resp.json().get("spaces", [])
        space_df = pd.DataFrame([
            {"space_id": s.get("space_id"), "space_name": s.get("title")}
            for s in spaces
        ])
        space_id_to_name = dict(zip(space_df["space_id"], space_df["space_name"]))
    
    print(f"âœ… ìŠ¤íŽ˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(space_id_to_name)} spaces")
    
    context['ti'].xcom_push(key='space_id_to_name', value=space_id_to_name)
    
    return len(space_id_to_name)

def get_message_details(**context):
    """
    Task 5: Genie APIë¡œ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    """
    ti = context['ti']
    config = get_databricks_config()
    headers = {"Authorization": f"Bearer {config['token']}"}
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_audit_with_group = pd.read_json(ti.xcom_pull(task_ids='enrich_with_groups', key='df_audit_with_group'), orient='split')
    space_id_to_name = ti.xcom_pull(task_ids='get_space_info', key='space_id_to_name')
    
    # ìŠ¤íŽ˜ì´ìŠ¤ ì´ë¦„ ì¶”ê°€
    df_audit_with_group["space_name"] = df_audit_with_group["space_id"].map(space_id_to_name)
    
    # group_nameì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë¯€ë¡œ anyë¡œ ì²´í¬
    def should_exclude(group_list):
        if isinstance(group_list, list):
            return any(g in exclude_groups for g in group_list)
        return False
    
    df_target = df_audit_with_group[~df_audit_with_group["group_name"].apply(should_exclude)].copy()
    
    # ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    contents = []
    queries = []
    statement_ids = []
    
    total_rows = len(df_target)
    print(f"ðŸ”„ ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œìž‘: {total_rows} rows")
    
    for idx, row in df_target.iterrows():
        if idx % 100 == 0:
            print(f"  Progress: {idx}/{total_rows}")
        
        space_id = row["space_id"]
        conversation_id = row["conversation_id"]
        message_id = row["message_id"]
        
        if None in [space_id, conversation_id, message_id]:
            contents.append(None)
            queries.append(None)
            statement_ids.append(None)
            continue
        
        url = f"https://{config['instance']}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages/{message_id}"
        
        try:
            resp = requests.get(url, headers=headers)
            if resp.status_code == 200:
                data = resp.json()
                
                # Content ì²˜ë¦¬
                content_raw = data.get("content")
                if isinstance(content_raw, str):
                    content = content_raw.replace("\n", " ")
                else:
                    content = str(content_raw) if content_raw is not None else None
                
                # Query ì²˜ë¦¬
                attachments = data.get("attachments", [])
                if attachments and isinstance(attachments, list):
                    query = attachments[0].get("query", {}).get("query", None)
                else:
                    query = None
                
                # Statement ID ì²˜ë¦¬
                statement_id = data.get("query_result", {}).get("statement_id")
                
            else:
                content, query, statement_id = None, None, None
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ ({idx}í–‰): {e}")
            content, query, statement_id = None, None, None
        
        contents.append(content)
        queries.append(query)
        statement_ids.append(statement_id)
    
    # ë°ì´í„° ì¶”ê°€
    df_target['content'] = contents
    df_target['query'] = queries
    df_target['statement_id'] = statement_ids
    
    print(f"âœ… ë©”ì‹œì§€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(df_target)} rows")
    
    context['ti'].xcom_push(key='df_target', value=df_target.to_json(orient='split', date_format='iso'))
    
    return len(df_target)


def merge_query_history(**context):
    """
    Task 6: Query historyì™€ ë³‘í•© ë° ìµœì¢… ë°ì´í„° ì €ìž¥
    """
    from databricks import sql
    import pandas as pd
    from io import StringIO
    import numpy as np
    
    ti = context['ti']
    config = get_databricks_config()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    json_data = ti.xcom_pull(task_ids='get_message_details', key='df_target')
    df_target = pd.read_json(StringIO(json_data), orient='split')
        
    print(f"ðŸ“Š df_target ì»¬ëŸ¼: {df_target.columns.tolist()}")
    print(f"ðŸ“Š df_target head:\n{df_target.head()}")

    if df_target.empty:
        print("âš ï¸ df_targetì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        print("ì´ì „ Task('get_message_details')ì—ì„œ ë°ì´í„° ì¡°íšŒê°€ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        return 0  # ë˜ëŠ” raise Exception("df_target is empty")


    # Databricks SQL ì—°ê²°
    connection = sql.connect(
        server_hostname=config['instance'].replace('https://', ''),
        http_path=Variable.get('databricks_http_path'),
        access_token=config['token']
    )
    
    cursor = connection.cursor()
    
    # Query history ì¡°íšŒ
    query_history_sql = """
    SELECT 
        statement_id, 
        executed_by, 
        execution_status, 
        CAST(total_duration_ms AS DOUBLE) / 1000 AS query_duration_seconds, 
        CAST(result_fetch_duration_ms AS DOUBLE) / 1000 AS query_result_fetch_duration_seconds, 
        CAST(end_time AS TIMESTAMP) + INTERVAL 9 HOURS AS query_end_time_kst
    FROM system.query.history
    WHERE query_source.genie_space_id IS NOT NULL
        AND statement_type = 'SELECT'
        AND DATE(end_time) >= CURRENT_DATE - INTERVAL 2 DAYS
        AND DATE(end_time) < CURRENT_DATE - INTERVAL 1 DAYS
    """
    
    cursor.execute(query_history_sql)
    query_df = cursor.fetchall_arrow().to_pandas()
    
    print(f"ðŸ“Š Query history ì¡°íšŒ ì™„ë£Œ: {len(query_df)} rows")
    # ===== ðŸ”¥ ë¹ˆ DataFrame ì²´í¬ ì¶”ê°€ =====
    if query_df.empty:
        print("âš ï¸ Query historyê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. Query ê´€ë ¨ ì»¬ëŸ¼ì„ NULLë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        
        # Query ê´€ë ¨ ì»¬ëŸ¼ì„ NULLë¡œ ì¶”ê°€
        df_audit_enriched = df_target.copy()
        df_audit_enriched['query_end_time_kst'] = None
        df_audit_enriched['query_duration_seconds'] = None
        df_audit_enriched['query_result_fetch_duration_seconds'] = None
        df_audit_enriched['execution_status'] = None
        df_audit_enriched['message_response_duration_seconds'] = None
        
        print(f"âœ… Query history ì—†ì´ ì§„í–‰: {len(df_audit_enriched)} rows")
    else:
        # ì»¬ëŸ¼ rename
        query_df_renamed = query_df.rename(columns={"executed_by": "user_email"})
   
        # ë³‘í•©
        df_audit_enriched = df_target.merge(
            query_df_renamed[[
                "statement_id", "user_email", "query_end_time_kst", 
                "query_duration_seconds", "query_result_fetch_duration_seconds", "execution_status"
            ]],
            how="left",
            on=["statement_id", "user_email"]
        )
        
        print(f"ðŸ“Š Query history ë³‘í•© ì™„ë£Œ: {len(df_audit_enriched)} rows")
        
        # ===== message_response_duration_seconds ê³„ì‚° =====
        # event_time_kstì™€ query_end_time_kstì˜ ì°¨ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ê³„ì‚°
        if 'event_time_kst' in df_audit_enriched.columns and 'query_end_time_kst' in df_audit_enriched.columns:
            # ë¬¸ìžì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            df_audit_enriched['event_time_kst'] = pd.to_datetime(df_audit_enriched['event_time_kst'])
            df_audit_enriched['query_end_time_kst'] = pd.to_datetime(df_audit_enriched['query_end_time_kst'])
            
            # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
            df_audit_enriched['message_response_duration_seconds'] = (
                df_audit_enriched['query_end_time_kst'] - df_audit_enriched['event_time_kst']
            ).dt.total_seconds()
            
            print(f"âœ… message_response_duration_seconds ê³„ì‚° ì™„ë£Œ")
        else:
            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ NULLë¡œ ì¶”ê°€
            df_audit_enriched['message_response_duration_seconds'] = None
            print(f"âš ï¸ event_time_kst ë˜ëŠ” query_end_time_kst ì»¬ëŸ¼ì´ ì—†ì–´ message_response_duration_secondsë¥¼ NULLë¡œ ì„¤ì •")
        
        print(f"ðŸ“‹ DataFrame ì»¬ëŸ¼: {df_audit_enriched.columns.tolist()}")
        
        # ===== íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ =====
        target_table = "datahub.injoy_ops_schema.injoy_monitoring_data"
        temp_table = "datahub.injoy_ops_schema.temp_merge_data"
        
        # íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ ë° íƒ€ìž… ì¡°íšŒ
        cursor.execute(f"DESCRIBE TABLE {target_table}")
        table_schema = cursor.fetchall_arrow().to_pandas()
        
        print(f"ðŸ“‹ íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:")
        print(table_schema[['col_name', 'data_type']].to_string())
        
        # ì»¬ëŸ¼ëª…ì„ í‚¤ë¡œ, ë°ì´í„° íƒ€ìž…ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
        target_column_types = dict(zip(table_schema['col_name'], table_schema['data_type']))
        target_columns = table_schema['col_name'].tolist()
        
        # DataFrameê³¼ íƒ€ê²Ÿ í…Œì´ë¸” ê³µí†µ ì»¬ëŸ¼ë§Œ ì„ íƒ
        df_columns = df_audit_enriched.columns.tolist()
        common_columns = [col for col in df_columns if col in target_columns]
        missing_in_target = [col for col in df_columns if col not in target_columns]
        missing_in_df = [col for col in target_columns if col not in df_columns]
        
        if missing_in_target:
            print(f"âš ï¸ íƒ€ê²Ÿ í…Œì´ë¸”ì— ì—†ëŠ” ì»¬ëŸ¼ (ì œì™¸ë¨): {missing_in_target}")
        
        if missing_in_df:
            print(f"âš ï¸ DataFrameì— ì—†ì§€ë§Œ íƒ€ê²Ÿ í…Œì´ë¸”ì— ìžˆëŠ” ì»¬ëŸ¼: {missing_in_df}")
            # íƒ€ê²Ÿ í…Œì´ë¸”ì—ëŠ” ìžˆì§€ë§Œ DataFrameì— ì—†ëŠ” ì»¬ëŸ¼ì„ NULLë¡œ ì¶”ê°€
            for col in missing_in_df:
                if col not in ['id', 'created_at', 'updated_at']:  # ìžë™ ìƒì„± ì»¬ëŸ¼ ì œì™¸
                    df_audit_enriched[col] = None
                    common_columns.append(col)
                    print(f"   â†’ {col} ì»¬ëŸ¼ì„ NULLë¡œ ì¶”ê°€")
        
        # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í•„í„°ë§
        df_to_insert = df_audit_enriched[common_columns].copy()
        
        print(f"âœ… ì‚¬ìš©í•  ì»¬ëŸ¼ ({len(common_columns)}ê°œ): {common_columns}")
        
        # ===== ìž„ì‹œ í…Œì´ë¸” ìƒì„± (íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ì‹¤ì œ íƒ€ìž… ì‚¬ìš©) =====
        
        # ê¸°ì¡´ ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ
        cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
        
        # íƒ€ê²Ÿ í…Œì´ë¸”ì˜ íƒ€ìž…ì„ ì‚¬ìš©í•˜ì—¬ CREATE TABLE ë¬¸ ìƒì„±
        column_definitions = []
        for col in common_columns:
            sql_type = target_column_types.get(col, 'STRING')  # íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ì‹¤ì œ íƒ€ìž… ì‚¬ìš©
            column_definitions.append(f"`{col}` {sql_type}")
        
        create_table_sql = f"""
        CREATE TABLE {temp_table} (
            {', '.join(column_definitions)}
        )
        USING DELTA
        """
        
        print(f"ðŸ“ ìž„ì‹œ í…Œì´ë¸” ìƒì„± SQL:")
        print(create_table_sql)
        
        cursor.execute(create_table_sql)
        print(f"âœ… ìž„ì‹œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {temp_table}")
        
        # ìƒì„±ëœ ìž„ì‹œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
        cursor.execute(f"DESCRIBE TABLE {temp_table}")
        temp_schema = cursor.fetchall_arrow().to_pandas()
        print(f"ðŸ“‹ ìƒì„±ëœ ìž„ì‹œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:")
        print(temp_schema[['col_name', 'data_type']].to_string())
        
        # DataFrameì„ batch INSERT
        columns = common_columns
        column_str = ", ".join([f"`{col}`" for col in columns])
        
        # NULL ê°’ì„ SQL NULLë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def convert_value(val, col_name):
            """
            ê°’ì„ SQL ë¬¸ìžì—´ë¡œ ë³€í™˜
            col_name: íƒ€ìž… ì²´í¬ë¥¼ ìœ„í•œ ì»¬ëŸ¼ëª…
            """
            # None ì²´í¬ ë¨¼ì €
            if val is None:
                return "NULL"
            # pandas NA íƒ€ìž… ì²´í¬
            if pd.isna(val) if not isinstance(val, (list, tuple, np.ndarray)) else False:
                return "NULL"
            # numpy nan ì²´í¬
            try:
                if np.isnan(val):
                    return "NULL"
            except (TypeError, ValueError):
                pass
            
            # íƒ€ê²Ÿ í…Œì´ë¸”ì˜ íƒ€ìž… í™•ì¸
            target_type = target_column_types.get(col_name, '').lower()
            
            # ARRAY íƒ€ìž… ì²˜ë¦¬
            if 'array' in target_type:
                if isinstance(val, (list, tuple)):
                    # ë¦¬ìŠ¤íŠ¸ë¥¼ SQL ARRAYë¡œ ë³€í™˜
                    array_elements = [f"'{str(v).replace(chr(39), chr(39)+chr(39))}'" for v in val]
                    return f"ARRAY({', '.join(array_elements)})"
                elif isinstance(val, str):
                    # ë¬¸ìžì—´ì„ ë‹¨ì¼ ìš”ì†Œ ë°°ì—´ë¡œ ë³€í™˜
                    escaped = val.replace("'", "''")
                    return f"ARRAY('{escaped}')"
                else:
                    return "NULL"
            
            # íƒ€ìž…ë³„ ì²˜ë¦¬
            if isinstance(val, str):
                # SQL Injection ë°©ì§€ë¥¼ ìœ„í•´ escape ì²˜ë¦¬
                escaped = val.replace("'", "''").replace("\\", "\\\\")
                return f"'{escaped}'"
            elif isinstance(val, bool):
                return str(val).upper()  # TRUE/FALSE
            elif isinstance(val, (int, float, np.integer, np.floating)):
                return str(val)
            elif isinstance(val, (pd.Timestamp, np.datetime64)):
                try:
                    ts = pd.Timestamp(val)
                    return f"'{ts.strftime('%Y-%m-%d %H:%M:%S')}'"
                except:
                    return "NULL"
            else:
                # ê¸°íƒ€ íƒ€ìž…ì€ ë¬¸ìžì—´ë¡œ ë³€í™˜
                try:
                    escaped = str(val).replace("'", "''").replace("\\", "\\\\")
                    return f"'{escaped}'"
                except:
                    return "NULL"
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ INSERT
        batch_size = 1000
        total_rows = len(df_to_insert)
        
        for start_idx in range(0, total_rows, batch_size):
            end_idx = min(start_idx + batch_size, total_rows)
            batch_df = df_to_insert.iloc[start_idx:end_idx]
            
            values_list = []
            for idx, row in batch_df.iterrows():
                try:
                    row_values = ", ".join([convert_value(row[col], col) for col in columns])
                    values_list.append(f"({row_values})")
                except Exception as e:
                    print(f"âš ï¸ Row {idx} ë³€í™˜ ì‹¤íŒ¨: {e}")
                    print(f"   ë¬¸ì œ ë°ì´í„°: {row.to_dict()}")
                    raise
            
            values_str = ", ".join(values_list)
            
            insert_sql = f"""
            INSERT INTO {temp_table} ({column_str})
            VALUES {values_str}
            """
            
            try:
                cursor.execute(insert_sql)
                print(f"ðŸ“ ë°°ì¹˜ INSERT ì™„ë£Œ: {start_idx+1}-{end_idx}/{total_rows}")
            except Exception as e:
                print(f"âš ï¸ INSERT ì‹¤íŒ¨ at batch {start_idx}-{end_idx}")
                print(f"   SQL ë¯¸ë¦¬ë³´ê¸°: {insert_sql[:500]}...")
                raise
        
        print(f"âœ… ìž„ì‹œ í…Œì´ë¸” ë°ì´í„° ì ìž¬ ì™„ë£Œ: {total_rows} rows")
        
        # ===== MERGE ì‹¤í–‰ =====
        merge_key = 'message_id'
        
        # ë™ì ìœ¼ë¡œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        update_set = ", ".join([f"target.`{col}` = source.`{col}`" for col in columns if col != merge_key])
        insert_columns = ", ".join([f"`{col}`" for col in columns])
        insert_values = ", ".join([f"source.`{col}`" for col in columns])
        
        merge_sql = f"""
        MERGE INTO {target_table} AS target
        USING {temp_table} AS source
        ON target.`{merge_key}` = source.`{merge_key}`
        WHEN MATCHED THEN
            UPDATE SET {update_set}
        WHEN NOT MATCHED THEN
            INSERT ({insert_columns})
            VALUES ({insert_values})
        """
        
        print(f"ðŸ“ MERGE ì‹¤í–‰ ì¤‘...")
        cursor.execute(merge_sql)
        print("âœ… ë°ì´í„° UPSERT ì™„ë£Œ")
        
        # ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ
        cursor.execute(f"DROP TABLE IF EXISTS {temp_table}")
        print(f"ðŸ—‘ï¸ ìž„ì‹œ í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")
        
        cursor.close()
        connection.close()
        
        print(f"âœ… Delta í…Œì´ë¸” ì €ìž¥ ì™„ë£Œ")
        
        return len(df_to_insert)

# Task ì •ì˜
bash_task = BashOperator(
    task_id = 'bash_task',
    outlets = [injoy_monitoringdata_producer],
    bash_command = 'echo "producer_1 ìˆ˜í–‰ ì™„ë£Œ"'
)

task0 = PythonOperator(
    task_id='tokenize_databricks',
    python_callable=tokenize_databricks,
    op_kwargs={'url': url, 'headers': headers},
    dag=dag,
)

task1 = PythonOperator(
    task_id='extract_audit_logs',
    python_callable=extract_audit_logs,
    dag=dag,
)

task2 = PythonOperator(
    task_id='get_user_groups',
    python_callable=get_user_groups,
    dag=dag,
)

task3 = PythonOperator(
    task_id='enrich_with_groups',
    python_callable=enrich_with_groups,
    dag=dag,
)

task4 = PythonOperator(
    task_id='get_space_info',
    python_callable=get_space_info,
    dag=dag,
)

task5 = PythonOperator(
    task_id='get_message_details',
    python_callable=get_message_details,
    dag=dag,
)

task6 = PythonOperator(
    task_id='merge_query_history',
    python_callable=merge_query_history,
    dag=dag,
)


# Task ì˜ì¡´ì„± ì„¤ì •
task0 >> [task1, task2] >> task3
[task3, task4] >> task5 >> task6 >> bash_task